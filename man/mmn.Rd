% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mmn.R
\name{mmn}
\alias{mmn}
\title{Train a Multi-Modal Neural Network (MMN)}
\usage{
mmn(
  formula,
  dataList = NULL,
  fusion_hidden = c(50L, 50L),
  fusion_activation = "selu",
  fusion_bias = TRUE,
  fusion_dropout = 0,
  loss = c("mse", "mae", "softmax", "cross-entropy", "gaussian", "binomial", "poisson",
    "mvp", "nbinom", "multinomial", "clogit"),
  optimizer = c("sgd", "adam", "adadelta", "adagrad", "rmsprop", "rprop"),
  lr = 0.01,
  alpha = 0.5,
  lambda = 0,
  validation = 0,
  batchsize = 32L,
  burnin = Inf,
  shuffle = TRUE,
  epochs = 100,
  early_stopping = NULL,
  lr_scheduler = NULL,
  custom_parameters = NULL,
  device = c("cpu", "cuda", "mps"),
  plot = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{formula}{A formula object specifying the model structure. See examples for more information.}

\item{dataList}{A list containing the data for training the model. The list should contain all variables used in the formula.}

\item{fusion_hidden}{A numeric vector specifying the number of units (nodes) in each hidden layer of the fusion network. The length of this vector determines the number of hidden layers created, with each element specifying the number of units in the corresponding layer.}

\item{fusion_activation}{A character vector specifying the activation function(s) applied after each hidden layer in the fusion network. If a single character string is provided, the same activation function will be applied to all hidden layers. Alternatively, a character vector of the same length as \code{fusion_hidden} can be provided to apply different activation functions to each layer. Available options include: \code{"relu"}, \code{"leaky_relu"}, \code{"tanh"}, \code{"elu"}, \code{"rrelu"}, \code{"prelu"}, \code{"softplus"}, \code{"celu"}, \code{"selu"}, \code{"gelu"}, \code{"relu6"}, \code{"sigmoid"}, \code{"softsign"}, \code{"hardtanh"}, \code{"tanhshrink"}, \code{"softshrink"}, \code{"hardshrink"}, \code{"log_sigmoid"}.}

\item{fusion_bias}{A logical value or a vector indicating whether to include bias terms in each layer of the fusion network. If a single logical value is provided, it will apply to all layers. To specify bias inclusion for each layer individually, provide a logical vector of length \code{length(fusion_hidden) + 1}, where each element corresponds to a hidden layer, and the final element controls whether a bias term is added to the output layer.}

\item{fusion_dropout}{The dropout rate(s) to apply to each hidden layer in the fusion network. This can be a single numeric value (between 0 and 1) to apply the same dropout rate to all hidden layers, or a numeric vector of length \code{length(fusion_hidden)} to set different dropout rates for each layer individually. The dropout rate is not applied to the output layer.}

\item{loss}{The loss function to be used. Options include "mse", "mae", "softmax", "cross-entropy", "gaussian", "binomial", "poisson", "nbinom", "mvp", "multinomial", and "clogit". You can also specify your own loss function. See Details for more information. Default is "mse".}

\item{optimizer}{The optimizer to be used. Options include "sgd", "adam", "adadelta", "adagrad", "rmsprop", and "rprop". See \code{\link{config_optimizer}} for further adjustments to the optimizer. Default is "sgd".}

\item{lr}{Learning rate for the optimizer. Default is 0.01.}

\item{alpha}{Alpha value for L1/L2 regularization. Default is 0.5.}

\item{lambda}{Lambda value for L1/L2 regularization. Default is 0.0.}

\item{validation}{Proportion of the data to be used for validation. Default is 0.0.}

\item{batchsize}{Batch size for training. Default is 32.}

\item{burnin}{Number of epochs after which the training stops if the loss is still above the base loss. Default is Inf.}

\item{shuffle}{Whether to shuffle the data before each epoch. Default is TRUE.}

\item{epochs}{Number of epochs to train the model. Default is 100.}

\item{early_stopping}{Number of epochs with no improvement after which training will be stopped. Default is NULL.}

\item{lr_scheduler}{Learning rate scheduler. See \code{\link{config_lr_scheduler}} for creating a learning rate scheduler. Default is NULL.}

\item{custom_parameters}{Parameters for the custom loss function. See the vignette for an example. Default is NULL.}

\item{device}{Device to be used for training. Options are "cpu", "cuda", and "mps". Default is "cpu".}

\item{plot}{Whether to plot the training progress. Default is TRUE.}

\item{verbose}{Whether to print detailed training progress. Default is TRUE.}
}
\value{
An S3 object of class \code{"citommn"} is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:
\item{net}{An object of class "nn_module", originates from the torch package and represents the core object of this workflow.}
\item{call}{The original function call.}
\item{loss}{A list which contains relevant information for the target variable and the used loss function.}
\item{data}{Contains the data used for the training of the model.}
\item{base_loss}{The loss of the intercept-only model.}
\item{weights}{List of parameters (weights and biases) of the models from the best and the last training epoch.}
\item{buffers}{List of buffers (e.g. running mean and variance of batch normalization layers) of the models from the best and the last training epoch.}
\item{use_model_epoch}{Integer, defines whether the model from the best (= 1) or the last (= 2) training epoch should be used for prediction.}
\item{loaded_model_epoch}{Integer, shows whether the parameters and buffers of the model from the best (= 1) or the last (= 2) training epoch are currently loaded in \code{net}.}
\item{model_properties}{A list of properties, that define the architecture of the model.}
\item{training_properties}{A list of all the training parameters used the last time the model was trained.}
\item{losses}{A data.frame containing training and validation losses of each epoch.}
}
\description{
This function trains a Multi-Modal Neural Network (MMN) which consists of a combination of DNNs and CNNs.
}
\seealso{
\code{\link{predict.citommn}}, \code{\link{print.citommn}}, \code{\link{summary.citommn}}, \code{\link{coef.citommn}}, \code{\link{continue_training}}, \code{\link{analyze_training}}
}
\author{
Armin Schenk
}
