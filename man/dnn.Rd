% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dnn.R
\name{dnn}
\alias{dnn}
\title{DNN}
\usage{
dnn(
  formula,
  data = NULL,
  family = stats::gaussian(),
  hidden = c(10L, 10L, 10L),
  activation = "relu",
  validation = 0,
  bias = TRUE,
  lambda = 0,
  alpha = 0.5,
  dropout = 0,
  optimizer = "adam",
  lr = 0.01,
  batchsize = 32L,
  shuffle = FALSE,
  epochs = 64,
  plot = TRUE,
  lr_scheduler = FALSE,
  device = "cuda",
  early_stopping = FALSE,
  config = list()
)
}
\arguments{
\item{formula}{formula object}

\item{data}{matrix or data.frame}

\item{family}{error distribution with link function, see details for supported family functions}

\item{hidden}{hidden units in layers, length of hidden corresponds to number of layers}

\item{activation}{activation functions, can be of length one, or a vector of activation functions for each layer. Currently supported: tanh, relu, leakyrelu, selu, or sigmoid}

\item{validation}{percentage of data set that should be taken as validation set (chosen randomly)}

\item{bias}{whether use biases in the layers, can be of length one, or a vector (number of hidden layers + 1 (last layer)) of logicals for each layer.}

\item{lambda}{lambda penalty, strength of regularization: \eqn{\lambda * (lasso + ridge)}}

\item{alpha}{add L1/L2 regularization to training  \eqn{(1 - \alpha) * |weights| + \alpha ||weights||^2} will get added for each layer. Can be single integer between 0 and 1 or vector of alpha values if layers should be regularized differently.}

\item{dropout}{probability of dropout rate}

\item{optimizer}{which optimizer used for training the network,}

\item{lr}{learning rate given to optimizer}

\item{batchsize}{how many samples data loader loads per batch}

\item{shuffle}{TRUE if data should be reshuffled every epoch (default: FALSE)}

\item{epochs}{epochs for training loop}

\item{plot}{plot training loss}

\item{lr_scheduler}{learning rate scheduler, can be "lambda", "multiplicative", "one_cycle" or "step" additional arguments bust be defined in config with "lr_scheduler." as prefix}

\item{device}{device on which network should be trained on, either "cpu" or "cuda"}

\item{early_stopping}{training will stop if validation loss worsened between current and past epoch, function expects the range how far back the comparison should be done}

\item{config}{list of additional arguments to be passed to optimizer or lr_scheduler should start with optimizer. and lr_scheduler.}
}
\description{
DNN
}
\examples{
library(cito)

set.seed(222)
validation_set<- sample(c(1:nrow(datasets::iris)),25)

# Build and train  Network
nn.fit<- dnn(Sepal.Length~., data = datasets::iris[-validation_set,])

# Sturcture of Neural Network
print(nn.fit)

# Use model on validation set
predictions <- predict(nn.fit, iris[validation_set,])

# Scatterplot
plot(iris[validation_set,]$Sepal.Length,predictions)
# MAE
mean(abs(predictions-iris[validation_set,]$Sepal.Length))

}
