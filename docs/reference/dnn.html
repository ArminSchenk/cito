<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="fits a custom deep neural network using the Multilayer Perceptron architecture. dnn() supports the formula syntax and allows to customize the neural network to a maximal degree."><title>DNN — dnn • cito</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="DNN — dnn"><meta property="og:description" content="fits a custom deep neural network using the Multilayer Perceptron architecture. dnn() supports the formula syntax and allows to customize the neural network to a maximal degree."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a>
    <a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a>
    <a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a>
    <a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>DNN</h1>
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/R/dnn.R" class="external-link"><code>R/dnn.R</code></a></small>
      <div class="d-none name"><code>dnn.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>fits a custom deep neural network using the Multilayer Perceptron architecture. <code>dnn()</code> supports the formula syntax and allows to customize the neural network to a maximal degree.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">dnn</span><span class="op">(</span></span>
<span>  <span class="va">formula</span>,</span>
<span>  data <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mse"</span>, <span class="st">"mae"</span>, <span class="st">"softmax"</span>, <span class="st">"cross-entropy"</span>, <span class="st">"gaussian"</span>, <span class="st">"binomial"</span>, <span class="st">"poisson"</span><span class="op">)</span>,</span>
<span>  hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">50L</span>, <span class="fl">50L</span><span class="op">)</span>,</span>
<span>  activation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"relu"</span>, <span class="st">"leaky_relu"</span>, <span class="st">"tanh"</span>, <span class="st">"elu"</span>, <span class="st">"rrelu"</span>, <span class="st">"prelu"</span>, <span class="st">"softplus"</span>,</span>
<span>    <span class="st">"celu"</span>, <span class="st">"selu"</span>, <span class="st">"gelu"</span>, <span class="st">"relu6"</span>, <span class="st">"sigmoid"</span>, <span class="st">"softsign"</span>, <span class="st">"hardtanh"</span>, <span class="st">"tanhshrink"</span>,</span>
<span>    <span class="st">"softshrink"</span>, <span class="st">"hardshrink"</span>, <span class="st">"log_sigmoid"</span><span class="op">)</span>,</span>
<span>  validation <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  bias <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  lambda <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  dropout <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"sgd"</span>, <span class="st">"adam"</span>, <span class="st">"adadelta"</span>, <span class="st">"adagrad"</span>, <span class="st">"rmsprop"</span>, <span class="st">"rprop"</span><span class="op">)</span>,</span>
<span>  lr <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span>  batchsize <span class="op">=</span> <span class="fl">32L</span>,</span>
<span>  shuffle <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  bootstrap <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  bootstrap_parallel <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  plot <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  lr_scheduler <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  custom_parameters <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  device <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"cpu"</span>, <span class="st">"cuda"</span>, <span class="st">"mps"</span><span class="op">)</span>,</span>
<span>  early_stopping <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>formula</dt>
<dd><p>an object of class "<code><a href="https://rdrr.io/r/stats/formula.html" class="external-link">formula</a></code>": a description of the model that should be fitted</p></dd>


<dt>data</dt>
<dd><p>matrix or data.frame with features/predictors and response variable</p></dd>


<dt>loss</dt>
<dd><p>loss after which network should be optimized. Can also be distribution from the stats package or own function, see details</p></dd>


<dt>hidden</dt>
<dd><p>hidden units in layers, length of hidden corresponds to number of layers</p></dd>


<dt>activation</dt>
<dd><p>activation functions, can be of length one, or a vector of different activation functions for each layer</p></dd>


<dt>validation</dt>
<dd><p>percentage of data set that should be taken as validation set (chosen randomly)</p></dd>


<dt>bias</dt>
<dd><p>whether use biases in the layers, can be of length one, or a vector (number of hidden layers + 1 (last layer)) of logicals for each layer.</p></dd>


<dt>lambda</dt>
<dd><p>strength of regularization: lambda penalty, \(\lambda * (L1 + L2)\) (see alpha)</p></dd>


<dt>alpha</dt>
<dd><p>add L1/L2 regularization to training  \((1 - \alpha) * |weights| + \alpha ||weights||^2\) will get added for each layer. Can be single integer between 0 and 1 or vector of alpha values if layers should be regularized differently.</p></dd>


<dt>dropout</dt>
<dd><p>dropout rate, probability of a node getting left out during training (see <code><a href="https://rdrr.io/pkg/torch/man/nn_dropout.html" class="external-link">nn_dropout</a></code>)</p></dd>


<dt>optimizer</dt>
<dd><p>which optimizer used for training the network, for more adjustments to optimizer see <code><a href="config_optimizer.html">config_optimizer</a></code></p></dd>


<dt>lr</dt>
<dd><p>learning rate given to optimizer</p></dd>


<dt>batchsize</dt>
<dd><p>number of samples that are used to calculate one learning rate step</p></dd>


<dt>shuffle</dt>
<dd><p>if TRUE, data in each batch gets reshuffled every epoch</p></dd>


<dt>epochs</dt>
<dd><p>epochs the training goes on for</p></dd>


<dt>bootstrap</dt>
<dd><p>bootstrap neural network or not, numeric corresponds to number of bootstrap samples</p></dd>


<dt>bootstrap_parallel</dt>
<dd><p>parallelize (CPU) bootstrapping</p></dd>


<dt>plot</dt>
<dd><p>plot training loss</p></dd>


<dt>verbose</dt>
<dd><p>print training and validation loss of epochs</p></dd>


<dt>lr_scheduler</dt>
<dd><p>learning rate scheduler created with <code><a href="config_lr_scheduler.html">config_lr_scheduler</a></code></p></dd>


<dt>custom_parameters</dt>
<dd><p>List of parameters/variables to be optimized. Can be used in a custom loss function. See Vignette for example.</p></dd>


<dt>device</dt>
<dd><p>device on which network should be trained on. mps correspond to M1/M2 GPU devices.</p></dd>


<dt>early_stopping</dt>
<dd><p>if set to integer, training will stop if loss has gotten higher for defined number of epochs in a row, will use validation loss is available.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>an S3 object of class <code>"cito.dnn"</code> is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:</p>
<dl><dt>net</dt>
<dd><p>An object of class "nn_sequential" "nn_module", originates from the torch package and represents the core object of this workflow.</p></dd>

<dt>call</dt>
<dd><p>The original function call</p></dd>

<dt>loss</dt>
<dd><p>A list which contains relevant information for the target variable and the used loss function</p></dd>

<dt>data</dt>
<dd><p>Contains data used for training the model</p></dd>

<dt>weigths</dt>
<dd><p>List of weights for each training epoch</p></dd>

<dt>use_model_epoch</dt>
<dd><p>Integer, which defines which model from which training epoch should be used for prediction. 1 = best model, 2 = last model</p></dd>

<dt>loaded_model_epoch</dt>
<dd><p>Integer, shows which model from which epoch is loaded currently into model$net.</p></dd>

<dt>model_properties</dt>
<dd><p>A list of properties of the neural network, contains number of input nodes, number of output nodes, size of hidden layers, activation functions, whether bias is included and if dropout layers are included.</p></dd>

<dt>training_properties</dt>
<dd><p>A list of all training parameters that were used the last time the model was trained. It consists of learning rate, information about an learning rate scheduler, information about the optimizer, number of epochs, whether early stopping was used, if plot was active, lambda and alpha for L1/L2 regularization, batchsize, shuffle, was the data set split into validation and training, which formula was used for training and at which epoch did the training stop.</p></dd>

<dt>losses</dt>
<dd><p>A data.frame containing training and validation losses of each epoch</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="loss-functions-likelihoods">Loss functions / Likelihoods<a class="anchor" aria-label="anchor" href="#loss-functions-likelihoods"></a></h2>
    <p>We support loss functions and likelihoods for different tasks:</p><table class="table table"><tr><td>Name</td><td>Explanation</td><td>Example / Task</td></tr><tr><td>mse</td><td>mean squared error</td><td>Regression, predicting continuous values</td></tr><tr><td>mae</td><td>mean absolute error</td><td>Regression, predicting continuous values</td></tr><tr><td>softmax</td><td>categorical cross entropy</td><td>Multi-class, species classification</td></tr><tr><td>cross-entropy</td><td>categorical cross entropy</td><td>Multi-class, species classification</td></tr><tr><td>gaussian</td><td>Normal likelihood</td><td>Regression, residual error is also estimated (similar to <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">stats::lm()</a></code>)</td></tr><tr><td>binomial</td><td>Binomial likelihood</td><td>Classification/Logistic regression, mortality</td></tr><tr><td>poisson</td><td>Poisson likelihood</td><td>Regression, count data, e.g. species abundances</td></tr></table></div>
    <div class="section level2">
    <h2 id="training-and-convergence-of-neural-networks">Training and convergence of neural networks<a class="anchor" aria-label="anchor" href="#training-and-convergence-of-neural-networks"></a></h2>
    <p>Ensuring convergence can be tricky when training neural networks. Their training is sensitive to a combination of the learning rate (how much the weights are updated in each optimization step), the batch size (a random subset of the data is used in each optimization step), and the number of epochs (number of optimization steps). Typically, the learning rate should be decreased with the size of the neural networks (depth of the network and width of the hidden layers). We provide a baseline loss (intercept only model) that can give hints about an appropriate learning rate:</p>
<p><img src="figures/learning_rates.jpg" alt="Learning rates"></p>
<p>If the training loss of the model doesn't fall below the baseline loss, the learning rate is either too high or too low. If this happens, try higher and lower learning rates.</p>
<p>A common strategy is to try (manually) a few different learning rates to see if the learning rate is on the right scale.</p>
<p>See the troubleshooting vignette (<code><a href="../articles/B-Training_neural_networks.html">vignette("B-Training_neural_networks")</a></code>) for more help on training and debugging neural networks.</p>
    </div>
    <div class="section level2">
    <h2 id="finding-the-right-architecture">Finding the right architecture<a class="anchor" aria-label="anchor" href="#finding-the-right-architecture"></a></h2>
    <p>As with the learning rate, there is no definitive guide to choosing the right architecture for the right task. However, there are some general rules/recommendations: In general, wider, and deeper neural networks can improve generalization - but this is a double-edged sword because it also increases the risk of overfitting. So, if you increase the width and depth of the network, you should also add regularization (e.g., by increasing the lambda parameter, which corresponds to the regularization strength). Furthermore, in <a href="https://arxiv.org/abs/2306.10551" class="external-link">Pichler &amp; Hartig, 2023</a>, we investigated the effects of the hyperparameters on the prediction performance as a function of the data size. For example, we found that the <code>selu</code> activation function outperforms <code>relu</code> for small data sizes (&lt;100 observations).</p>
<p>We recommend starting with moderate sizes (like the defaults), and if the model doesn't generalize/converge, try larger networks along with a regularization that helps minimize the risk of overfitting (see <code><a href="../articles/B-Training_neural_networks.html">vignette("B-Training_neural_networks")</a></code> ).</p>
    </div>
    <div class="section level2">
    <h2 id="overfitting">Overfitting<a class="anchor" aria-label="anchor" href="#overfitting"></a></h2>
    <p>Overfitting means that the model fits the training data well, but generalizes poorly to new observations. We can use the validation argument to detect overfitting. If the validation loss starts to increase again at a certain point, it often means that the models are starting to overfit your training data:</p>
<p><img src="figures/overfitting.jpg" alt="Overfitting"></p>
<p><strong>Solutions</strong>:</p>
<ul><li><p>Re-train with epochs = point where model started to overfit</p></li>
<li><p>Early stopping, stop training when model starts to overfit, can be specified using the <code>early_stopping=…</code> argument</p></li>
<li><p>Use regularization (dropout or elastic-net, see next section)</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="regularization">Regularization<a class="anchor" aria-label="anchor" href="#regularization"></a></h2>
    <p>Elastic Net regularization combines the strengths of L1 (Lasso) and L2 (Ridge) regularization. It introduces a penalty term that encourages sparse weight values while maintaining overall weight shrinkage. By controlling the sparsity of the learned model, Elastic Net regularization helps avoid overfitting while allowing for meaningful feature selection. We advise using elastic net (e.g. lambda = 0.001 and alpha = 0.2).</p>
<p>Dropout regularization helps prevent overfitting by randomly disabling a portion of neurons during training. This technique encourages the network to learn more robust and generalized representations, as it prevents individual neurons from relying too heavily on specific input patterns. Dropout has been widely adopted as a simple yet effective regularization method in deep learning.</p>
<p>By utilizing these regularization methods in your neural network training with the cito package, you can improve generalization performance and enhance the network's ability to handle unseen data. These techniques act as valuable tools in mitigating overfitting and promoting more robust and reliable model performance.</p>
    </div>
    <div class="section level2">
    <h2 id="uncertainty">Uncertainty<a class="anchor" aria-label="anchor" href="#uncertainty"></a></h2>
    <p>We can use bootstrapping to generate uncertainties for all outputs. Bootstrapping can be enabled by setting <code>bootstrap = ...</code> to the number of bootstrap samples to be used. Note, however, that the computational cost can be excessive.</p>
<p>In some cases it may be worthwhile to parallelize bootstrapping, for example if you have a GPU and the neural network is small. Parallelization for bootstrapping can be enabled by setting the <code>bootstrap_parallel = ...</code> argument to the desired number of calls to run in parallel.</p>
    </div>
    <div class="section level2">
    <h2 id="custom-optimizer-and-learning-rate-schedulers">Custom Optimizer and Learning Rate Schedulers<a class="anchor" aria-label="anchor" href="#custom-optimizer-and-learning-rate-schedulers"></a></h2>
    <p>When training a network, you have the flexibility to customize the optimizer settings and learning rate scheduler to optimize the learning process. In the cito package, you can initialize these configurations using the <code><a href="config_lr_scheduler.html">config_lr_scheduler</a></code> and <code><a href="config_optimizer.html">config_optimizer</a></code> functions.</p>
<p><code><a href="config_lr_scheduler.html">config_lr_scheduler</a></code> allows you to define a specific learning rate scheduler that controls how the learning rate changes over time during training. This is beneficial in scenarios where you want to adaptively adjust the learning rate to improve convergence or avoid getting stuck in local optima.</p>
<p>Similarly, the <code><a href="config_optimizer.html">config_optimizer</a></code> function enables you to specify the optimizer for your network. Different optimizers, such as stochastic gradient descent (SGD), Adam, or RMSprop, offer various strategies for updating the network's weights and biases during training. Choosing the right optimizer can significantly impact the training process and the final performance of your neural network.</p>
    </div>
    <div class="section level2">
    <h2 id="how-neural-networks-work">How neural networks work<a class="anchor" aria-label="anchor" href="#how-neural-networks-work"></a></h2>
    <p>In Multilayer Perceptron (MLP) networks, each neuron is connected to every neuron in the previous layer and every neuron in the subsequent layer. The value of each neuron is computed using a weighted sum of the outputs from the previous layer, followed by the application of an activation function. Specifically, the value of a neuron is calculated as the weighted sum of the outputs of the neurons in the previous layer, combined with a bias term. This sum is then passed through an activation function, which introduces non-linearity into the network. The calculated value of each neuron becomes the input for the neurons in the next layer, and the process continues until the output layer is reached. The choice of activation function and the specific weight values determine the network's ability to learn and approximate complex relationships between inputs and outputs.</p>
<p>Therefore the value of each neuron can be calculated using: \( a (\sum_j{ w_j * a_j})\). Where \(w_j\) is the weight and \(a_j\) is the value from neuron j to the current one. a() is the activation function, e.g. \( relu(x) = max(0,x)\)</p>
    </div>
    <div class="section level2">
    <h2 id="training-on-graphic-cards">Training on graphic cards<a class="anchor" aria-label="anchor" href="#training-on-graphic-cards"></a></h2>
    <p>If you have an NVIDIA CUDA-enabled device and have installed the CUDA toolkit version 11.3 and cuDNN 8.4, you can take advantage of GPU acceleration for training your neural networks. It is crucial to have these specific versions installed, as other versions may not be compatible.
For detailed installation instructions and more information on utilizing GPUs for training, please refer to the <a href="https://torch.mlverse.org/docs/articles/installation.html" class="external-link">mlverse: 'torch' documentation</a>.</p>
<p>Note: GPU training is optional, and the package can still be used for training on CPU even without CUDA and cuDNN installations.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><code><a href="predict.citodnn.html">predict.citodnn</a></code>, <code><a href="plot.citodnn.html">plot.citodnn</a></code>,  <code><a href="coef.citodnn.html">coef.citodnn</a></code>,<code><a href="print.citodnn.html">print.citodnn</a></code>, <code><a href="summary.citodnn.html">summary.citodnn</a></code>, <code><a href="continue_training.html">continue_training</a></code>, <code><a href="analyze_training.html">analyze_training</a></code>, <code><a href="PDP.html">PDP</a></code>, <code><a href="ALE.html">ALE</a></code>,</p></div>
    </div>
    <div class="section level2">
    <h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a></h2>
    <p>Christian Amesoeder, Maximilian Pichler</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="kw">if</span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://citoverse.github.io/cito/" class="external-link">cito</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example workflow in cito</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Build and train  Network</span></span></span>
<span class="r-in"><span><span class="co">### softmax is used for multi-class responses (e.g., Species)</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu">dnn</span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>, loss <span class="op">=</span> <span class="st">"softmax"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## The training loss is below the baseline loss but at the end of the</span></span></span>
<span class="r-in"><span><span class="co">## training the loss was still decreasing, so continue training for another 50</span></span></span>
<span class="r-in"><span><span class="co">## epochs</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="continue_training.html">continue_training</a></span><span class="op">(</span><span class="va">nn.fit</span>, epochs <span class="op">=</span> <span class="fl">50L</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Sturcture of Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Plot Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## 4 Input nodes (first layer) because of 4 features</span></span></span>
<span class="r-in"><span><span class="co">## 3 Output nodes (last layer) because of 3 response species (one node for each</span></span></span>
<span class="r-in"><span><span class="co">## level in the response variable).</span></span></span>
<span class="r-in"><span><span class="co">## The layers between the input and output layer are called hidden layers (two</span></span></span>
<span class="r-in"><span><span class="co">## of them)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## We now want to understand how the predictions are made, what are the</span></span></span>
<span class="r-in"><span><span class="co">## important features? The summary function automatically calculates feature</span></span></span>
<span class="r-in"><span><span class="co">## importance (the interpretation is similar to an anova) and calculates</span></span></span>
<span class="r-in"><span><span class="co">## average conditional effects that are similar to linear effects:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## To visualize the effect (response-feature effect), we can use the ALE and</span></span></span>
<span class="r-in"><span><span class="co">## PDP functions</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Partial dependencies</span></span></span>
<span class="r-in"><span><span class="fu"><a href="PDP.html">PDP</a></span><span class="op">(</span><span class="va">nn.fit</span>, variable <span class="op">=</span> <span class="st">"Petal.Length"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Accumulated local effect plots</span></span></span>
<span class="r-in"><span><span class="fu"><a href="ALE.html">ALE</a></span><span class="op">(</span><span class="va">nn.fit</span>, variable <span class="op">=</span> <span class="st">"Petal.Length"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Per se, it is difficult to get confidence intervals for our xAI metrics (or</span></span></span>
<span class="r-in"><span><span class="co"># for the predictions). But we can use bootstrapping to obtain uncertainties</span></span></span>
<span class="r-in"><span><span class="co"># for all cito outputs:</span></span></span>
<span class="r-in"><span><span class="co">## Re-fit the neural network with bootstrapping</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu">dnn</span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="st">"softmax"</span>,</span></span>
<span class="r-in"><span>             epochs <span class="op">=</span> <span class="fl">150L</span>,</span></span>
<span class="r-in"><span>             bootstrap <span class="op">=</span> <span class="fl">20L</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## convergence can be tested via the analyze_training function</span></span></span>
<span class="r-in"><span><span class="fu"><a href="analyze_training.html">analyze_training</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Summary for xAI metrics (can take some time):</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## Now with standard errors and p-values</span></span></span>
<span class="r-in"><span><span class="co">## Note: Take the p-values with a grain of salt! We do not know yet if they are</span></span></span>
<span class="r-in"><span><span class="co">## correct (e.g. if you use regularization, they are likely conservative == too</span></span></span>
<span class="r-in"><span><span class="co">## large)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Predictions with bootstrapping:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## The first dim corresponds to the bootstrapping, if you want the average</span></span></span>
<span class="r-in"><span><span class="co">## predictions, you need to calculate the mean by your own:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">3</span>, <span class="va">mean</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Advanced: Custom loss functions and additional parameters</span></span></span>
<span class="r-in"><span><span class="co">## Normal Likelihood with sd parameter:</span></span></span>
<span class="r-in"><span><span class="va">custom_loss</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">pred</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">logLik</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/distr_normal.html" class="external-link">distr_normal</a></span><span class="op">(</span><span class="va">pred</span>,</span></span>
<span class="r-in"><span>                               scale <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_relu.html" class="external-link">nnf_relu</a></span><span class="op">(</span><span class="va">scale</span><span class="op">)</span><span class="op">+</span></span></span>
<span class="r-in"><span>                                 <span class="fl">0.001</span><span class="op">)</span><span class="op">$</span><span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">logLik</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu">dnn</span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="va">custom_loss</span>,</span></span>
<span class="r-in"><span>             custom_parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">scale</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Multivariate normal likelihood with parametrized covariance matrix</span></span></span>
<span class="r-in"><span><span class="co">## Sigma = L*L^t + D</span></span></span>
<span class="r-in"><span><span class="co">## Helper function to build covariance matrix</span></span></span>
<span class="r-in"><span><span class="va">create_cov</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">LU</span>, <span class="va">Diag</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_matmul.html" class="external-link">torch_matmul</a></span><span class="op">(</span><span class="va">LU</span>, <span class="va">LU</span><span class="op">$</span><span class="fu">t</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_diag.html" class="external-link">torch_diag</a></span><span class="op">(</span><span class="va">Diag</span><span class="op">+</span><span class="fl">0.01</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">custom_loss_MVN</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">pred</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">Sigma</span> <span class="op">=</span> <span class="fu">create_cov</span><span class="op">(</span><span class="va">SigmaPar</span>, <span class="va">SigmaDiag</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="va">logLik</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/distr_multivariate_normal.html" class="external-link">distr_multivariate_normal</a></span><span class="op">(</span><span class="va">pred</span>,</span></span>
<span class="r-in"><span>                                            covariance_matrix <span class="op">=</span> <span class="va">Sigma</span><span class="op">)</span><span class="op">$</span></span></span>
<span class="r-in"><span>    <span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">logLik</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu">dnn</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">Sepal.Length</span>, <span class="va">Sepal.Width</span>, <span class="va">Petal.Length</span><span class="op">)</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             lr <span class="op">=</span> <span class="fl">0.01</span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="va">custom_loss_MVN</span>,</span></span>
<span class="r-in"><span>             custom_parameters <span class="op">=</span></span></span>
<span class="r-in"><span>               <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>SigmaDiag <span class="op">=</span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>                    SigmaPar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">6</span>, sd <span class="op">=</span> <span class="fl">0.001</span><span class="op">)</span>, <span class="fl">3</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="fu">create_cov</span><span class="op">(</span><span class="va">nn.fit</span><span class="op">$</span><span class="va">loss</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">SigmaPar</span>,</span></span>
<span class="r-in"><span>                     <span class="va">nn.fit</span><span class="op">$</span><span class="va">loss</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">SigmaDiag</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 1.065587, lr: 0.01000</span>
<span class="r-plt img"><img src="dnn-1.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 1.045537, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 1.033668, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 1.017907, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 1.006849, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.995328, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.988243, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.970856, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.960457, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.948781, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.933681, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.922838, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.907165, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.897470, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.880641, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.864645, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.848825, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.831893, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.816971, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.804700, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.784271, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.769089, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.757345, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.742720, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.726832, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.710507, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.695839, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.684298, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.669355, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.653247, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.648130, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.628556, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.615809, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.602869, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.597741, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.583509, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.573927, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.562790, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.548485, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.545526, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.533280, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.529714, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.522611, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.512663, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.506597, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.498931, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.491755, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.486617, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.475977, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.468087, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.463612, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 0.451922, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 0.464202, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.446133, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.446746, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.438635, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 0.432133, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 0.426573, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.415083, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 0.421923, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 0.416640, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.412315, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.407844, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.402836, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 0.396606, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 0.391906, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 0.387799, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.388354, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 0.375120, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.374901, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 0.372473, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 0.379303, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.364045, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.357666, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 0.349731, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 0.361867, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 0.350933, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.344431, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 0.347441, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.338411, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.333502, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.328227, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 0.326076, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 0.326710, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.326590, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 0.316785, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.316128, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.311458, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 0.307364, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 0.306280, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 0.302568, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 0.297286, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 0.294916, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.292004, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.284879, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 0.281926, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 0.278003, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.282367, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 0.277810, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.272724, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 0.267740, lr: 0.01000</span>
<span class="r-plt img"><img src="dnn-2.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 0.263029, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 0.262375, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 0.261976, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.256125, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.251810, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.254371, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.241207, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.252270, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.240559, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.236752, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.239149, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.238607, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.236097, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.228270, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.224600, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.224357, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.224262, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.214033, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.215840, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.216228, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.215786, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.209270, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.203533, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.205215, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.204890, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.203401, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.201398, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.198252, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.202850, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.195463, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.191191, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.189134, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.193236, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.190242, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.181587, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.184684, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.185235, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.190547, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.172701, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.175559, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.173598, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.172551, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.169627, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.172806, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.174474, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.166384, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.165504, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.160572, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.168297, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> dnn(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>     Petal.Width, data = datasets::iris, loss = "softmax")</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> An `nn_module` containing 2,953 parameters.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> ── Modules ─────────────────────────────────────────────────────────────────────</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 0: &lt;nn_linear&gt; #250 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 1: &lt;nn_relu&gt; #0 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 2: &lt;nn_linear&gt; #2,550 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 3: &lt;nn_relu&gt; #0 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 4: &lt;nn_linear&gt; #153 parameters</span>
<span class="r-plt img"><img src="dnn-3.png" alt="" width="700" height="433"></span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-plt img"><img src="dnn-4.png" alt="" width="700" height="433"></span>
<span class="r-plt img"><img src="dnn-5.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 9.760445, lr: 0.01000</span>
<span class="r-plt img"><img src="dnn-6.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 3.183024, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 1.727380, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 1.524011, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 1.499740, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 1.481393, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 1.461300, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 1.443897, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 1.423697, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 1.402989, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 1.382694, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 1.362169, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 1.341399, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 1.319498, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 1.296972, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 1.274549, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 1.250271, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 1.223327, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 1.198251, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 1.172040, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 1.141806, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 1.111661, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 1.079147, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 1.044105, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 1.012417, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.982869, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.934165, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.892179, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.854077, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.809030, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.766903, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.745378, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 1.167400, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 1.827039, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.782636, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.780542, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 1.043249, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 1.335738, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.703551, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.807243, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 1.330173, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 1.267785, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 1.003416, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.675547, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.623374, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 1.922334, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.999620, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.698197, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.652502, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.706199, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 3.090413, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 0.855821, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 0.807530, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.786728, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.711848, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.734114, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 1.131146, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 1.141767, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.787600, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 1.352212, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 1.253697, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.865818, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.706952, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.676496, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 1.772869, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 1.143607, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 0.695634, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.906730, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 1.113961, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.675069, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 0.851134, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 1.817961, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.796082, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.659886, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 1.088492, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 1.205976, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 0.695424, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.732254, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 1.505544, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.932866, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.701197, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.928616, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 1.411932, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 0.718241, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.609630, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 1.990642, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.725899, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.664007, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 0.611548, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 1.306172, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 2.069035, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 0.760763, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 0.697237, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.727187, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.735940, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 1.570442, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 0.700566, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.668642, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 1.694696, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.686824, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 27.745816, lr: 0.01000</span>
<span class="r-plt img"><img src="dnn-7.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 18.821146, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 11.712875, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 6.100753, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 4.508971, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 4.326973, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 4.287146, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 4.272509, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 4.244032, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 4.228217, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 4.197692, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 4.152000, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 4.135088, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 4.124618, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 4.114032, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 4.088676, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 4.069934, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 4.048794, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 4.015899, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 4.026738, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 3.989723, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 3.981067, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 3.945836, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 3.938266, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 3.911921, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 3.905098, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 3.874997, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 3.852737, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 3.822335, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 3.807595, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 3.780057, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 3.761074, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 3.739507, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 3.711409, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 3.675645, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 3.659762, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 3.628340, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 3.592564, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 3.563512, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 3.531159, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 3.495033, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 3.465714, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 3.427635, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 3.408741, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 3.373616, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 3.330872, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 3.290766, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 3.258677, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 3.215470, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 3.179991, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 3.144733, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 3.121463, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 3.079996, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 3.034628, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 2.993763, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 2.954139, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 2.904359, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 2.857372, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 2.824324, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 2.771538, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 2.707765, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 2.684931, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 2.617567, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 2.554169, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 2.456590, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 2.474915, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 2.737766, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 2.381409, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 2.605435, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 2.269596, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 2.215162, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 3.240647, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 2.281391, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 2.188206, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 2.124397, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 2.131946, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 2.400887, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 2.024941, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 2.005982, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 2.773260, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 2.093207, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 1.992454, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 2.487759, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 2.131102, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 2.172138, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 2.385194, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 1.935449, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 2.811954, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 2.047901, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 2.160699, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 2.582674, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 1.841074, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 2.041726, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 2.427768, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 1.928738, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 2.419885, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 2.568875, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 1.860385, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 2.408194, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 2.053319, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>              [,1]         [,2]         [,3]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1,] 0.5340226889 0.0006108225 0.0006123304</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [2,] 0.0006108225 0.1961034834 0.0007730430</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [3,] 0.0006123304 0.0007730430 0.2358240932</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

