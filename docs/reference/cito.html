<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="'cito' simplifies the building and training of (deep) neural networks by relying on standard R syntax and familiar methods from statistical packages. Model creation and training can be done with a single line of code. Furthermore, all generic R methods such as print or plot can be used on the fitted model. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch' (with optional GPU support). The 'torch' package is native to R, so no Python installation or other API is required for this package."><title>'cito': Building and training neural networks — cito • cito</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="'cito': Building and training neural networks — cito"><meta property="og:description" content="'cito' simplifies the building and training of (deep) neural networks by relying on standard R syntax and familiar methods from statistical packages. Model creation and training can be done with a single line of code. Furthermore, all generic R methods such as print or plot can be used on the fitted model. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch' (with optional GPU support). The 'torch' package is native to R, so no Python installation or other API is required for this package."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a>
    <a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a>
    <a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a>
    <a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>'cito': Building and training neural networks</h1>
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/R/cito.R" class="external-link"><code>R/cito.R</code></a></small>
      <div class="d-none name"><code>cito.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>'cito' simplifies the building and training of (deep) neural networks by relying on standard R syntax and familiar methods from statistical packages. Model creation and training can be done with a single line of code. Furthermore, all generic R methods such as print or plot can be used on the fitted model. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch' (with optional GPU support). The 'torch' package is native to R, so no Python installation or other API is required for this package.</p>
    </div>


    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>Cito is built around its main function <code><a href="dnn.html">dnn</a></code>, which creates and trains a deep neural network. Various tools for analyzing the trained neural network are available.</p>
    </div>
    <div class="section level2">
    <h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a></h2>
    


<p>in order to install cito please follow these steps:</p>
<p><code>install.packages("cito")</code></p>
<p><code><a href="https://torch.mlverse.org/docs" class="external-link">library(torch)</a></code></p>
<p><code>install_torch(reinstall = TRUE)</code></p>
<p><code><a href="https://citoverse.github.io/cito/" class="external-link">library(cito)</a></code></p>
    </div>
    <div class="section level2">
    <h2 id="cito-functions-and-typical-workflow">cito functions and typical workflow<a class="anchor" aria-label="anchor" href="#cito-functions-and-typical-workflow"></a></h2>
    

<ul><li><p><code><a href="dnn.html">dnn</a></code>: train deep neural network</p></li>
<li><p><code><a href="analyze_training.html">analyze_training</a></code>: check for convergence by comparing training loss with baseline loss</p></li>
<li><p><code><a href="continue_training.html">continue_training</a></code>: continues training of an existing cito dnn model for additional epochs</p></li>
<li><p><code><a href="summary.citodnn.html">summary.citodnn</a></code>: extract xAI metrics/effects to understand how predictions are made</p></li>
<li><p><code><a href="PDP.html">PDP</a></code>: plot the partial dependency plot for a specific feature</p></li>
<li><p><code><a href="ALE.html">ALE</a></code>: plot the accumulated local effect plot for a specific feature</p></li>
</ul><p>Check out the vignettes for more details on training NN and how a typical workflow with 'cito' could look like.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="kw">if</span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://citoverse.github.io/cito/" class="external-link">cito</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example workflow in cito</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Build and train  Network</span></span></span>
<span class="r-in"><span><span class="co">### softmax is used for multi-class responses (e.g., Species)</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>, loss <span class="op">=</span> <span class="st">"softmax"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## The training loss is below the baseline loss but at the end of the</span></span></span>
<span class="r-in"><span><span class="co">## training the loss was still decreasing, so continue training for another 50</span></span></span>
<span class="r-in"><span><span class="co">## epochs</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="continue_training.html">continue_training</a></span><span class="op">(</span><span class="va">nn.fit</span>, epochs <span class="op">=</span> <span class="fl">50L</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Sturcture of Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Plot Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## 4 Input nodes (first layer) because of 4 features</span></span></span>
<span class="r-in"><span><span class="co">## 3 Output nodes (last layer) because of 3 response species (one node for each</span></span></span>
<span class="r-in"><span><span class="co">## level in the response variable).</span></span></span>
<span class="r-in"><span><span class="co">## The layers between the input and output layer are called hidden layers (two</span></span></span>
<span class="r-in"><span><span class="co">## of them)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## We now want to understand how the predictions are made, what are the</span></span></span>
<span class="r-in"><span><span class="co">## important features? The summary function automatically calculates feature</span></span></span>
<span class="r-in"><span><span class="co">## importance (the interpretation is similar to an anova) and calculates</span></span></span>
<span class="r-in"><span><span class="co">## average conditional effects that are similar to linear effects:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## To visualize the effect (response-feature effect), we can use the ALE and</span></span></span>
<span class="r-in"><span><span class="co">## PDP functions</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Partial dependencies</span></span></span>
<span class="r-in"><span><span class="fu"><a href="PDP.html">PDP</a></span><span class="op">(</span><span class="va">nn.fit</span>, variable <span class="op">=</span> <span class="st">"Petal.Length"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Accumulated local effect plots</span></span></span>
<span class="r-in"><span><span class="fu"><a href="ALE.html">ALE</a></span><span class="op">(</span><span class="va">nn.fit</span>, variable <span class="op">=</span> <span class="st">"Petal.Length"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Per se, it is difficult to get confidence intervals for our xAI metrics (or</span></span></span>
<span class="r-in"><span><span class="co"># for the predictions). But we can use bootstrapping to obtain uncertainties</span></span></span>
<span class="r-in"><span><span class="co"># for all cito outputs:</span></span></span>
<span class="r-in"><span><span class="co">## Re-fit the neural network with bootstrapping</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="st">"softmax"</span>,</span></span>
<span class="r-in"><span>             epochs <span class="op">=</span> <span class="fl">150L</span>,</span></span>
<span class="r-in"><span>             bootstrap <span class="op">=</span> <span class="fl">20L</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## convergence can be tested via the analyze_training function</span></span></span>
<span class="r-in"><span><span class="fu"><a href="analyze_training.html">analyze_training</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Summary for xAI metrics (can take some time):</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## Now with standard errors and p-values</span></span></span>
<span class="r-in"><span><span class="co">## Note: Take the p-values with a grain of salt! We do not know yet if they are</span></span></span>
<span class="r-in"><span><span class="co">## correct (e.g. if you use regularization, they are likely conservative == too</span></span></span>
<span class="r-in"><span><span class="co">## large)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Predictions with bootstrapping:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## The first dim corresponds to the bootstrapping, if you want the average</span></span></span>
<span class="r-in"><span><span class="co">## predictions, you need to calculate the mean by your own:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">3</span>, <span class="va">mean</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Advanced: Custom loss functions and additional parameters</span></span></span>
<span class="r-in"><span><span class="co">## Normal Likelihood with sd parameter:</span></span></span>
<span class="r-in"><span><span class="va">custom_loss</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">pred</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">logLik</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/distr_normal.html" class="external-link">distr_normal</a></span><span class="op">(</span><span class="va">pred</span>,</span></span>
<span class="r-in"><span>                               scale <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/nnf_relu.html" class="external-link">nnf_relu</a></span><span class="op">(</span><span class="va">scale</span><span class="op">)</span><span class="op">+</span></span></span>
<span class="r-in"><span>                                 <span class="fl">0.001</span><span class="op">)</span><span class="op">$</span><span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">logLik</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="va">custom_loss</span>,</span></span>
<span class="r-in"><span>             custom_parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">scale</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Multivariate normal likelihood with parametrized covariance matrix</span></span></span>
<span class="r-in"><span><span class="co">## Sigma = L*L^t + D</span></span></span>
<span class="r-in"><span><span class="co">## Helper function to build covariance matrix</span></span></span>
<span class="r-in"><span><span class="va">create_cov</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">LU</span>, <span class="va">Diag</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_matmul.html" class="external-link">torch_matmul</a></span><span class="op">(</span><span class="va">LU</span>, <span class="va">LU</span><span class="op">$</span><span class="fu">t</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_diag.html" class="external-link">torch_diag</a></span><span class="op">(</span><span class="va">Diag</span><span class="op">+</span><span class="fl">0.01</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">custom_loss_MVN</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">pred</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">Sigma</span> <span class="op">=</span> <span class="fu">create_cov</span><span class="op">(</span><span class="va">SigmaPar</span>, <span class="va">SigmaDiag</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="va">logLik</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/distr_multivariate_normal.html" class="external-link">distr_multivariate_normal</a></span><span class="op">(</span><span class="va">pred</span>,</span></span>
<span class="r-in"><span>                                            covariance_matrix <span class="op">=</span> <span class="va">Sigma</span><span class="op">)</span><span class="op">$</span></span></span>
<span class="r-in"><span>    <span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">logLik</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">Sepal.Length</span>, <span class="va">Sepal.Width</span>, <span class="va">Petal.Length</span><span class="op">)</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             lr <span class="op">=</span> <span class="fl">0.01</span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="va">custom_loss_MVN</span>,</span></span>
<span class="r-in"><span>             custom_parameters <span class="op">=</span></span></span>
<span class="r-in"><span>               <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>SigmaDiag <span class="op">=</span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>                    SigmaPar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">6</span>, sd <span class="op">=</span> <span class="fl">0.001</span><span class="op">)</span>, <span class="fl">3</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="fu">create_cov</span><span class="op">(</span><span class="va">nn.fit</span><span class="op">$</span><span class="va">loss</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">SigmaPar</span>,</span></span>
<span class="r-in"><span>                     <span class="va">nn.fit</span><span class="op">$</span><span class="va">loss</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">SigmaDiag</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 1.065049, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-1.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 1.027628, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 1.004477, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 0.983245, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 0.967963, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.948421, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.931273, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.916006, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.895298, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.882443, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.860543, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.843392, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.828311, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.802579, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.784417, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.767315, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.750361, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.731836, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.715122, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.698683, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.679742, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.664098, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.650354, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.632398, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.620713, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.603727, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.596997, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.585469, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.567929, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.553626, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.542136, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.542455, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.518536, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.518194, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.507935, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.510277, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.495337, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.487845, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.478105, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.469434, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.465439, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.455091, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.449910, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.443794, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.435259, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.430625, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.428497, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.425301, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.419522, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.414512, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.403484, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 0.412761, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 0.395700, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.394916, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.394260, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.387197, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 0.383012, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 0.379242, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.374052, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 0.374783, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 0.368747, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.360053, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.361000, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.362994, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 0.353772, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 0.344994, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 0.338481, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.341143, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 0.346832, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.334854, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 0.343439, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 0.324088, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.320237, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.331182, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 0.315901, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 0.312287, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 0.312442, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.313917, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 0.301025, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.300277, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.292553, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.291293, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 0.287825, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 0.286039, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.282651, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 0.276298, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.284066, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.275559, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 0.265755, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 0.268388, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 0.262847, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 0.263303, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 0.263202, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.257293, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.249757, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 0.250012, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 0.252101, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.245484, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 0.247632, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.235264, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 0.238877, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-2.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 0.238377, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 0.233425, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 0.230803, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.225427, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.223351, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.221283, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.233539, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.230145, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.230027, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.214036, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.214557, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.219384, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.211438, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.208443, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.227504, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.220256, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.209368, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.197746, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.202060, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.200072, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.197821, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.196539, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.198681, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.189142, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.188034, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.199795, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.188171, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.179839, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.182780, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.178533, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.182867, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.177350, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.184642, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.171863, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.174983, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.170033, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.179699, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.164375, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.168562, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.164956, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.172058, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.163846, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.163793, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.159261, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.164122, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.160498, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.169673, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.160149, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.155777, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> dnn(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>     Petal.Width, data = datasets::iris, loss = "softmax")</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> An `nn_module` containing 2,953 parameters.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> ── Modules ─────────────────────────────────────────────────────────────────────</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 0: &lt;nn_linear&gt; #250 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 1: &lt;nn_relu&gt; #0 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 2: &lt;nn_linear&gt; #2,550 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 3: &lt;nn_relu&gt; #0 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 4: &lt;nn_linear&gt; #153 parameters</span>
<span class="r-plt img"><img src="cito-3.png" alt="" width="700" height="433"></span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-plt img"><img src="cito-4.png" alt="" width="700" height="433"></span>
<span class="r-plt img"><img src="cito-5.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 10.074408, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-6.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 4.250164, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 2.277819, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 1.692868, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 1.622443, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 1.604075, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 1.588198, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 1.570633, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 1.551531, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 1.533990, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 1.517699, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 1.500496, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 1.479889, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 1.460220, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 1.438098, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 1.418953, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 1.398857, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 1.375629, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 1.353491, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 1.329815, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 1.307210, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 1.279889, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 1.256068, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 1.226760, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 1.200132, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 1.173620, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 1.140928, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 1.109399, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 1.071665, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 1.040869, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 1.001738, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.966473, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.924403, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.879835, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.848468, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.847554, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.741719, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 1.000494, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 1.705277, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.750244, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.726829, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 1.223134, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 1.750370, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.879562, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.875529, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 1.065595, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.959944, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 1.274454, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 1.012278, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.971622, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.718819, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 1.292465, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 1.575427, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.778499, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.686147, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.676577, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 1.844583, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 3.018168, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.949146, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 0.902004, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 0.867256, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.811246, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.757786, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.703904, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 0.698468, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 1.064214, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 1.572272, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.710424, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 0.641817, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.647672, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 2.278164, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 1.098815, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.733263, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.672230, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 0.599592, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 0.533639, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 3.155525, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.862835, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 0.812207, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.765564, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.733322, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.655458, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 1.035781, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 1.808634, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.719768, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 0.660526, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.709132, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.981133, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 1.698339, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 0.738942, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 0.632876, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 1.506914, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 1.005981, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.630742, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.586460, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 0.929027, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 2.769915, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.806260, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 0.749866, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.692758, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 23.848149, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-7.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 10.724342, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 5.022502, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 4.609323, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 4.463349, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 4.377104, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 4.297513, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 4.247162, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 4.157361, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 4.125531, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 4.035927, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 4.006950, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 3.957924, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 3.896683, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 3.874329, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 3.825920, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 3.812213, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 3.738711, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 3.698074, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 3.667172, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 3.629838, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 3.587143, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 3.532552, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 3.500178, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 3.459958, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 3.417570, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 3.369418, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 3.339876, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 3.292446, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 3.249409, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 3.205774, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 3.168814, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 3.119734, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 3.092095, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 3.048394, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 3.001948, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 2.963142, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 2.929680, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 2.895639, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 2.839630, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 2.796179, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 2.755159, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 2.700162, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 2.679659, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 2.607045, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 2.561079, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 2.508246, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 2.439914, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 2.358980, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 2.292836, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 2.231887, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 2.129829, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 2.051687, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 1.995867, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 3.824277, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 2.175135, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 2.135261, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 2.429566, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 2.169004, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 2.074759, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 2.710105, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 2.290439, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 1.962069, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 1.984866, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 2.929019, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 2.371087, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 1.984282, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 2.371584, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 2.722491, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 1.854833, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 2.200941, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 1.874055, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 2.326036, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 2.048599, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 2.830273, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 1.944944, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 1.995249, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 1.936717, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 2.783663, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 2.314756, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 2.377834, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 1.804115, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 3.091102, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 1.751989, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 1.862302, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 3.255487, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 1.832299, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 1.817727, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 2.268558, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 2.675754, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 1.650740, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 1.847586, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 2.281655, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 2.056749, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 1.739586, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 1.815763, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 2.436647, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 1.525938, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 1.700433, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 2.275153, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>            [,1]       [,2]       [,3]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1,] 0.33559042 0.06044896 0.09379424</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [2,] 0.06044896 0.22351924 0.04387569</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [3,] 0.09379424 0.04387569 0.21925192</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

