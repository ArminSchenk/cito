<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>'cito': Building and training neural networks — cito • cito</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="'cito': Building and training neural networks — cito"><meta name="description" content="The 'cito' package provides a user-friendly interface for training and interpreting deep neural networks (DNN). 'cito' simplifies the fitting of DNNs by supporting the familiar formula syntax, hyperparameter tuning under cross-validation, and helps to detect and handle convergence problems.  DNNs can be trained on CPU, GPU and MacOS GPUs. In addition, 'cito' has many downstream functionalities such as various explainable AI (xAI) metrics (e.g. variable importance, partial dependence plots, accumulated local effect plots, and effect estimates) to interpret trained DNNs. 'cito' optionally provides confidence intervals (and p-values) for all xAI metrics and predictions. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch'. The 'torch' package is native to R, so no Python installation or other API is required for this package."><meta property="og:description" content="The 'cito' package provides a user-friendly interface for training and interpreting deep neural networks (DNN). 'cito' simplifies the fitting of DNNs by supporting the familiar formula syntax, hyperparameter tuning under cross-validation, and helps to detect and handle convergence problems.  DNNs can be trained on CPU, GPU and MacOS GPUs. In addition, 'cito' has many downstream functionalities such as various explainable AI (xAI) metrics (e.g. variable importance, partial dependence plots, accumulated local effect plots, and effect estimates) to interpret trained DNNs. 'cito' optionally provides confidence intervals (and p-values) for all xAI metrics and predictions. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch'. The 'torch' package is native to R, so no Python installation or other API is required for this package."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a></li>
    <li><a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a></li>
    <li><a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a></li>
    <li><a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a></li>
    <li><a class="dropdown-item" href="../articles/E-CNN_and_MMN.html">Convultions neural networks and Multi modal neural networks</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>'cito': Building and training neural networks</h1>
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/R/cito.R" class="external-link"><code>R/cito.R</code></a></small>
      <div class="d-none name"><code>cito.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>The 'cito' package provides a user-friendly interface for training and interpreting deep neural networks (DNN). 'cito' simplifies the fitting of DNNs by supporting the familiar formula syntax, hyperparameter tuning under cross-validation, and helps to detect and handle convergence problems.  DNNs can be trained on CPU, GPU and MacOS GPUs. In addition, 'cito' has many downstream functionalities such as various explainable AI (xAI) metrics (e.g. variable importance, partial dependence plots, accumulated local effect plots, and effect estimates) to interpret trained DNNs. 'cito' optionally provides confidence intervals (and p-values) for all xAI metrics and predictions. At the same time, 'cito' is computationally efficient because it is based on the deep learning framework 'torch'. The 'torch' package is native to R, so no Python installation or other API is required for this package.</p>
    </div>


    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>Cito is built around its main function <code><a href="dnn.html">dnn</a></code>, which creates and trains a deep neural network. Various tools for analyzing the trained neural network are available.</p>
    </div>
    <div class="section level2">
    <h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a></h2>



<p>in order to install cito please follow these steps:</p>
<p><code>install.packages("cito")</code></p>
<p><code><a href="https://torch.mlverse.org/docs" class="external-link">library(torch)</a></code></p>
<p><code>install_torch(reinstall = TRUE)</code></p>
<p><code><a href="https://citoverse.github.io/cito/" class="external-link">library(cito)</a></code></p>
    </div>
    <div class="section level2">
    <h2 id="cito-functions-and-typical-workflow">cito functions and typical workflow<a class="anchor" aria-label="anchor" href="#cito-functions-and-typical-workflow"></a></h2>


<ul><li><p><code><a href="dnn.html">dnn</a></code>: train deep neural network</p></li>
<li><p><code><a href="analyze_training.html">analyze_training</a></code>: check for convergence by comparing training loss with baseline loss</p></li>
<li><p><code><a href="continue_training.html">continue_training</a></code>: continues training of an existing cito dnn model for additional epochs</p></li>
<li><p><code><a href="summary.citodnn.html">summary.citodnn</a></code>: extract xAI metrics/effects to understand how predictions are made</p></li>
<li><p><code><a href="PDP.html">PDP</a></code>: plot the partial dependency plot for a specific feature</p></li>
<li><p><code><a href="ALE.html">ALE</a></code>: plot the accumulated local effect plot for a specific feature</p></li>
</ul><p>Check out the vignettes for more details on training NN and how a typical workflow with 'cito' could look like.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>Useful links:</p><ul><li><p><a href="https://citoverse.github.io/cito/" class="external-link">https://citoverse.github.io/cito/</a></p></li>
<li><p>Report bugs at <a href="https://github.com/citoverse/cito/issues" class="external-link">https://github.com/citoverse/cito/issues</a></p></li>
</ul></div>
    </div>
    <div class="section level2">
    <h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a></h2>
    <p><strong>Maintainer</strong>: Maximilian Pichler <a href="mailto:maximilian.pichler@biologie.uni-regensburg.de">maximilian.pichler@biologie.uni-regensburg.de</a> (<a href="https://orcid.org/0000-0003-2252-8327" class="external-link">ORCID</a>)</p>
<p>Authors:</p><ul><li><p>Christian Amesöder <a href="mailto:Christian.Amesoeder@informatik.uni-regensburg.de">Christian.Amesoeder@informatik.uni-regensburg.de</a></p></li>
</ul><p>Other contributors:</p><ul><li><p>Florian Hartig <a href="mailto:florian.hartig@biologie.uni-regensburg.de">florian.hartig@biologie.uni-regensburg.de</a> (<a href="https://orcid.org/0000-0002-6255-9059" class="external-link">ORCID</a>) [contributor]</p></li>
<li><p>Armin Schenk <a href="mailto:armin.schenk99@gmail.com">armin.schenk99@gmail.com</a> [contributor]</p></li>
</ul></div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="kw">if</span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://citoverse.github.io/cito/" class="external-link">cito</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example workflow in cito</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Build and train  Network</span></span></span>
<span class="r-in"><span><span class="co">### softmax is used for multi-class responses (e.g., Species)</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>, loss <span class="op">=</span> <span class="st">"softmax"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## The training loss is below the baseline loss but at the end of the</span></span></span>
<span class="r-in"><span><span class="co">## training the loss was still decreasing, so continue training for another 50</span></span></span>
<span class="r-in"><span><span class="co">## epochs</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="continue_training.html">continue_training</a></span><span class="op">(</span><span class="va">nn.fit</span>, epochs <span class="op">=</span> <span class="fl">50L</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Sturcture of Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Plot Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## 4 Input nodes (first layer) because of 4 features</span></span></span>
<span class="r-in"><span><span class="co">## 3 Output nodes (last layer) because of 3 response species (one node for each</span></span></span>
<span class="r-in"><span><span class="co">## level in the response variable).</span></span></span>
<span class="r-in"><span><span class="co">## The layers between the input and output layer are called hidden layers (two</span></span></span>
<span class="r-in"><span><span class="co">## of them)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## We now want to understand how the predictions are made, what are the</span></span></span>
<span class="r-in"><span><span class="co">## important features? The summary function automatically calculates feature</span></span></span>
<span class="r-in"><span><span class="co">## importance (the interpretation is similar to an anova) and calculates</span></span></span>
<span class="r-in"><span><span class="co">## average conditional effects that are similar to linear effects:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## To visualize the effect (response-feature effect), we can use the ALE and</span></span></span>
<span class="r-in"><span><span class="co">## PDP functions</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Partial dependencies</span></span></span>
<span class="r-in"><span><span class="fu"><a href="PDP.html">PDP</a></span><span class="op">(</span><span class="va">nn.fit</span>, variable <span class="op">=</span> <span class="st">"Petal.Length"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Accumulated local effect plots</span></span></span>
<span class="r-in"><span><span class="fu"><a href="ALE.html">ALE</a></span><span class="op">(</span><span class="va">nn.fit</span>, variable <span class="op">=</span> <span class="st">"Petal.Length"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Per se, it is difficult to get confidence intervals for our xAI metrics (or</span></span></span>
<span class="r-in"><span><span class="co"># for the predictions). But we can use bootstrapping to obtain uncertainties</span></span></span>
<span class="r-in"><span><span class="co"># for all cito outputs:</span></span></span>
<span class="r-in"><span><span class="co">## Re-fit the neural network with bootstrapping</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="st">"softmax"</span>,</span></span>
<span class="r-in"><span>             epochs <span class="op">=</span> <span class="fl">150L</span>,</span></span>
<span class="r-in"><span>             verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>             bootstrap <span class="op">=</span> <span class="fl">20L</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## convergence can be tested via the analyze_training function</span></span></span>
<span class="r-in"><span><span class="fu"><a href="analyze_training.html">analyze_training</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Summary for xAI metrics (can take some time):</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## Now with standard errors and p-values</span></span></span>
<span class="r-in"><span><span class="co">## Note: Take the p-values with a grain of salt! We do not know yet if they are</span></span></span>
<span class="r-in"><span><span class="co">## correct (e.g. if you use regularization, they are likely conservative == too</span></span></span>
<span class="r-in"><span><span class="co">## large)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Predictions with bootstrapping:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## predictions are by default averaged (over the bootstrap samples)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Multinomial and conditional logit regression</span></span></span>
<span class="r-in"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">iris</span>, loss <span class="op">=</span> <span class="st">"clogit"</span>, lr <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">iris</span>, loss <span class="op">=</span> <span class="st">"multinomial"</span>, lr <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">Y</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu">stats</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/stats/Multinom.html" class="external-link">rmultinom</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">10</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">X1</span>, <span class="va">X2</span>, <span class="va">X3</span><span class="op">)</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">Y</span>, A <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, loss <span class="op">=</span> <span class="st">"multinomial"</span>, lr <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## conditional logit for size &gt; 1 is not supported yet</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Hyperparameter tuning (experimental feature)</span></span></span>
<span class="r-in"><span><span class="va">hidden_values</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">2</span>,</span></span>
<span class="r-in"><span>                         <span class="fl">4</span>, <span class="fl">2</span>,</span></span>
<span class="r-in"><span>                         <span class="fl">10</span>,<span class="fl">2</span>,</span></span>
<span class="r-in"><span>                         <span class="fl">15</span>,<span class="fl">2</span><span class="op">)</span>, <span class="fl">4</span>, <span class="fl">2</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## Potential architectures we want to test, first column == number of nodes</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">hidden_values</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span> <span class="op">=</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="va">iris</span>,</span></span>
<span class="r-in"><span>             epochs <span class="op">=</span> <span class="fl">30L</span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="st">"softmax"</span>,</span></span>
<span class="r-in"><span>             hidden <span class="op">=</span> <span class="fu"><a href="tune.html">tune</a></span><span class="op">(</span>values <span class="op">=</span> <span class="va">hidden_values</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>             lr <span class="op">=</span> <span class="fu"><a href="tune.html">tune</a></span><span class="op">(</span><span class="fl">0.00001</span>, <span class="fl">0.1</span><span class="op">)</span> <span class="co"># tune lr between range 0.00001 and 0.1</span></span></span>
<span class="r-in"><span>             <span class="op">)</span></span></span>
<span class="r-in"><span><span class="co">## Tuning results:</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">$</span><span class="va">tuning</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># test = Inf means that tuning was cancelled after only one fit (within the CV)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Advanced: Custom loss functions and additional parameters</span></span></span>
<span class="r-in"><span><span class="co">## Normal Likelihood with sd parameter:</span></span></span>
<span class="r-in"><span><span class="va">custom_loss</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">pred</span>, <span class="va">true</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">logLik</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/distr_normal.html" class="external-link">distr_normal</a></span><span class="op">(</span><span class="va">pred</span>,</span></span>
<span class="r-in"><span>                               scale <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/nnf_relu.html" class="external-link">nnf_relu</a></span><span class="op">(</span><span class="va">scale</span><span class="op">)</span><span class="op">+</span></span></span>
<span class="r-in"><span>                                 <span class="fl">0.001</span><span class="op">)</span><span class="op">$</span><span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">logLik</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="va">custom_loss</span>,</span></span>
<span class="r-in"><span>             verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>             custom_parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">scale</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Multivariate normal likelihood with parametrized covariance matrix</span></span></span>
<span class="r-in"><span><span class="co">## Sigma = L*L^t + D</span></span></span>
<span class="r-in"><span><span class="co">## Helper function to build covariance matrix</span></span></span>
<span class="r-in"><span><span class="va">create_cov</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">LU</span>, <span class="va">Diag</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_matmul.html" class="external-link">torch_matmul</a></span><span class="op">(</span><span class="va">LU</span>, <span class="va">LU</span><span class="op">$</span><span class="fu">t</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_diag.html" class="external-link">torch_diag</a></span><span class="op">(</span><span class="va">Diag</span><span class="op">$</span><span class="fu">exp</span><span class="op">(</span><span class="op">)</span><span class="op">+</span><span class="fl">0.01</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">custom_loss_MVN</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">pred</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">Sigma</span> <span class="op">=</span> <span class="fu">create_cov</span><span class="op">(</span><span class="va">SigmaPar</span>, <span class="va">SigmaDiag</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="va">logLik</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/distr_multivariate_normal.html" class="external-link">distr_multivariate_normal</a></span><span class="op">(</span><span class="va">pred</span>,</span></span>
<span class="r-in"><span>                                            covariance_matrix <span class="op">=</span> <span class="va">Sigma</span><span class="op">)</span><span class="op">$</span></span></span>
<span class="r-in"><span>    <span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="op">-</span><span class="va">logLik</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">nn.fit</span><span class="op">&lt;-</span> <span class="fu"><a href="dnn.html">dnn</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">Sepal.Length</span>, <span class="va">Sepal.Width</span>, <span class="va">Petal.Length</span><span class="op">)</span><span class="op">~</span><span class="va">.</span>,</span></span>
<span class="r-in"><span>             data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span></span>
<span class="r-in"><span>             lr <span class="op">=</span> <span class="fl">0.01</span>,</span></span>
<span class="r-in"><span>             verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span></span>
<span class="r-in"><span>             loss <span class="op">=</span> <span class="va">custom_loss_MVN</span>,</span></span>
<span class="r-in"><span>             custom_parameters <span class="op">=</span></span></span>
<span class="r-in"><span>               <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>SigmaDiag <span class="op">=</span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>                    SigmaPar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">6</span>, sd <span class="op">=</span> <span class="fl">0.001</span><span class="op">)</span>, <span class="fl">3</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="fu">create_cov</span><span class="op">(</span><span class="va">nn.fit</span><span class="op">$</span><span class="va">loss</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">SigmaPar</span>,</span></span>
<span class="r-in"><span>                     <span class="va">nn.fit</span><span class="op">$</span><span class="va">loss</span><span class="op">$</span><span class="va">parameter</span><span class="op">$</span><span class="va">SigmaDiag</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 1.114507, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-1.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 0.921588, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 0.802500, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 0.722514, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 0.651183, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.597195, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.559294, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.522748, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.479563, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.446431, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.423882, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.397875, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.390342, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.379736, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.348979, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.349096, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.311746, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.314781, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.286511, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.275567, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.269158, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.252210, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.247484, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.251595, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.235704, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.224738, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.219063, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.208005, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.193599, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.190068, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.234760, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.183407, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.173909, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.176590, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.165226, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.169586, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.187235, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.148333, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.154191, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.154290, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.141859, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.140341, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.135480, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.139144, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.138702, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.126017, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.125611, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.134287, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.120630, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.127448, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.118732, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 0.112829, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 0.116284, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.106568, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.150816, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.121862, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 0.139179, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 0.122244, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.106601, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 0.105961, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 0.094644, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.118553, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.101125, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.093866, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 0.100464, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 0.105228, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 0.093940, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.094464, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 0.092658, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.111676, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 0.102666, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 0.096125, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.104300, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.089672, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 0.088387, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 0.090926, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 0.095100, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.084575, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 0.109941, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.098977, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.089755, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.097087, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 0.085874, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 0.080107, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.081135, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 0.093528, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.085032, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.098563, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 0.103210, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 0.083423, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 0.085040, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 0.084034, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 0.130019, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.089831, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.069158, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 0.087342, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 0.072387, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.071732, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 0.082378, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.078333, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 101: 0.098652, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-2.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 102: 0.078609, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 103: 0.080407, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 104: 0.071869, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 105: 0.084137, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 106: 0.091813, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 107: 0.078743, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 108: 0.088892, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 109: 0.104082, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 110: 0.068768, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 111: 0.088212, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 112: 0.081445, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 113: 0.074226, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 114: 0.075458, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 115: 0.093506, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 116: 0.075530, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 117: 0.074877, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 118: 0.076698, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 119: 0.081380, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 120: 0.074709, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 121: 0.086922, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 122: 0.067865, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 123: 0.083059, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 124: 0.071605, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 125: 0.084262, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 126: 0.077469, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 127: 0.063985, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 128: 0.082847, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 129: 0.104097, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 130: 0.063463, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 131: 0.070642, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 132: 0.066022, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 133: 0.082793, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 134: 0.073286, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 135: 0.069963, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 136: 0.067032, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 137: 0.062976, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 138: 0.085282, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 139: 0.083725, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 140: 0.081426, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 141: 0.060674, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 142: 0.069499, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 143: 0.072004, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 144: 0.074019, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 145: 0.072269, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 146: 0.075942, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 147: 0.062878, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 148: 0.070208, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 149: 0.103406, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 150: 0.070684, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> dnn(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>     Petal.Width, data = datasets::iris, loss = "softmax")</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> An `nn_module` containing 2,953 parameters.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> ── Modules ─────────────────────────────────────────────────────────────────────</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 0: &lt;nn_linear&gt; #250 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 1: &lt;nn_selu&gt; #0 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 2: &lt;nn_linear&gt; #2,550 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 3: &lt;nn_selu&gt; #0 parameters</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> • 4: &lt;nn_linear&gt; #153 parameters</span>
<span class="r-plt img"><img src="cito-3.png" alt="" width="700" height="433"></span>
<span class="r-plt img"><img src="cito-4.png" alt="" width="700" height="433"></span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-msg co"><span class="r-pr">#&gt;</span> Number of Neighborhoods reduced to 8</span>
<span class="r-plt img"><img src="cito-5.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 0.643897, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-6.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 0.555745, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 0.512150, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 0.474177, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 0.443038, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.416772, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.398623, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.372028, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.354860, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.340035, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.322298, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.319518, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.304396, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.303584, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.286626, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.275044, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.266997, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.260079, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.252994, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.251043, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.239745, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.233857, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.222980, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.219172, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.213398, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.207827, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.200886, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.194247, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.191512, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.184514, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.181743, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.180079, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.174511, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.164900, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.162060, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.159814, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.153084, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.147497, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.148244, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.145539, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.137232, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.141704, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.133631, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.130067, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.135948, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.125341, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.127492, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.122133, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.119526, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.119513, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.113398, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 0.111348, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 0.106891, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.108192, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.109036, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.104111, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 0.101453, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 0.098868, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.097930, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 0.096846, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 0.094722, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.096138, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.092181, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.096568, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 0.090962, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 0.091709, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 0.085122, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.084825, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 0.089362, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.083973, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 0.082875, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 0.083606, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.080940, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.080260, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 0.078783, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 0.079708, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 0.074265, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.078397, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 0.084753, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.082464, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.073646, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.074243, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 0.074324, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 0.078233, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.075044, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 0.073955, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.081271, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.066154, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 0.073000, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 0.069090, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 0.070269, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 0.068443, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 0.071502, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.069543, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.069553, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 0.069503, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 0.063291, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.066313, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 0.063259, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.066227, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 0.960986, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-7.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 0.781178, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 0.686269, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 0.598250, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 0.531461, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 0.500856, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 0.454028, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 0.425335, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 0.411643, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 0.401492, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 0.362653, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 0.343971, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 0.331653, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 0.319508, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 0.297470, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 0.280757, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 0.285967, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 0.267452, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 0.256154, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 0.283718, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 0.232927, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 0.225556, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 0.208946, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 0.201307, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 0.198571, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 0.212871, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 0.191037, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 0.183692, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 0.183684, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 0.172652, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 0.158642, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 0.153388, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 0.159529, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 0.155886, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 0.141287, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 0.148917, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 0.152619, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 0.134613, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 0.135044, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 0.127126, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 0.129378, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 0.126594, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 0.121052, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 0.124277, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 0.122621, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 0.115365, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 0.112895, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 0.125984, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 0.121907, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 0.109851, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 0.107121, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 0.107226, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 0.105890, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 0.116141, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 0.111994, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 0.102944, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 0.108748, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 0.108930, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 0.111944, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 0.093649, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 0.098714, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 0.094321, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 0.099834, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 0.109629, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 0.088243, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 0.096260, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 0.094974, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 0.117039, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 0.087526, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 0.104483, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 0.084531, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 0.088917, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 0.085176, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 0.087149, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 0.105903, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 0.084588, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 0.096303, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 0.082790, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 0.091180, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 0.079777, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 0.079841, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 0.083883, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 0.075314, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 0.098915, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 0.087546, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 0.073237, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 0.073049, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 0.080827, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 0.088070, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 0.074319, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 0.093662, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 0.086580, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 0.084527, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 0.076824, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 0.086306, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 0.071252, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 0.082253, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 0.073169, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 0.100330, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 0.068483, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 1: 3.745124, lr: 0.01000</span>
<span class="r-plt img"><img src="cito-8.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 2: 3.328251, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 3: 3.302773, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 4: 3.293567, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 5: 3.296934, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 6: 3.277131, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 7: 3.276492, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 8: 3.265617, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 9: 3.264305, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 10: 3.262001, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 11: 3.250504, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 12: 3.241606, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 13: 3.241011, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 14: 3.225572, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 15: 3.215665, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 16: 3.216906, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 17: 3.199386, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 18: 3.188991, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 19: 3.178707, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 20: 3.168317, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 21: 3.153696, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 22: 3.140789, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 23: 3.122335, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 24: 3.112207, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 25: 3.092271, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 26: 3.083137, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 27: 3.062716, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 28: 3.036073, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 29: 3.025473, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 30: 3.009989, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 31: 2.979538, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 32: 2.960473, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 33: 2.930881, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 34: 2.911455, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 35: 2.890503, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 36: 2.876068, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 37: 2.846273, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 38: 2.828163, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 39: 2.808601, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 40: 2.793963, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 41: 2.776763, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 42: 2.757956, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 43: 2.742979, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 44: 2.734031, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 45: 2.718337, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 46: 2.714531, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 47: 2.698085, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 48: 2.690153, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 49: 2.681735, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 50: 2.669424, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 51: 2.661329, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 52: 2.652574, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 53: 2.644271, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 54: 2.636587, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 55: 2.626241, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 56: 2.621020, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 57: 2.614562, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 58: 2.603937, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 59: 2.598161, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 60: 2.589479, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 61: 2.577862, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 62: 2.571892, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 63: 2.561261, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 64: 2.553368, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 65: 2.540743, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 66: 2.531925, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 67: 2.522212, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 68: 2.511969, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 69: 2.503509, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 70: 2.491279, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 71: 2.481796, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 72: 2.467680, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 73: 2.457318, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 74: 2.446015, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 75: 2.440097, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 76: 2.423248, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 77: 2.411274, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 78: 2.402145, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 79: 2.395086, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 80: 2.379084, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 81: 2.369966, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 82: 2.360886, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 83: 2.353713, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 84: 2.335225, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 85: 2.335493, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 86: 2.327543, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 87: 2.317135, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 88: 2.311433, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 89: 2.303082, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 90: 2.295989, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 91: 2.289847, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 92: 2.283935, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 93: 2.278621, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 94: 2.270831, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 95: 2.267579, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 96: 2.264175, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 97: 2.260736, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 98: 2.255387, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 99: 2.251438, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Loss at epoch 100: 2.247267, lr: 0.01000</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>      [,1] [,2]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1,]    5    2</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [2,]    4    2</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [3,]   10    2</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [4,]   15    2</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Starting hyperparameter tuning...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Fitting final model...</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #949494;"># A tibble: 10 × 6</span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>    steps  test train models hidden         lr</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;lgl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 1</span>     1  51.3     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.016<span style="text-decoration: underline;">8</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 2</span>     2  63.5     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.072<span style="text-decoration: underline;">2</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 3</span>     3  37.3     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.086<span style="text-decoration: underline;">2</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 4</span>     4  33.1     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.086<span style="text-decoration: underline;">9</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 5</span>     5  41.1     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.022<span style="text-decoration: underline;">5</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 6</span>     6  30.6     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.062<span style="text-decoration: underline;">5</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 7</span>     7  35.1     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.096<span style="text-decoration: underline;">0</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 8</span>     8  32.9     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.065<span style="text-decoration: underline;">4</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;"> 9</span>     9  56.0     0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.034<span style="text-decoration: underline;">2</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> <span style="color: #BCBCBC;">10</span>    10 102.      0 <span style="color: #BB0000;">NA</span>     <span style="color: #949494;">&lt;dbl [2]&gt;</span> 0.006<span style="text-decoration: underline;">92</span></span>
<span class="r-plt img"><img src="cito-9.png" alt="" width="700" height="433"></span>
<span class="r-plt img"><img src="cito-10.png" alt="" width="700" height="433"></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>            [,1]       [,2]       [,3]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1,] 0.33247098 0.04241669 0.07494953</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [2,] 0.04241669 0.16979757 0.02973274</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [3,] 0.07494953 0.02973274 0.22169146</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

