<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Train a Convolutional Neural Network (CNN) — cnn • cito</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Train a Convolutional Neural Network (CNN) — cnn"><meta name="description" content="This function trains a Convolutional Neural Network (CNN) on the provided input data X and the target data Y using the specified architecture, loss function, and optimizer."><meta property="og:description" content="This function trains a Convolutional Neural Network (CNN) on the provided input data X and the target data Y using the specified architecture, loss function, and optimizer."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a></li>
    <li><a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a></li>
    <li><a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a></li>
    <li><a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a></li>
    <li><a class="dropdown-item" href="../articles/E-CNN_and_MMN.html">Convultions neural networks and Multi modal neural networks</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Train a Convolutional Neural Network (CNN)</h1>
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/R/cnn.R" class="external-link"><code>R/cnn.R</code></a></small>
      <div class="d-none name"><code>cnn.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function trains a Convolutional Neural Network (CNN) on the provided input data <code>X</code> and the target data <code>Y</code> using the specified architecture, loss function, and optimizer.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">cnn</span><span class="op">(</span></span>
<span>  <span class="va">X</span>,</span>
<span>  Y <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  <span class="va">architecture</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mse"</span>, <span class="st">"mae"</span>, <span class="st">"softmax"</span>, <span class="st">"cross-entropy"</span>, <span class="st">"gaussian"</span>, <span class="st">"binomial"</span>, <span class="st">"poisson"</span>,</span>
<span>    <span class="st">"mvp"</span>, <span class="st">"nbinom"</span>, <span class="st">"multinomial"</span>, <span class="st">"clogit"</span><span class="op">)</span>,</span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"sgd"</span>, <span class="st">"adam"</span>, <span class="st">"adadelta"</span>, <span class="st">"adagrad"</span>, <span class="st">"rmsprop"</span>, <span class="st">"rprop"</span><span class="op">)</span>,</span>
<span>  lr <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  lambda <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  validation <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  batchsize <span class="op">=</span> <span class="fl">32L</span>,</span>
<span>  burnin <span class="op">=</span> <span class="fl">30</span>,</span>
<span>  shuffle <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  early_stopping <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  lr_scheduler <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  custom_parameters <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  device <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"cpu"</span>, <span class="st">"cuda"</span>, <span class="st">"mps"</span><span class="op">)</span>,</span>
<span>  plot <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-x">X<a class="anchor" aria-label="anchor" href="#arg-x"></a></dt>
<dd><p>An array of input data with a minimum of 3 and a maximum of 5 dimensions. The first dimension represents the samples, the second dimension represents the channels, and the third to fifth dimensions represent the input dimensions.</p></dd>


<dt id="arg-y">Y<a class="anchor" aria-label="anchor" href="#arg-y"></a></dt>
<dd><p>The target data. It can be a factor, numeric vector, or a numeric or logical matrix.</p></dd>


<dt id="arg-architecture">architecture<a class="anchor" aria-label="anchor" href="#arg-architecture"></a></dt>
<dd><p>An object of class 'citoarchitecture'. See <code><a href="create_architecture.html">create_architecture</a></code> for more information.</p></dd>


<dt id="arg-loss">loss<a class="anchor" aria-label="anchor" href="#arg-loss"></a></dt>
<dd><p>The loss function to be used. Options include "mse", "mae", "softmax", "cross-entropy", "gaussian", "binomial", "poisson", "nbinom", "mvp", "multinomial", and "clogit". You can also specify your own loss function. See Details for more information. Default is "mse".</p></dd>


<dt id="arg-optimizer">optimizer<a class="anchor" aria-label="anchor" href="#arg-optimizer"></a></dt>
<dd><p>The optimizer to be used. Options include "sgd", "adam", "adadelta", "adagrad", "rmsprop", and "rprop". See <code><a href="config_optimizer.html">config_optimizer</a></code> for further adjustments to the optimizer. Default is "sgd".</p></dd>


<dt id="arg-lr">lr<a class="anchor" aria-label="anchor" href="#arg-lr"></a></dt>
<dd><p>Learning rate for the optimizer. Default is 0.01.</p></dd>


<dt id="arg-alpha">alpha<a class="anchor" aria-label="anchor" href="#arg-alpha"></a></dt>
<dd><p>Alpha value for L1/L2 regularization. Default is 0.5.</p></dd>


<dt id="arg-lambda">lambda<a class="anchor" aria-label="anchor" href="#arg-lambda"></a></dt>
<dd><p>Lambda value for L1/L2 regularization. Default is 0.0.</p></dd>


<dt id="arg-validation">validation<a class="anchor" aria-label="anchor" href="#arg-validation"></a></dt>
<dd><p>Proportion of the data to be used for validation. Default is 0.0.</p></dd>


<dt id="arg-batchsize">batchsize<a class="anchor" aria-label="anchor" href="#arg-batchsize"></a></dt>
<dd><p>Batch size for training. Default is 32.</p></dd>


<dt id="arg-burnin">burnin<a class="anchor" aria-label="anchor" href="#arg-burnin"></a></dt>
<dd><p>Number of epochs after which the training stops if the loss is still above the base loss. Default is 30.</p></dd>


<dt id="arg-shuffle">shuffle<a class="anchor" aria-label="anchor" href="#arg-shuffle"></a></dt>
<dd><p>Whether to shuffle the data before each epoch. Default is TRUE.</p></dd>


<dt id="arg-epochs">epochs<a class="anchor" aria-label="anchor" href="#arg-epochs"></a></dt>
<dd><p>Number of epochs to train the model. Default is 100.</p></dd>


<dt id="arg-early-stopping">early_stopping<a class="anchor" aria-label="anchor" href="#arg-early-stopping"></a></dt>
<dd><p>Number of epochs with no improvement after which training will be stopped. Default is NULL.</p></dd>


<dt id="arg-lr-scheduler">lr_scheduler<a class="anchor" aria-label="anchor" href="#arg-lr-scheduler"></a></dt>
<dd><p>Learning rate scheduler. See <code><a href="config_lr_scheduler.html">config_lr_scheduler</a></code> for creating a learning rate scheduler. Default is NULL.</p></dd>


<dt id="arg-custom-parameters">custom_parameters<a class="anchor" aria-label="anchor" href="#arg-custom-parameters"></a></dt>
<dd><p>Parameters for the custom loss function. See the vignette for an example. Default is NULL.</p></dd>


<dt id="arg-device">device<a class="anchor" aria-label="anchor" href="#arg-device"></a></dt>
<dd><p>Device to be used for training. Options are "cpu", "cuda", and "mps". Default is "cpu".</p></dd>


<dt id="arg-plot">plot<a class="anchor" aria-label="anchor" href="#arg-plot"></a></dt>
<dd><p>Whether to plot the training progress. Default is TRUE.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Whether to print detailed training progress. Default is TRUE.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>An S3 object of class <code>"citocnn"</code> is returned. It is a list containing everything there is to know about the model and its training process.
The list consists of the following attributes:</p>
<dl><dt>net</dt>
<dd><p>An object of class "nn_sequential" "nn_module", originates from the torch package and represents the core object of this workflow.</p></dd>

<dt>call</dt>
<dd><p>The original function call.</p></dd>

<dt>loss</dt>
<dd><p>A list which contains relevant information for the target variable and the used loss function.</p></dd>

<dt>data</dt>
<dd><p>Contains the data used for the training of the model.</p></dd>

<dt>base_loss</dt>
<dd><p>The loss of the intercept-only model.</p></dd>

<dt>weights</dt>
<dd><p>List of parameters (weights and biases) of the models from the best and the last training epoch.</p></dd>

<dt>buffers</dt>
<dd><p>List of buffers (e.g. running mean and variance of batch normalization layers) of the models from the best and the last training epoch.</p></dd>

<dt>use_model_epoch</dt>
<dd><p>Integer, defines whether the model from the best (= 1) or the last (= 2) training epoch should be used for prediction.</p></dd>

<dt>loaded_model_epoch</dt>
<dd><p>Integer, shows whether the parameters and buffers of the model from the best (= 1) or the last (= 2) training epoch are currently loaded in <code>net</code>.</p></dd>

<dt>model_properties</dt>
<dd><p>A list of properties, that define the architecture of the model.</p></dd>

<dt>training_properties</dt>
<dd><p>A list of all the training parameters used the last time the model was trained.</p></dd>

<dt>losses</dt>
<dd><p>A data.frame containing training and validation losses of each epoch.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="convolutional-neural-networks-">Convolutional Neural Networks:<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks-"></a></h2>
    <p>Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured data, such as images.
The key components of a CNN are convolutional layers, pooling layers and fully-connected (linear) layers:</p><ul><li><p><strong>Convolutional layers</strong> are the core building blocks of CNNs. They consist of filters (also called kernels), which are small, learnable matrices. These filters slide over the input data to perform element-wise multiplication, producing feature maps that capture local patterns and features. Multiple filters are used to detect different features in parallel. They help the network learn hierarchical representations of the input data by capturing low-level features (edges, textures) and gradually combining them (in subsequent convolutional layers) to form higher-level features.</p></li>
<li><p><strong>Pooling layers</strong> reduce the size of the feature maps created by convolutional layers, while retaining important information. A common type is max pooling, which keeps the highest value in a region, simplifying the data while preserving essential features.</p></li>
<li><p><strong>Fully-connected (linear) layers</strong> connect every neuron in one layer to every neuron in the next layer. These layers are found at the end of the network and are responsible for combining high-level features to make final predictions.</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="loss-functions-likelihoods">Loss functions / Likelihoods<a class="anchor" aria-label="anchor" href="#loss-functions-likelihoods"></a></h2>
    <p>We support loss functions and likelihoods for different tasks:</p><table class="table table"><tr><td>Name</td><td>Explanation</td><td>Example / Task</td></tr><tr><td>mse</td><td>mean squared error</td><td>Regression, predicting continuous values</td></tr><tr><td>mae</td><td>mean absolute error</td><td>Regression, predicting continuous values</td></tr><tr><td>softmax</td><td>categorical cross entropy</td><td>Multi-class, species classification</td></tr><tr><td>cross-entropy</td><td>categorical cross entropy</td><td>Multi-class, species classification</td></tr><tr><td>gaussian</td><td>Normal likelihood</td><td>Regression, residual error is also estimated (similar to <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">stats::lm()</a></code>)</td></tr><tr><td>binomial</td><td>Binomial likelihood</td><td>Classification/Logistic regression, mortality</td></tr><tr><td>poisson</td><td>Poisson likelihood</td><td>Regression, count data, e.g. species abundances</td></tr><tr><td>nbinom</td><td>Negative binomial likelihood</td><td>Regression, count data with dispersion parameter</td></tr><tr><td>mvp</td><td>multivariate probit model</td><td>joint species distribution model, multi species (presence absence)</td></tr><tr><td>multinomial</td><td>Multinomial likelihood</td><td>step selection in animal movement models</td></tr><tr><td>clogit</td><td>conditional binomial</td><td>step selection in animal movement models</td></tr></table></div>
    <div class="section level2">
    <h2 id="training-and-convergence-of-neural-networks">Training and convergence of neural networks<a class="anchor" aria-label="anchor" href="#training-and-convergence-of-neural-networks"></a></h2>
    <p>Ensuring convergence can be tricky when training neural networks. Their training is sensitive to a combination of the learning rate (how much the weights are updated in each optimization step), the batch size (a random subset of the data is used in each optimization step), and the number of epochs (number of optimization steps). Typically, the learning rate should be decreased with the size of the neural networks (amount of learnable parameters). We provide a baseline loss (intercept only model) that can give hints about an appropriate learning rate:</p>
<p><img src="figures/learningrates.jpg" alt="Learning rates"></p>
<p>If the training loss of the model doesn't fall below the baseline loss, the learning rate is either too high or too low. If this happens, try higher and lower learning rates.</p>
<p>A common strategy is to try (manually) a few different learning rates to see if the learning rate is on the right scale.</p>
<p>See the troubleshooting vignette (<code><a href="../articles/B-Training_neural_networks.html">vignette("B-Training_neural_networks")</a></code>) for more help on training and debugging neural networks.</p>
    </div>
    <div class="section level2">
    <h2 id="finding-the-right-architecture">Finding the right architecture<a class="anchor" aria-label="anchor" href="#finding-the-right-architecture"></a></h2>
    <p>As with the learning rate, there is no definitive guide to choosing the right architecture for the right task. However, there are some general rules/recommendations: In general, wider, and deeper neural networks can improve generalization - but this is a double-edged sword because it also increases the risk of overfitting. So, if you increase the width and depth of the network, you should also add regularization (e.g., by increasing the lambda parameter, which corresponds to the regularization strength). Furthermore, in <a href="https://arxiv.org/abs/2306.10551" class="external-link">Pichler &amp; Hartig, 2023</a>, we investigated the effects of the hyperparameters on the prediction performance as a function of the data size. For example, we found that the <code>selu</code> activation function outperforms <code>relu</code> for small data sizes (&lt;100 observations).</p>
<p>We recommend starting with moderate sizes (like the defaults), and if the model doesn't generalize/converge, try larger networks along with a regularization that helps minimize the risk of overfitting (see <code><a href="../articles/B-Training_neural_networks.html">vignette("B-Training_neural_networks")</a></code> ).</p>
    </div>
    <div class="section level2">
    <h2 id="overfitting">Overfitting<a class="anchor" aria-label="anchor" href="#overfitting"></a></h2>
    <p>Overfitting means that the model fits the training data well, but generalizes poorly to new observations. We can use the validation argument to detect overfitting. If the validation loss starts to increase again at a certain point, it often means that the models are starting to overfit your training data:</p>
<p><img src="figures/overfitting.jpg" alt="Overfitting"></p>
<p><strong>Solutions</strong>:</p>
<ul><li><p>Re-train with epochs = point where model started to overfit</p></li>
<li><p>Early stopping, stop training when model starts to overfit, can be specified using the <code>early_stopping=…</code> argument</p></li>
<li><p>Use regularization (dropout or elastic-net, see next section)</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="regularization">Regularization<a class="anchor" aria-label="anchor" href="#regularization"></a></h2>
    <p>Elastic Net regularization combines the strengths of L1 (Lasso) and L2 (Ridge) regularization. It introduces a penalty term that encourages sparse weight values while maintaining overall weight shrinkage. By controlling the sparsity of the learned model, Elastic Net regularization helps avoid overfitting while allowing for meaningful feature selection. We advise using elastic net (e.g. lambda = 0.001 and alpha = 0.2).</p>
<p>Dropout regularization helps prevent overfitting by randomly disabling a portion of neurons during training. This technique encourages the network to learn more robust and generalized representations, as it prevents individual neurons from relying too heavily on specific input patterns. Dropout has been widely adopted as a simple yet effective regularization method in deep learning.</p>
<p>By utilizing these regularization methods in your neural network training with the cito package, you can improve generalization performance and enhance the network's ability to handle unseen data. These techniques act as valuable tools in mitigating overfitting and promoting more robust and reliable model performance.</p>
    </div>
    <div class="section level2">
    <h2 id="custom-optimizer-and-learning-rate-schedulers">Custom Optimizer and Learning Rate Schedulers<a class="anchor" aria-label="anchor" href="#custom-optimizer-and-learning-rate-schedulers"></a></h2>
    <p>When training a network, you have the flexibility to customize the optimizer settings and learning rate scheduler to optimize the learning process. In the cito package, you can initialize these configurations using the <code><a href="config_lr_scheduler.html">config_lr_scheduler</a></code> and <code><a href="config_optimizer.html">config_optimizer</a></code> functions.</p>
<p><code><a href="config_lr_scheduler.html">config_lr_scheduler</a></code> allows you to define a specific learning rate scheduler that controls how the learning rate changes over time during training. This is beneficial in scenarios where you want to adaptively adjust the learning rate to improve convergence or avoid getting stuck in local optima.</p>
<p>Similarly, the <code><a href="config_optimizer.html">config_optimizer</a></code> function enables you to specify the optimizer for your network. Different optimizers, such as stochastic gradient descent (SGD), Adam, or RMSprop, offer various strategies for updating the network's weights and biases during training. Choosing the right optimizer can significantly impact the training process and the final performance of your neural network.</p>
    </div>
    <div class="section level2">
    <h2 id="training-on-graphic-cards">Training on graphic cards<a class="anchor" aria-label="anchor" href="#training-on-graphic-cards"></a></h2>
    <p>If you have an NVIDIA CUDA-enabled device and have installed the CUDA toolkit version 11.3 and cuDNN 8.4, you can take advantage of GPU acceleration for training your neural networks. It is crucial to have these specific versions installed, as other versions may not be compatible.
For detailed installation instructions and more information on utilizing GPUs for training, please refer to the <a href="https://torch.mlverse.org/docs/articles/installation.html" class="external-link">mlverse: 'torch' documentation</a>.</p>
<p>Note: GPU training is optional, and the package can still be used for training on CPU even without CUDA and cuDNN installations.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><code><a href="predict.citocnn.html">predict.citocnn</a></code>, <code><a href="print.citocnn.html">print.citocnn</a></code>, <code><a href="plot.citocnn.html">plot.citocnn</a></code>, <code><a href="summary.citocnn.html">summary.citocnn</a></code>, <code><a href="coef.citocnn.html">coef.citocnn</a></code>, <code><a href="continue_training.html">continue_training</a></code>, <code><a href="analyze_training.html">analyze_training</a></code></p></div>
    </div>
    <div class="section level2">
    <h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a></h2>
    <p>Armin Schenk, Maximilian Pichler</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="kw">if</span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span></span>
<span class="r-in"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://citoverse.github.io/cito/" class="external-link">cito</a></span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example workflow in cito</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">device</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/cuda_is_available.html" class="external-link">cuda_is_available</a></span><span class="op">(</span><span class="op">)</span>, <span class="st">"cuda"</span>, <span class="st">"cpu"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Data</span></span></span>
<span class="r-in"><span><span class="co">### We generate our own data:</span></span></span>
<span class="r-in"><span><span class="co">### 320 images (3x50x50) of either rectangles or ellipsoids</span></span></span>
<span class="r-in"><span><span class="va">shapes</span> <span class="op">&lt;-</span> <span class="fu">cito</span><span class="fu">:::</span><span class="fu"><a href="simulate_shapes.html">simulate_shapes</a></span><span class="op">(</span>n<span class="op">=</span><span class="fl">320</span>, size<span class="op">=</span><span class="fl">50</span>, channels<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">shapes</span><span class="op">$</span><span class="va">data</span></span></span>
<span class="r-in"><span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">shapes</span><span class="op">$</span><span class="va">labels</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Architecture</span></span></span>
<span class="r-in"><span><span class="co">### Declare the architecture of the CNN</span></span></span>
<span class="r-in"><span><span class="co">### Note that the output layer is added automatically by cnn()</span></span></span>
<span class="r-in"><span><span class="va">architecture</span> <span class="op">&lt;-</span> <span class="fu"><a href="create_architecture.html">create_architecture</a></span><span class="op">(</span><span class="fu"><a href="conv.html">conv</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>, <span class="fu"><a href="maxPool.html">maxPool</a></span><span class="op">(</span><span class="op">)</span>, <span class="fu"><a href="conv.html">conv</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>, <span class="fu"><a href="maxPool.html">maxPool</a></span><span class="op">(</span><span class="op">)</span>, <span class="fu"><a href="linear.html">linear</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Build and train network</span></span></span>
<span class="r-in"><span><span class="co">### softmax is used for classification</span></span></span>
<span class="r-in"><span><span class="va">cnn.fit</span> <span class="op">&lt;-</span> <span class="fu">cnn</span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span>, <span class="va">architecture</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, epochs <span class="op">=</span> <span class="fl">50</span>, validation <span class="op">=</span> <span class="fl">0.1</span>, lr <span class="op">=</span> <span class="fl">0.05</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## The training loss is below the baseline loss but at the end of the</span></span></span>
<span class="r-in"><span><span class="co">## training the loss was still decreasing, so continue training for another 50</span></span></span>
<span class="r-in"><span><span class="co">## epochs</span></span></span>
<span class="r-in"><span><span class="va">cnn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="continue_training.html">continue_training</a></span><span class="op">(</span><span class="va">cnn.fit</span>, epochs <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Structure of Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cnn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Plot Neural Network</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cnn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Convergence can be tested via the analyze_training function</span></span></span>
<span class="r-in"><span><span class="fu"><a href="analyze_training.html">analyze_training</a></span><span class="op">(</span><span class="va">cnn.fit</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## Transfer learning</span></span></span>
<span class="r-in"><span><span class="co">### With the transfer() function we can use predefined architectures with pretrained weights</span></span></span>
<span class="r-in"><span><span class="va">transfer_architecture</span> <span class="op">&lt;-</span> <span class="fu"><a href="create_architecture.html">create_architecture</a></span><span class="op">(</span><span class="fu"><a href="transfer.html">transfer</a></span><span class="op">(</span><span class="st">"resnet18"</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">resnet</span> <span class="op">&lt;-</span> <span class="fu">cnn</span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span>, <span class="va">transfer_architecture</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>,</span></span>
<span class="r-in"><span>              epochs <span class="op">=</span> <span class="fl">10</span>, validation <span class="op">=</span> <span class="fl">0.1</span>, lr <span class="op">=</span> <span class="fl">0.05</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">resnet</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">resnet</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-err co"><span class="r-pr">#&gt;</span> <span class="error">Error in match.arg(tolower(optimizer), choices = c("sgd", "adam", "adadelta",     "adagrad", "rmsprop", "rprop", "ignite_adam")):</span> 'arg' must be of length 1</span>
<span class="r-in"><span><span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

