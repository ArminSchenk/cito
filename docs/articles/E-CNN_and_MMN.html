<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Convultions neural networks and Multi modal neural networks • cito</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Convultions neural networks and Multi modal neural networks">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a></li>
    <li><a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a></li>
    <li><a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a></li>
    <li><a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a></li>
    <li><a class="dropdown-item" href="../articles/E-CNN_and_MMN.html">Convultions neural networks and Multi modal neural networks</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Convultions neural networks and Multi modal neural networks</h1>
                        <h4 data-toc-skip class="author">Maximilian
Pichler</h4>
            
            <h4 data-toc-skip class="date">2025-05-27</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/vignettes/E-CNN_and_MMN.Rmd" class="external-link"><code>vignettes/E-CNN_and_MMN.Rmd</code></a></small>
      <div class="d-none name"><code>E-CNN_and_MMN.Rmd</code></div>
    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      This vignette demonstrates the use of CITO for complex data (CNN)
      and a mixture of complex and tabular data (MMN). CNN can be used
      for various tasks, such as image classification (‘Is this image a
      dog or a cat?’) and ecological tasks, such as multi-deep species
      distribution models (deepSDM), where the input could be an
      environmental time series or remote sensing data and the goal is
      to predict species occurrences. MMN extends CNN by combining
      complex data, such as satellite images, and tabular data, such as
      environmental or spatial information. Furthermore, MMN can combine
      complex data of different dimensions and scales (e.g. LiDAR 3D
      inputs and coloured 2D optical satellite images). This vignette
      explains how to prepare the data and suggests an example project
      structure.
    </div>
    
<div class="section level2">
<h2 id="data-preparation-of-complex-data">Data Preparation of complex data<a class="anchor" aria-label="anchor" href="#data-preparation-of-complex-data"></a>
</h2>
<p>In Cito, the workflow for preparing complex data for Convolutional
Neural Networks (CNNs) and Multimodal Neural Networks (MMNs) is the
same. The only difference is that MMNs can process multiple data types
simultaneously (see the ‘MMN’ section below).</p>
<p>Before we dive into the details, let’s clarify how we represent
different types of complex data in R using multidimensional arrays:</p>
<ul>
<li>Black/white image:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[height, width]</annotation></semantics></math>
with height/width being the number of pixels</li>
<li>Colored images:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>3</mn><mo>,</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[3, height, width]</annotation></semantics></math>
3 channels for the three colors</li>
<li>LiDAR point clouds:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[x, y, z]</annotation></semantics></math>
</li>
<li>Environmental time series (can be also represented by an “image”):
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">time_steps</mtext><mo>,</mo><mtext mathvariant="normal">n_covariates</mtext><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\text{time_steps}, \text{n_covariates}]</annotation></semantics></math>
</li>
</ul>
<p>Note: Cito currently supports up to three-dimensional inputs
(excluding the sample dimension). Four-dimensional arrays, such as
temporal RGB sequences
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>3</mn><mo>,</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo>,</mo><mi>t</mi><mi>i</mi><mi>m</mi><msub><mi>e</mi><mi>s</mi></msub><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[3, height, width, time_steps]</annotation></semantics></math>,
are not yet supported. Please contact us if you need support for 4D
inputs.</p>
<div class="section level3">
<h3 id="the-format-of-the-inputs-we-expect-in-cito">The format of the inputs we expect in cito<a class="anchor" aria-label="anchor" href="#the-format-of-the-inputs-we-expect-in-cito"></a>
</h3>
<p>The <code><a href="../reference/cnn.html">cnn()</a></code> and <code><a href="../reference/mmn.html">mmn()</a></code> functions both expect
their X argument to be a single array, with the first dimension indexing
samples. The subsequent dimensions then correspond to the data structure
of each sample. Specifically:</p>
<ul>
<li>Grayscale images:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">n_samples</mtext><mo>,</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\text{n_samples}, height, width]</annotation></semantics></math>
</li>
<li>RGB images:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">n_samples</mtext><mo>,</mo><mn>3</mn><mo>,</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\text{n_samples}, 3, height, width]</annotation></semantics></math>
</li>
<li>LiDAR point clouds:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">n_samples</mtext><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\text{n_samples}, x, y, z]</annotation></semantics></math>
</li>
<li>Time series data:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">n_samples</mtext><mo>,</mo><mi>t</mi><mi>i</mi><mi>m</mi><msub><mi>e</mi><mi>s</mi></msub><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi><mo>,</mo><msub><mi>n</mi><mi>c</mi></msub><mi>o</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[\text{n_samples}, time_steps, n_covariates]</annotation></semantics></math>
</li>
</ul>
<p>It is crucial that the order of samples in X matches the order of
observations in your response (target) vector (or matrix for multiple
response) y.</p>
<p>Ultimately, the only requirement for cito is that it has these
multidimensional arrays as inputs. Please note that there are several
ways in which we can build them; the following workflow is just one
example.</p>
</div>
<div class="section level3">
<h3 id="preparing-your-data-on-disk">1. Preparing your data on disk<a class="anchor" aria-label="anchor" href="#preparing-your-data-on-disk"></a>
</h3>
<p>Although images can be saved in different formats, we recommend using
formats for which an R function or R package is available to allow you
to load the images into R.</p>
<p>Grayscale and RGB images should be saved as <code>.png</code> or
<code>.jpeg</code>. Time series data can technically be interpreted as
grayscale images and should therefore also be saved as <code>.png</code>
or <code>.jpeg</code>.</p>
<p>However, LiDAR point clouds and/or other remote sensing data have
more ‘channels’ (ofc, they do not have channels at all) than grayscale
or RGB images and cannot therefore be saved as <code>.png</code> or
<code>.jpeg</code>. Classical formats for saving such data are
<code>.tiff</code> (GeoTiff) and <code>.nc</code> (netCDF).</p>
<p>We recommend saving each image individually to your hard drive and
using a naming strategy that allows the observation ID to be inferred
from the image name. Here is an example:</p>
<pre><code>project/
├── data/
│   ├── RGBimages/
│   │   ├── 001-img.jpeg
│   │   ├── 002-img.jpeg
│   │   ├── 003-img.jpeg
│   │   ├── 004-img.jpeg
│   │   └── ...
│   ├── LiDAR/
│   │   ├── 001-LiDAR.tiff
│   │   ├── 002-LiDAR.tiff
│   │   ├── 003-LiDAR.tiff
│   │   ├── 004-LiDAR.tiff
│   │   └── ...
│   └── Response/
│       └── Y.csv
└── code/
    ├── 01-CNN.R
    └── 02-MMN.R</code></pre>
</div>
<div class="section level3">
<h3 id="load-images-into-r">2. Load images into R<a class="anchor" aria-label="anchor" href="#load-images-into-r"></a>
</h3>
<p>Before we can run/train cito, we must load the images into R,
transform them into arrays, and concatenate the individual images of one
input type into one array.</p>
<ul>
<li>Reading <code>.jpeg</code> files: This can be done either using the
<code>imager</code> package via
<code>as.array(imager::load.image("path-to-img.jpeg")))</code> or using
the <code>torchvision</code> package (dependency of cito) via
<code>torchvision::base_loader("path-to-img.jpeg")</code>
</li>
<li>Reading <code>.png</code> files: This can be done using the
<code>torchvision</code> package (dependency of cito) via
<code>torchvision::base_loader("path-to-img.jpeg")</code>
</li>
<li>Reading <code>.tiff</code> files:
<code>tiff::readTIFF("path-to-img.tiff")</code>
</li>
<li>Reading <code>.tiff</code> (GeoTIFF) files:
<code>as.array(raster::brick("path-to-img.tiff"))</code>
</li>
<li>Reading <code>.nc</code> (netCDF) files:
<code>ncdf4::ncvar_get(ncdf4::nc_open("path-to-img.nc"))</code>
</li>
</ul>
<p>Loop to read images into R:</p>
<p>First data type:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">RGBimages_files</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.files.html" class="external-link">list.files</a></span><span class="op">(</span>path <span class="op">=</span> <span class="st">"RGBimages/"</span>, full.names <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">RGBimages</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">vector</a></span><span class="op">(</span><span class="st">"list"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">RGBimages_files</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">RGBimages_files</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">RGBimages</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="fu">torchvision</span><span class="fu">::</span><span class="fu"><a href="https://torchvision.mlverse.org/reference/base_loader.html" class="external-link">base_loader</a></span><span class="op">(</span><span class="va">RGBimages_files</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Second data type:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">LiDAR_files</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.files.html" class="external-link">list.files</a></span><span class="op">(</span>path <span class="op">=</span> <span class="st">"LiDAR/"</span>, full.names <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">LiDAR</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">vector</a></span><span class="op">(</span><span class="st">"list"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">RGBimages_files</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">LiDAR_files</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">LiDAR</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">=</span> <span class="fu">torchvision</span><span class="fu">::</span><span class="fu"><a href="https://torchvision.mlverse.org/reference/base_loader.html" class="external-link">base_loader</a></span><span class="op">(</span><span class="va">LiDAR_files</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Change list of arrays into one array:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">RGBimages</span> <span class="op">=</span> <span class="fu">abind</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/abind/man/abind.html" class="external-link">abind</a></span><span class="op">(</span><span class="va">RGBimages</span>, along <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">LiDAR</span> <span class="op">=</span> <span class="fu">abind</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/abind/man/abind.html" class="external-link">abind</a></span><span class="op">(</span><span class="va">LiDAR</span>, along <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="normalize-and-check-channel-dimension">3. Normalize and check channel dimension<a class="anchor" aria-label="anchor" href="#normalize-and-check-channel-dimension"></a>
</h3>
<p>Deep Neural Networks converge better when the inputs are
normalized/standardized, for complex data, we can divide them by their
max value to bring the values into the range of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">RGBimages</span> <span class="op">=</span> <span class="va">RGBimages</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">RGBimages</span><span class="op">)</span></span>
<span><span class="va">LiDAR</span> <span class="op">=</span> <span class="va">LiDAR</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">LiDAR</span><span class="op">)</span></span></code></pre></div>
<p>Also, cito expects the channel dimension for RGB images to be in the
second dimension. For LiDAR, the question is which dimension should be
treated as the channel dimension. The channel dimension is treated
slightly differently in CNN, so I would propose setting the z dimension
as the channel dimension. However, when we read images into R, the
channel dimension is usually the last dimension. In Cito, though, it
must be the second dimension. (Reminder: our dimensions for RGB are
currently:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>n</mi><mo>,</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi><mo>,</mo><mn>3</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[n, height, width, 3]</annotation></semantics></math>)</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">RGBimages</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/aperm.html" class="external-link">aperm</a></span><span class="op">(</span><span class="va">RGBimages</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span> <span class="co"># change order of dimensions</span></span>
<span><span class="va">LiDAR</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/aperm.html" class="external-link">aperm</a></span><span class="op">(</span><span class="va">LiDAR</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>  <span class="co"># change order of dimensions</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="prepare-tabular-data-response-and-other-tabular-data-such-as-altitude-and-spatial-coordinates">4. Prepare tabular data (response and other tabular data such as
altitude and spatial coordinates)<a class="anchor" aria-label="anchor" href="#prepare-tabular-data-response-and-other-tabular-data-such-as-altitude-and-spatial-coordinates"></a>
</h3>
<p>Read tabular data into R using the <code>read.csv</code> function.
Predictors (e.g. spatial coordinates, altitude and climatic variables
such as bioclim variables) should be standardised using the
<code>scale</code> function.</p>
<p><strong>Note:</strong> there should be no missing values in the data!
If you have 1,000 images and 1,000 response values with NAs, Cito/R will
drop the NA observations in the response, meaning the number of
observations will no longer match up. Of course, there should also be no
NAs in the images!</p>
<p><strong>Note:</strong> The order of the tabular data (responses and
predictors) should match the order of the images.</p>
</div>
</div>
<div class="section level2">
<h2 id="convolutional-neural-networks">Convolutional neural networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<p>We can setup the architecture of the CNN by using the
<code>create_architecture</code> function. CNN usually consist of
several convolutional layers, each layer followed by a pooling layer,
and finally fully connected layers:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">architecture</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/create_architecture.html">create_architecture</a></span><span class="op">(</span><span class="fu"><a href="../reference/conv.html">conv</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>, <span class="co"># convolutional layer with 5 kernels</span></span>
<span>                                    <span class="fu"><a href="../reference/maxPool.html">maxPool</a></span><span class="op">(</span><span class="op">)</span>,  <span class="co"># max pooling layer to reduce the dimension of the feature maps</span></span>
<span>                                    <span class="fu"><a href="../reference/conv.html">conv</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>, <span class="co"># convolutional layer with 5 kernels</span></span>
<span>                                    <span class="fu"><a href="../reference/maxPool.html">maxPool</a></span><span class="op">(</span><span class="op">)</span>, <span class="co"># max pooling layer to reduce the dimension of the feature maps</span></span>
<span>                                    <span class="fu"><a href="../reference/linear.html">linear</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span> <span class="co"># fully connected layer</span></span></code></pre></div>
<p>The idea is that the convolutional layers learn to extract structures
from the images such as shapes and edges. These structures are then
presented to the fully connected layer that is then doing the actual
classification or regression.</p>
<p>Finding a good architecture can require a lot of experience and
knowledge of CNNs. As an alternative, we recommend using transfer
learning, which is also state of the art. Rather than training our own
convolutional layers, we use a pre-trained CNN (usually trained on a
large dataset with hundreds or thousands of response categories) and
only train the final fully connected layer. It has been found that the
convolutional layers often learn the same things, so there is no need to
retrain them each time. This saves a lot of computational runtime, but
more importantly, we don’t need as much training data because we only
have to train a small part of our model:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">architecture</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/create_architecture.html">create_architecture</a></span><span class="op">(</span><span class="fu"><a href="../reference/transfer.html">transfer</a></span><span class="op">(</span><span class="st">"resnet18"</span><span class="op">)</span>, <span class="co"># use pretrained resnet18 architecture</span></span>
<span>                                    <span class="fu"><a href="../reference/linear.html">linear</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="co"># our fully connnected layer</span></span></code></pre></div>
<p>Also, with that, we don’t have to think about our own
architecture!</p>
<p>Finally we can fit our model:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">LiDAR</span>, <span class="va">Y</span>, <span class="va">architecture</span>, loss <span class="op">=</span> <span class="st">"binomial"</span>,</span>
<span>              epochs <span class="op">=</span> <span class="fl">10</span>, validation <span class="op">=</span> <span class="fl">0.1</span>, lr <span class="op">=</span> <span class="fl">0.05</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span></span></code></pre></div>
<p>Note:</p>
<ol style="list-style-type: decimal">
<li>The format of Y depends on the loss and your task.</li>
<li>Be aware of convergence issues; the loss should be higher than the
baseline loss.</li>
<li>Use the validation split to monitor overfitting; training can be
cancelled automatically based on the validation loss using early
stopping.</li>
</ol>
<p>All of these points are described in the <a href="https://cran.r-project.org/web/packages/cito/vignettes/A-Introduction_to_cito.html" class="external-link">Introduction
to cito vignette</a> and apply to the <code><a href="../reference/dnn.html">dnn()</a></code> and
<code><a href="../reference/cnn.html">cnn()</a></code> functions.</p>
<p>When the model is trained, we can make predictions via the
<code>predict</code> method:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pred</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model</span>, <span class="va">LiDAR</span><span class="op">)</span> <span class="co"># by default predictions will be on the scale of the link (so no probabilities)</span></span>
<span><span class="va">pred_proba</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model</span>, <span class="va">LiDAR</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span> <span class="co"># change type to get probabilities</span></span></code></pre></div>
<p>Model can be visualized via <code>plot(model)</code></p>
</div>
<div class="section level2">
<h2 id="multi-modal-neural-networks">Multi-modal neural networks<a class="anchor" aria-label="anchor" href="#multi-modal-neural-networks"></a>
</h2>
<p>Multi-modal neural networks (MMNs) are useful when:</p>
<ul>
<li>There are different input data types, e.g. when combining LiDAR and
optical satellite images (RGB images), or when combining LiDAR with
tabular data (e.g. bioclimatic variables).</li>
<li>You want to combine different resolutions of the same input data
type</li>
<li>Or both of the above</li>
</ul>
<p>Each complex input data must be passed within its own
multidimensional array and its own architecture:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">architecture_LiDAR</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/create_architecture.html">create_architecture</a></span><span class="op">(</span><span class="fu"><a href="../reference/transfer.html">transfer</a></span><span class="op">(</span><span class="st">"resnet18"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">architecture_RGBimages</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/create_architecture.html">create_architecture</a></span><span class="op">(</span><span class="fu"><a href="../reference/transfer.html">transfer</a></span><span class="op">(</span><span class="st">"resnet18"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">=</span></span>
<span>  <span class="fu"><a href="../reference/mmn.html">mmn</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">Y</span> <span class="op">~</span></span>
<span>      <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">LiDAR</span>, architecture <span class="op">=</span> <span class="va">architecture_LiDAR</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">RGBimages</span> , architecture <span class="op">=</span> <span class="va">architecture_RGBimages</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="op">~</span><span class="va">Temp</span><span class="op">+</span><span class="va">Precip</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span>,</span>
<span>      loss <span class="op">=</span> <span class="st">'binomial'</span>,</span>
<span>      optimizer <span class="op">=</span> <span class="st">"adam"</span><span class="op">)</span></span></code></pre></div>
<p><strong>Important:</strong> Tabular data must be within one
data.frame, so here, the response variable <code>Y</code> is in the same
data.frame as <code>Temp</code> and <code>Precip</code>!</p>
<p>For multiple responses:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">=</span></span>
<span>  <span class="fu"><a href="../reference/mmn.html">mmn</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">Y1</span>, <span class="va">df</span><span class="op">$</span><span class="va">Y2</span>, <span class="va">df</span><span class="op">$</span><span class="va">Y3</span><span class="op">)</span> <span class="op">~</span></span>
<span>      <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">LiDAR</span>, architecture <span class="op">=</span> <span class="va">architecture_LiDAR</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">RGBimages</span> , architecture <span class="op">=</span> <span class="va">architecture_RGBimages</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="op">~</span><span class="va">Temp</span><span class="op">+</span><span class="va">Precip</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span>,</span>
<span>      loss <span class="op">=</span> <span class="st">'binomial'</span>,</span>
<span>      optimizer <span class="op">=</span> <span class="st">"adam"</span><span class="op">)</span></span></code></pre></div>
<p>Newdata must be passed as list to the predict function. The datasets
must have the same order as the model components in the mmn:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">LiDAR</span>, <span class="va">RGBimages</span>, <span class="va">df</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span></code></pre></div>
<p>Multiple different responses with different losses:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="va">custom_joint_loss</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">pred</span>, <span class="va">true</span><span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>  <span class="co"># first loss, e.g. binomial -&gt; negative loglikelihood</span></span>
<span>  <span class="va">loss1</span> <span class="op">=</span> <span class="op">-</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/distr_bernoulli.html" class="external-link">distr_bernoulli</a></span><span class="op">(</span>logits <span class="op">=</span> <span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_sigmoid.html" class="external-link">torch_sigmoid</a></span><span class="op">(</span><span class="va">pred</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># second loss, e.g. mse</span></span>
<span>  <span class="va">loss2</span> <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/nnf_mse_loss.html" class="external-link">nnf_mse_loss</a></span><span class="op">(</span><span class="va">pred</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span>, <span class="va">true</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># third loss, e.g. poisson</span></span>
<span>  <span class="va">loss3</span> <span class="op">=</span> <span class="op">-</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/distr_poisson.html" class="external-link">distr_poisson</a></span><span class="op">(</span><span class="va">pred</span><span class="op">[</span>,<span class="fl">3</span><span class="op">]</span><span class="op">$</span><span class="fu">exp</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="fu">log_prob</span><span class="op">(</span><span class="va">true</span><span class="op">[</span>,<span class="fl">3</span><span class="op">]</span><span class="op">)</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># return joint loss</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">loss1</span> <span class="op">+</span> <span class="va">loss2</span> <span class="op">+</span> <span class="va">loss3</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">=</span></span>
<span>  <span class="fu"><a href="../reference/mmn.html">mmn</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">Y1</span>, <span class="va">df</span><span class="op">$</span><span class="va">Y2</span>, <span class="va">df</span><span class="op">$</span><span class="va">Y3</span><span class="op">)</span> <span class="op">~</span></span>
<span>      <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">LiDAR</span>, architecture <span class="op">=</span> <span class="va">architecture_LiDAR</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="../reference/cnn.html">cnn</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">RGBimages</span> , architecture <span class="op">=</span> <span class="va">architecture_RGBimages</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="op">~</span><span class="va">Temp</span><span class="op">+</span><span class="va">Precip</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span>,</span>
<span>      loss <span class="op">=</span> <span class="va">custom_joint_loss</span>,</span>
<span>      epochs <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>      optimizer <span class="op">=</span> <span class="st">"adam"</span><span class="op">)</span></span></code></pre></div>
<p>As cito now lacks the inverse link functions, we have to apply them
to the predictions ourselves:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pred</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">LiDAR</span>, <span class="va">RGBimages</span>, <span class="va">df</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pred</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html" class="external-link">plogis</a></span><span class="op">(</span><span class="va">pred</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">pred</span><span class="op">[</span>,<span class="fl">3</span><span class="op">]</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">pred</span><span class="op">[</span>,<span class="fl">3</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p>Note:</p>
<ol style="list-style-type: decimal">
<li>The format of Y depends on the loss and your task.</li>
<li>Be aware of convergence issues; the loss should be higher than the
baseline loss.</li>
<li>Use the validation split to monitor overfitting; training can be
cancelled automatically based on the validation loss using early
stopping.</li>
</ol>
<p>All of these points are described in the <a href="https://cran.r-project.org/web/packages/cito/vignettes/A-Introduction_to_cito.html" class="external-link">Introduction
to cito vignette</a> and apply to the <code><a href="../reference/dnn.html">dnn()</a></code> and
<code><a href="../reference/cnn.html">cnn()</a></code> functions.</p>
</div>
<div class="section level2">
<h2 id="computational-considerations-and-constraints">Computational Considerations and Constraints<a class="anchor" aria-label="anchor" href="#computational-considerations-and-constraints"></a>
</h2>
<p>When working with convolutional neural networks (CNNs) and multimodal
networks (MMNs), two critical computational factors are memory (RAM) and
GPU availability.</p>
<div class="section level3">
<h3 id="gpu-requirements">1. GPU Requirements<a class="anchor" aria-label="anchor" href="#gpu-requirements"></a>
</h3>
<p>CNNs benefit significantly from GPU acceleration:</p>
<ul>
<li>
<strong>Speed</strong>: Training on a GPU is orders of magnitude
faster than on a CPU.</li>
<li>
<strong>Memory</strong>: Due to the intensive tensor operations, we
recommend a GPU with at least <strong>12 GB</strong> of VRAM to handle
typical batch sizes (e.g., <code>batch_size = 20</code>).</li>
</ul>
</div>
<div class="section level3">
<h3 id="system-memory-ram">2. System Memory (RAM)<a class="anchor" aria-label="anchor" href="#system-memory-ram"></a>
</h3>
<p>Currently, all images must be loaded into a single R session:</p>
<ul>
<li>This approach limits the number of observations by your available
system RAM.</li>
<li>We are implementing data loaders to stream only mini-batches into
memory.</li>
</ul>
<div class="section level4">
<h4 id="example-calculation-500-observations">Example Calculation (500 observations)<a class="anchor" aria-label="anchor" href="#example-calculation-500-observations"></a>
</h4>
<table class="table">
<colgroup>
<col width="40%">
<col width="29%">
<col width="29%">
</colgroup>
<thead><tr class="header">
<th>Data Type</th>
<th>Dimensions</th>
<th>Memory Usage</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>LiDAR volume</td>
<td>(500, 500, 500, 500)</td>
<td>~500 GB</td>
</tr>
<tr class="even">
<td>Optical satellite images</td>
<td>(500, 3, 500, 500)</td>
<td>~3 GB</td>
</tr>
<tr class="odd">
<td>Tabular data</td>
<td>Negligible</td>
<td>&lt; 1 GB (ignored)</td>
</tr>
<tr class="even">
<td>Model parameters (~2.24 M)</td>
<td>—</td>
<td>&lt; 0.1 GB (negligible)</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td></td>
<td><strong>~503 GB</strong></td>
</tr>
</tbody>
</table>
<p>To run this dataset in a single R session you would need <strong>~550
GB</strong> of system RAM. By contrast, GPU memory is less constraining
because only one batch is loaded at a time on the GPU. For
<code>batch_size = 20</code>, a 12–14 GB GPU should suffice.</p>
<p><strong>Summary</strong>: The primary bottleneck is system RAM on the
CPU, not VRAM on the GPU.</p>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
