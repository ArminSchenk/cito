<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="cito">
<title>Training neural networks • cito</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Training neural networks">
<meta property="og:description" content="cito">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a>
    <a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a>
    <a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a>
    <a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Training neural networks</h1>
                        <h4 data-toc-skip class="author">Maximilian
Pichler</h4>
            
            <h4 data-toc-skip class="date">2024-02-21</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/vignettes/B-Training_neural_networks.Rmd" class="external-link"><code>vignettes/B-Training_neural_networks.Rmd</code></a></small>
      <div class="d-none name"><code>B-Training_neural_networks.Rmd</code></div>
    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      This vignette helps to address certain problems that occur when
      training neural networks (NN) and gives hints on how to increase
      the likelihood of their convergence.
    </div>
    
<div class="section level2">
<h2 id="possible-issues">Possible issues<a class="anchor" aria-label="anchor" href="#possible-issues"></a>
</h2>
<ul>
<li>
<p>Convergence issues, (often because of the learning rate),
<strong>training loss above baseline loss</strong>:</p>
<pre><code><span><span class="co">#&gt; Error in cito:::visualize.training(m$losses, epoch = 100, new = TRUE, : argument "main" is missing, with no default</span></span></code></pre>
<p><img src="B/B-unnamed-chunk-2-1.png" style="display: block; margin: auto;"></p>
<p>If it looks like that, go to the <a href="#lr">adjusting the learning
rate section</a></p>
</li>
<li>
<p>Overfitting, difference between training and testing/holdout/new
data error is too high, or validation loss starts to increase again at
some point during the training</p>
<pre><code><span><span class="co">#&gt; Error in cito:::visualize.training(m$losses, epoch = 600L, new = TRUE, : argument "main" is missing, with no default</span></span></code></pre>
<p><img src="B/B-unnamed-chunk-3-1.png" style="display: block; margin: auto;"></p>
<p>if it loos like that, go to the <a href="#overfitting">overfitting
section</a></p>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="lr">Convergence issues<a class="anchor" aria-label="anchor" href="#lr"></a>
</h2>
<p>Ensuring convergence can be tricky when training neural networks.
Their training is sensitive to a combination of the learning rate (how
much the weights are updated in each optimization step), the batch size
(a random subset of the data is used in each optimization step), and the
number of epochs (number of optimization steps).</p>
<div class="section level3">
<h3 id="epochs">Epochs<a class="anchor" aria-label="anchor" href="#epochs"></a>
</h3>
<p>Give the neural network enough time to learn. The epochs should be
high enough so that the training loss “stabilizes”:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">iris</span>, epochs <span class="op">=</span> <span class="fl">10L</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-4-1.png" style="display: block; margin: auto;"></p>
<p>After 10 epochs the loss was still decreasing, let’s run the model
for more epochs:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">iris</span>, epochs <span class="op">=</span> <span class="fl">200L</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-5-1.png" style="display: block; margin: auto;"></p>
<p>It takes around 190-200 epochs until the loss doesn’t decrease
anymore. The “speed” of the learning depends also on the learning rate.
Higher rates means larger steps into direction of the minima of the loss
function:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">iris</span>, epochs <span class="op">=</span> <span class="fl">200L</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, lr <span class="op">=</span> <span class="fl">0.05</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-6-1.png" style="display: block; margin: auto;"></p>
<p>Now it only takes about 100 epochs, but we also see that the training
loss becomes wobbly. Larger learning rates increase the probability that
local minima are skipped and the optimizer has problems to hit a
minima.</p>
</div>
<div class="section level3">
<h3 id="learning-rate">Learning rate<a class="anchor" aria-label="anchor" href="#learning-rate"></a>
</h3>
<p>Typically, the learning rate should be decreased with the size of the
neural networks (depth of the network and width of the hidden layers).
We provide a baseline loss (intercept only model) that can give hints
about an appropriate learning rate.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit_good</span><span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>, lr <span class="op">=</span> <span class="fl">0.09</span>, epochs <span class="op">=</span> <span class="fl">20L</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">nn.fit_high</span><span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>, lr <span class="op">=</span> <span class="fl">2.09</span>, epochs <span class="op">=</span> <span class="fl">20L</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">nn.fit_low</span><span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>, lr <span class="op">=</span> <span class="fl">0.00000001</span>, epochs <span class="op">=</span> <span class="fl">20L</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span>, mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">cito</span><span class="fu">:::</span><span class="fu">visualize.training</span><span class="op">(</span><span class="va">nn.fit_good</span><span class="op">$</span><span class="va">losses</span>, epoch <span class="op">=</span> <span class="fl">20</span>, new <span class="op">=</span> <span class="cn">TRUE</span>, baseline <span class="op">=</span> <span class="va">nn.fit_good</span><span class="op">$</span><span class="va">base_loss</span><span class="op">)</span></span>
<span><span class="co">#&gt; Error in cito:::visualize.training(nn.fit_good$losses, epoch = 20, new = TRUE, : argument "main" is missing, with no default</span></span>
<span><span class="fu">cito</span><span class="fu">:::</span><span class="fu">visualize.training</span><span class="op">(</span><span class="va">nn.fit_high</span><span class="op">$</span><span class="va">losses</span>, epoch <span class="op">=</span> <span class="fl">20</span>, new <span class="op">=</span> <span class="cn">TRUE</span>, baseline <span class="op">=</span> <span class="va">nn.fit_good</span><span class="op">$</span><span class="va">base_loss</span><span class="op">)</span></span>
<span><span class="co">#&gt; Error in cito:::visualize.training(nn.fit_high$losses, epoch = 20, new = TRUE, : argument "main" is missing, with no default</span></span>
<span><span class="fu">cito</span><span class="fu">:::</span><span class="fu">visualize.training</span><span class="op">(</span><span class="va">nn.fit_low</span><span class="op">$</span><span class="va">losses</span>, epoch <span class="op">=</span> <span class="fl">20</span>, new <span class="op">=</span> <span class="cn">TRUE</span>, baseline <span class="op">=</span> <span class="va">nn.fit_good</span><span class="op">$</span><span class="va">base_loss</span><span class="op">)</span></span>
<span><span class="co">#&gt; Error in cito:::visualize.training(nn.fit_low$losses, epoch = 20, new = TRUE, : argument "main" is missing, with no default</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-7-1.png" style="display: block; margin: auto;"></p>
<p>If the training loss of the model doesn’t fall below the baseline
loss, the learning rate is either too high or too low. If this happens,
try higher and lower learning rates.</p>
<p>A common strategy is to try (manually) a few different learning rates
to see if the learning rate is on the right scale.</p>
</div>
<div class="section level3">
<h3 id="solution-learning-rate-scheduler">Solution: learning rate scheduler<a class="anchor" aria-label="anchor" href="#solution-learning-rate-scheduler"></a>
</h3>
<p>A common strategy to deal with the learning rate problem is to start
with a high learning rate, and if the loss does not decrease, the
learning rate is reduced according to a specific plan.</p>
<p>I favor the “reduce learning rate on plateau” scheduler. If a loss
plateau isn’t resolved for a certain number of epochs (patience), the
learning rate will be reduced (<span class="math inline">\(lr_{new} =
factor * lr_{old}\)</span>):</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit_high</span><span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span>
<span>                  lr <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>                  epochs <span class="op">=</span> <span class="fl">60L</span>,</span>
<span>                  loss <span class="op">=</span> <span class="st">"softmax"</span>,</span>
<span>                  lr_scheduler <span class="op">=</span> <span class="fu"><a href="../reference/config_lr_scheduler.html">config_lr_scheduler</a></span><span class="op">(</span><span class="st">"reduce_on_plateau"</span>, patience <span class="op">=</span> <span class="fl">5</span>, factor <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>                  verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  plot <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loss at epoch 1: 1.087108, lr: 0.20000</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-8-1.png" style="display: block; margin: auto;"></p>
<pre><code><span><span class="co">#&gt; Loss at epoch 2: 0.903815, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 3: 0.573949, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 4: 0.897187, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 5: 0.749726, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 6: 0.632035, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 7: 0.578772, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 8: 0.524611, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 9: 0.478397, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 10: 0.452353, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 11: 0.576571, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 12: 0.369899, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 13: 0.325961, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 14: 0.525180, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 15: 0.457416, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 16: 0.360753, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 17: 0.394748, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 18: 0.386267, lr: 0.20000</span></span>
<span><span class="co">#&gt; Loss at epoch 19: 0.472987, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 20: 0.305213, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 21: 0.199477, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 22: 0.172473, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 23: 0.195197, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 24: 0.254813, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 25: 0.279079, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 26: 0.281923, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 27: 0.230909, lr: 0.10000</span></span>
<span><span class="co">#&gt; Loss at epoch 28: 0.233069, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 29: 0.125458, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 30: 0.128728, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 31: 0.114669, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 32: 0.111258, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 33: 0.107564, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 34: 0.107815, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 35: 0.085994, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 36: 0.112124, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 37: 0.081247, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 38: 0.094082, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 39: 0.120218, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 40: 0.117239, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 41: 0.118780, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 42: 0.084812, lr: 0.05000</span></span>
<span><span class="co">#&gt; Loss at epoch 43: 0.089787, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 44: 0.079257, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 45: 0.083253, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 46: 0.083143, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 47: 0.073754, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 48: 0.081439, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 49: 0.075276, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 50: 0.074044, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 51: 0.072283, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 52: 0.083920, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 53: 0.098379, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 54: 0.082494, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 55: 0.088671, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 56: 0.085296, lr: 0.02500</span></span>
<span><span class="co">#&gt; Loss at epoch 57: 0.081099, lr: 0.01250</span></span>
<span><span class="co">#&gt; Loss at epoch 58: 0.070063, lr: 0.01250</span></span>
<span><span class="co">#&gt; Loss at epoch 59: 0.074567, lr: 0.01250</span></span>
<span><span class="co">#&gt; Loss at epoch 60: 0.073408, lr: 0.01250</span></span></code></pre>
<p>At the end of the training, the learning rate is 0.025</p>
<p>Note: The learning rate scheduler is a powerful approach to improve
the likeliness of convergence, BUT it cannot help with much too high
learning rates!</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit_high</span><span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span>,</span>
<span>                  lr <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                  epochs <span class="op">=</span> <span class="fl">60L</span>,</span>
<span>                  loss <span class="op">=</span> <span class="st">"softmax"</span>,</span>
<span>                  lr_scheduler <span class="op">=</span> <span class="fu"><a href="../reference/config_lr_scheduler.html">config_lr_scheduler</a></span><span class="op">(</span><span class="st">"reduce_on_plateau"</span>, patience <span class="op">=</span> <span class="fl">5</span>, factor <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>                  verbose <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  plot <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loss at epoch 1: 6.736276, lr: 2.00000</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-9-1.png" style="display: block; margin: auto;"></p>
<pre><code><span><span class="co">#&gt; Loss at epoch 2: 1.129189, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 3: 1.159925, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 4: 1.197325, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 5: 1.123606, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 6: 1.123105, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 7: 1.155655, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 8: 1.147644, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 9: 1.172323, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 10: 1.127120, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 11: 1.126877, lr: 2.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 12: 1.166185, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 13: 1.115378, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 14: 1.104248, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 15: 1.127626, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 16: 1.117002, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 17: 1.122147, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 18: 1.109230, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 19: 1.102897, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 20: 1.105727, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 21: 1.113893, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 22: 1.114613, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 23: 1.103737, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 24: 1.127322, lr: 1.00000</span></span>
<span><span class="co">#&gt; Loss at epoch 25: 1.114796, lr: 0.50000</span></span>
<span><span class="co">#&gt; Loss at epoch 26: 1.104987, lr: 0.50000</span></span>
<span><span class="co">#&gt; Loss at epoch 27: 1.104206, lr: 0.50000</span></span>
<span><span class="co">#&gt; Loss at epoch 28: 1.113337, lr: 0.50000</span></span>
<span><span class="co">#&gt; Loss at epoch 29: 1.110005, lr: 0.50000</span></span>
<span><span class="co">#&gt; Cancel training because loss is still above baseline, please hyperparameters. See vignette('B-Training_neural_networks') for help.</span></span></code></pre>
<p>Although the learning rate ended up being 0.01562, the loss never
outperformed the baseline loss. The optimizer jumped right at the
beginning into a completely unrealistic solution space for the
parameters of the NN, from which we could not recover.</p>
</div>
</div>
<div class="section level2">
<h2 id="overfitting">Overfitting<a class="anchor" aria-label="anchor" href="#overfitting"></a>
</h2>
<p>Overfitting means that the model fits the training data well, but
generalizes poorly to new observations. We can use the validation
argument to detect overfitting. If the validation loss starts to
increase again at a certain point, it often means that the models are
starting to overfit your training data:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/TheoreticalEcology/" class="external-link">EcoData</a></span><span class="op">)</span> <span class="co"># can be install from github using devtools::install_github(repo = "TheoreticalEcology/EcoData", dependencies = FALSE, build_vignettes = FALSE)</span></span>
<span><span class="va">df</span> <span class="op">=</span> <span class="va">elephant</span><span class="op">$</span><span class="va">occurenceData</span></span>
<span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Presence</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">df</span>, lr <span class="op">=</span> <span class="fl">0.03</span>, epochs <span class="op">=</span> <span class="fl">600L</span>, loss <span class="op">=</span> <span class="st">"binomial"</span>, validation <span class="op">=</span> <span class="fl">0.2</span>,  hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">350L</span>, <span class="fl">350L</span>, <span class="fl">350L</span><span class="op">)</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, batchsize <span class="op">=</span> <span class="fl">150L</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span>, plot <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-10-1.png" style="display: block; margin: auto;"></p>
<p><strong>Solutions</strong>:</p>
<ul>
<li><p>Re-train with epochs = point where model started to
overfit</p></li>
<li><p>Early stopping, stop training when model starts to overfit, can
be specified using the <code>⁠early_stopping=…⁠</code> argument</p></li>
<li><p>Use regularization (dropout or elastic-net, see next
section)</p></li>
</ul>
<div class="section level3">
<h3 id="early-stopping-and-regularization">Early stopping and regularization<a class="anchor" aria-label="anchor" href="#early-stopping-and-regularization"></a>
</h3>
<p>Early stopping = stop training when validation loss cannot be
improved for x epochs (if there is no validation split, the training
loss is used).</p>
<p>lambda = 0.001 is the regularization strength and alpha = 0.2 means
that 20% L1 and 80% L2 weighting.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Presence</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">df</span>, lr <span class="op">=</span> <span class="fl">0.03</span>, epochs <span class="op">=</span> <span class="fl">600L</span>, loss <span class="op">=</span> <span class="st">"binomial"</span>, validation <span class="op">=</span> <span class="fl">0.2</span>,  hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">350L</span>, <span class="fl">350L</span>, <span class="fl">350L</span><span class="op">)</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, batchsize <span class="op">=</span> <span class="fl">150L</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span>, plot <span class="op">=</span> <span class="cn">TRUE</span>, early_stopping <span class="op">=</span> <span class="fl">10</span>, lambda <span class="op">=</span> <span class="fl">0.001</span>, alpha <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="B/B-unnamed-chunk-11-1.png" style="display: block; margin: auto;"></p>
<p>The training is aborted!</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
