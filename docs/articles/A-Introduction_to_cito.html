<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="cito">
<title>Introduction to cito • cito</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to cito">
<meta property="og:description" content="cito">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cito</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/A-Introduction_to_cito.html">Introduction to cito</a>
    <a class="dropdown-item" href="../articles/B-Training_neural_networks.html">Training neural networks</a>
    <a class="dropdown-item" href="../articles/C-Example_Species_distribution_modeling.html">Example: (Multi-) Species distribution models with cito</a>
    <a class="dropdown-item" href="../articles/D-Advanced_custom_loss_functions.html">Advanced: Custom loss functions and prediction intervals</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/citoverse/cito/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Introduction to cito</h1>
                        <h4 data-toc-skip class="author">Christian
Amesoeder &amp; Maximilian Pichler</h4>
            
            <h4 data-toc-skip class="date">2024-03-05</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/citoverse/cito/blob/HEAD/vignettes/A-Introduction_to_cito.Rmd" class="external-link"><code>vignettes/A-Introduction_to_cito.Rmd</code></a></small>
      <div class="d-none name"><code>A-Introduction_to_cito.Rmd</code></div>
    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      ‘cito’ allows you to build and train neural networks using the R
      formula syntax. It relies on the ‘torch’ package for numerical
      computations and optional graphic card support.
    </div>
    
<div class="section level2">
<h2 id="setup---installing-torch">Setup - Installing torch<a class="anchor" aria-label="anchor" href="#setup---installing-torch"></a>
</h2>
<p>Before using ‘cito’ make sure that the current version of ‘torch’ is
installed and running.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">require</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs" class="external-link">torch</a></span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"torch"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: torch</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs" class="external-link">torch</a></span><span class="op">)</span></span>
<span><span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/pkg/torch/man/install_torch.html" class="external-link">install_torch</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span> <span class="op">(</span><span class="va"><a href="https://citoverse.github.io/cito/" class="external-link">cito</a></span><span class="op">)</span></span></code></pre></div>
<p>If you have problems installing Torch, check out the <a href="https://torch.mlverse.org/docs/articles/installation.html" class="external-link">installation
help from the torch developer</a>.</p>
</div>
<div class="section level2">
<h2 id="details-about-the-internal-implementation-of-cito">Details about the internal implementation of ‘cito’<a class="anchor" aria-label="anchor" href="#details-about-the-internal-implementation-of-cito"></a>
</h2>
<div class="section level3">
<h3 id="internal-data-representation">Internal Data Representation<a class="anchor" aria-label="anchor" href="#internal-data-representation"></a>
</h3>
<p>Tabular data are typically supplied to cito as a data.frame in the
usual encoding via the data argument. Categorical data are processed
internally using the model.matrix functions from the R ‘stats’ package.
Categorical features are automatically one-hot encoded (no reference
level). The data are then split into a feature matrix (variables) and a
response matrix.</p>
<p>The training of the neural networks is based on stochastic gradient
descent, which involves sampling random batches of data at each
optimization step. To optimize the sampling and subsetting of the data,
‘torch’ provides optimized data loader objects that create data
iterators (objects that iterate once over the data set corresponding to
an epoch). The feature and response matrices are passed to ‘torch’ data
loaders that reside on the CPU (to avoid memory overflows on the GPU, if
available).</p>
</div>
<div class="section level3">
<h3 id="construction-of-the-neural-networks">Construction of the neural networks<a class="anchor" aria-label="anchor" href="#construction-of-the-neural-networks"></a>
</h3>
<p>Neural networks (NN) are built using the ‘nn_sequential’ object from
the ‘torch’ package. nn_sequential’ expects a list of layers and then
returns a fully functional neural network object. It also initializes
all weights of the layers. The list of layers is built based on user
input, in particular the hidden argument for the number and size of
hidden layers, and the activation argument for the corresponding
activation layers.</p>
</div>
<div class="section level3">
<h3 id="training-and-evaluation">Training and Evaluation<a class="anchor" aria-label="anchor" href="#training-and-evaluation"></a>
</h3>
<p>Once the NN structure and data are prepared, the actual training
starts (the baseline loss is also calculated before the training
starts). The training consists of two loops. An outer loop for the
number of epochs (one epoch means that the data has been used once to
update the weights). And an inner loop for the stochastic gradient
descent, i.e. for n = 100 observations and batch size = 20, it takes 5
batches of data to traverse the dataset once.</p>
<p>At each step of the inner loop, the optimizer is reset, a batch of
data is returned from the data iterator (initialized at each epoch by
the data loader), split into feature and response tensors, pushed to the
GPU (if one is available), the feature matrix is passed to the NN for
prediction, the average loss is computed based on the response tensor
and the predictions, the average loss is backpropagated, and the weights
are updated by the optimizer based on the back-propagated errors. This
inner loop is repeated until the dataset has been used once, completing
the epoch. The process is then repeated for n epochs.</p>
<p>If validation is turned on, after each epoch the model is evaluated
on the validation holdout (which was separated from the data at the
beginning).</p>
</div>
<div class="section level3">
<h3 id="transferability">Transferability<a class="anchor" aria-label="anchor" href="#transferability"></a>
</h3>
<p>To make the model portable (e.g. save and reload the model), the
weights are stored as R matrices in the final object of the ‘dnn’
function. This is necessary because the ‘torch’ objects are just
pointers to the corresponding data structures. Naively, storing these
pointers is pointless, because after the R session ends, the memory is
freed and the pointers are meaningless.</p>
</div>
</div>
<div class="section level2">
<h2 id="introduction-to-models-and-model-structures">Introduction to models and model structures<a class="anchor" aria-label="anchor" href="#introduction-to-models-and-model-structures"></a>
</h2>
<div class="section level3">
<h3 id="loss-functions-likelihoods">Loss functions / Likelihoods<a class="anchor" aria-label="anchor" href="#loss-functions-likelihoods"></a>
</h3>
<p>Cito can handle many different response types. Common loss functions
from ML but also likelihoods for statistical models are supported:</p>
<table class="table">
<colgroup>
<col width="19%">
<col width="19%">
<col width="34%">
<col width="27%">
</colgroup>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Explanation</td>
<td align="left">Example / Task</td>
<td>Meta-Code</td>
</tr>
<tr class="even">
<td align="left"><code>mse</code></td>
<td align="left">mean squared error</td>
<td align="left">Regression, predicting continuous values</td>
<td><code>dnn(Sepal.Length~., ..., loss = "mse")</code></td>
</tr>
<tr class="odd">
<td align="left"><code>mae</code></td>
<td align="left">mean absolute error</td>
<td align="left">Regression, predicting continuous values</td>
<td><code>dnn(Sepal.Length~., ..., loss = "msa")</code></td>
</tr>
<tr class="even">
<td align="left"><code>softmax</code></td>
<td align="left">categorical cross entropy</td>
<td align="left">Multi-class, species classification</td>
<td><code>dnn(Species~., ..., loss = "softmax")</code></td>
</tr>
<tr class="odd">
<td align="left"><code>cross-entropy</code></td>
<td align="left">categorical cross entropy</td>
<td align="left">Multi-class, species classification</td>
<td><code>dnn(Species~., ..., loss = "cross-entropy")</code></td>
</tr>
<tr class="even">
<td align="left"><code>gaussian</code></td>
<td align="left">Normal likelihood</td>
<td align="left">Regression, residual error is also estimated (similar
to <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">stats::lm()</a></code>)</td>
<td><code>dnn(Sepal.Length~., ..., loss = "gaussian")</code></td>
</tr>
<tr class="odd">
<td align="left"><code>binomial</code></td>
<td align="left">Binomial likelihood</td>
<td align="left">Classification/Logistic regression, mortality (0/1
data)</td>
<td><code>dnn(Presence~., ..., loss = "Binomial")</code></td>
</tr>
<tr class="even">
<td align="left"><code>poisson</code></td>
<td align="left">Poisson likelihood</td>
<td align="left">Regression, count data, e.g. species abundances</td>
<td><code>dnn(Abundance~., ..., loss = "Poisson")</code></td>
</tr>
<tr class="odd">
<td align="left"><code>mvp</code></td>
<td align="left">Multivariate probit model</td>
<td align="left">Joint Species Distribution model</td>
<td><code>dnn(cbind(Species1, Species2, Species3)~,..., loss="mvp")</code></td>
</tr>
</tbody>
</table>
<p>Moreover, all non multilabel losses (all except for softmax or
cross-entropy) can be modeled as multilabel using the cbind syntax</p>
<p><code>dnn(cbind(Sepal.Length, Sepal.Width)~., …, loss = "mse")</code></p>
<p>The likelihoods (Gaussian, Binomial, and Poisson) can be also passed
as their stats equivalents:</p>
<p><code>dnn(Sepal.Length~., ..., loss = stats::gaussian)</code></p>
</div>
<div class="section level3">
<h3 id="data">Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h3>
<p>In this vignette, we will work with the irirs dataset and build a
regression model.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">datasets</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/r/datasets/iris.html" class="external-link">iris</a></span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species</span></span>
<span><span class="co">#&gt; 1          5.1         3.5          1.4         0.2  setosa</span></span>
<span><span class="co">#&gt; 2          4.9         3.0          1.4         0.2  setosa</span></span>
<span><span class="co">#&gt; 3          4.7         3.2          1.3         0.2  setosa</span></span>
<span><span class="co">#&gt; 4          4.6         3.1          1.5         0.2  setosa</span></span>
<span><span class="co">#&gt; 5          5.0         3.6          1.4         0.2  setosa</span></span>
<span><span class="co">#&gt; 6          5.4         3.9          1.7         0.4  setosa</span></span>
<span></span>
<span><span class="co">#scale dataset</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/scale.html" class="external-link">scale</a></span><span class="op">(</span><span class="va">data</span><span class="op">[</span>,<span class="op">-</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span>,Species <span class="op">=</span> <span class="va">data</span><span class="op">[</span>,<span class="fl">5</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="fitting-a-simple-model">Fitting a simple model<a class="anchor" aria-label="anchor" href="#fitting-a-simple-model"></a>
</h3>
<p>In ‘cito’, neural networks are specified and fitted with the
<code><a href="../reference/dnn.html">dnn()</a></code> function. Models can also be trained on the GPU by
setting <code>device = "cuda"</code>(but only if you have installed the
CUDA dependencies). This is suggested if you are working with large data
sets or networks.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://citoverse.github.io/cito/" class="external-link">cito</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">#fitting a regression model to predict Sepal.Length</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span> , data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">12</span>, loss <span class="op">=</span> <span class="st">"mse"</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>You can plot the network structure to give you a visual feedback of
the created object. e aware that this may take some time for large
networks.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-plotnn-1.png" style="display: block; margin: auto;"></p>
<p>The neural network 5 input nodes (3 continuoues features,
Sepal.Width, Petal.Length, Petal.Width and the contrasts for the Species
variable (n_classes - 1)) and 1 output node for the response
(Sepal.Length).</p>
</div>
<div class="section level3">
<h3 id="baseline-loss">Baseline loss<a class="anchor" aria-label="anchor" href="#baseline-loss"></a>
</h3>
<p>At the start of the training we calculate a baseline loss for an an
intercept only model. It allows us to control the training because the
goal is to beat the baseline loss. If we don’t, we need to adjust the
optimization parameters (epochs and lr (learning rate)):</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span> , data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">50</span>, lr <span class="op">=</span> <span class="fl">0.6</span>, loss <span class="op">=</span> <span class="st">"mse"</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="co"># lr too high</span></span></code></pre></div>
<p><img src="A/A-unnamed-chunk-2-1.png" style="display: block; margin: auto;"></p>
<p>vs</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span> , data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">50</span>, lr <span class="op">=</span> <span class="fl">0.01</span>, loss <span class="op">=</span> <span class="st">"mse"</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-unnamed-chunk-3-1.png" style="display: block; margin: auto;"></p>
<p>See <code>vignette("B-Training_neural_networks"</code>) for more
details on how to adjust the optimization procedure and increase the
probability of convergence.</p>
</div>
<div class="section level3">
<h3 id="adding-a-validation-set-to-the-training-process">Adding a validation set to the training process<a class="anchor" aria-label="anchor" href="#adding-a-validation-set-to-the-training-process"></a>
</h3>
<p>In order to see where your model might suffer from overfitting the
addition of a validation set can be useful. With <code><a href="../reference/dnn.html">dnn()</a></code> you
can put <code>validation = 0.x</code> and define a percentage that will
not be used for training and only for validation after each epoch.
During training, a loss plot will show you how the two losses behave
(see <code>vignette("B-Training_neural_networks"</code>) for details on
training NN and guaranteeing their convergence).</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#20% of data set is used as validation set</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">32</span>,</span>
<span>              loss<span class="op">=</span> <span class="st">"mse"</span>, validation <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span></code></pre></div>
<p>Weights oft the last and the epoch with the lowest validation loss
are saved:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2</span></span></code></pre></div>
<p>The default is to use the weights of the last epoch. But we can also
tell the model to use the weights with the lowest validation loss:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit</span><span class="op">$</span><span class="va">use_model_epoch</span> <span class="op">=</span> <span class="fl">1</span> <span class="co"># Default, use last epoch</span></span>
<span><span class="va">nn.fit</span><span class="op">$</span><span class="va">use_model_epoch</span> <span class="op">=</span> <span class="fl">2</span> <span class="co"># Use weights from epoch with lowest validation loss</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="methods">Methods<a class="anchor" aria-label="anchor" href="#methods"></a>
</h3>
<p>cito supports many of the well-known methods from other statistical
packages:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="explainable-ai---understanding-your-model">Explainable AI - Understanding your model<a class="anchor" aria-label="anchor" href="#explainable-ai---understanding-your-model"></a>
</h2>
<p>xAI can produce outputs that are similar to known outputs from
statistical models:</p>
<table class="table">
<colgroup>
<col width="22%">
<col width="56%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th>xAI Method</th>
<th>Explanation</th>
<th>Statistical equivalent</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Feature importance (returned by <code>summary(nn.fit)</code>)</td>
<td>Feature importance based on permutations. See <a href="#id_0">Fisher, Rudin, and Dominici (2018)</a> Similar to how much
variance in the data is explained by the features.</td>
<td><code>anova(model)</code></td>
</tr>
<tr class="even">
<td>Average conditional effects (ACE) (returned by
<code>summary(nn.fit)</code>)</td>
<td>Average of local derivatives, approximation of linear effects. See
<a href="https://arxiv.org/abs/2201.08837" class="external-link">Scholbeck et al. (2022)</a>
and <a href="https://arxiv.org/abs/2306.10551" class="external-link">Pichler and Hartig
(2023)</a>
</td>
<td><code>lm(Y~X)</code></td>
</tr>
<tr class="odd">
<td>Standard Deviation of Conditional Effects (SDCE) (returned by
<code>summary(nn.fit)</code>)</td>
<td>Standard deviation of the average conditional effects. Correlates
with the non-linearity of the effects.</td>
<td></td>
</tr>
<tr class="even">
<td>Partial dependency plots (<code>PDP(nn.fit)</code>)</td>
<td>Visualization of the response-effect curve.</td>
<td><code>plot(allEffects(model))</code></td>
</tr>
<tr class="odd">
<td>Accumulated local effect plots (<code>ALE(nn.fit)</code>)</td>
<td>Visualization of the response-effect curve. More robust against
collinearity compared to PDPs</td>
<td><code>plot(allEffects(model))</code></td>
</tr>
</tbody>
</table>
<p>The <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> returns feature importance, ACE and
SDCE:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate and return feature importance</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span>
<span><span class="co">#&gt; Summary of Deep Neural Network Model</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Feature Importance:</span></span>
<span><span class="co">#&gt;       variable importance_1</span></span>
<span><span class="co">#&gt; 1  Sepal.Width     1.614207</span></span>
<span><span class="co">#&gt; 2 Petal.Length     3.854555</span></span>
<span><span class="co">#&gt; 3  Petal.Width     2.161364</span></span>
<span><span class="co">#&gt; 4      Species     1.147288</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Average Conditional Effects:</span></span>
<span><span class="co">#&gt;              Response_1</span></span>
<span><span class="co">#&gt; Sepal.Width   0.2443863</span></span>
<span><span class="co">#&gt; Petal.Length  0.5085570</span></span>
<span><span class="co">#&gt; Petal.Width   0.3216568</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Standard Deviation of Conditional Effects:</span></span>
<span><span class="co">#&gt;              Response_1</span></span>
<span><span class="co">#&gt; Sepal.Width  0.07628443</span></span>
<span><span class="co">#&gt; Petal.Length 0.09923259</span></span>
<span><span class="co">#&gt; Petal.Width  0.11358102</span></span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#returns weights of neural network</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="uncertaintiesp-values">Uncertainties/p-Values<a class="anchor" aria-label="anchor" href="#uncertaintiesp-values"></a>
</h3>
<p>We can use bootstrapping to obtain uncertainties for the xAI metrics
(and also for the predictions). For that we have to retrain our model
with enabled bootstrapping:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span> <span class="op">=</span> <span class="va">data</span></span>
<span><span class="va">df</span><span class="op">[</span>,<span class="fl">2</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html" class="external-link">scale</a></span><span class="op">(</span><span class="va">df</span><span class="op">[</span>,<span class="fl">2</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span> <span class="co"># scaling can help the NN to convergence faster</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">df</span>,</span>
<span>              epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>              verbose <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>              loss<span class="op">=</span> <span class="st">"mse"</span>,</span>
<span>              bootstrap <span class="op">=</span> <span class="fl">30L</span></span>
<span>              <span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span>
<span><span class="co">#&gt; Summary of Deep Neural Network Model</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Feature Importance</span></span>
<span><span class="co">#&gt;                 Importance Std.Err Z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; Sepal.Width →        1.570   0.392    4.00  6.3e-05 ***</span></span>
<span><span class="co">#&gt; Petal.Length →      15.283   6.210    2.46    0.014 *  </span></span>
<span><span class="co">#&gt; Petal.Width →        0.563   0.463    1.22    0.224    </span></span>
<span><span class="co">#&gt; Species →            0.476   0.271    1.75    0.079 .  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Average Conditional Effects</span></span>
<span><span class="co">#&gt;                     ACE Std.Err Z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; Sepal.Width →    0.2971  0.0565    5.26  1.4e-07 ***</span></span>
<span><span class="co">#&gt; Petal.Length →   1.1058  0.1849    5.98  2.2e-09 ***</span></span>
<span><span class="co">#&gt; Petal.Width →   -0.0215  0.1715   -0.13      0.9    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Standard Deviation of Conditional Effects</span></span>
<span><span class="co">#&gt;                    ACE Std.Err Z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; Sepal.Width →   0.1340  0.0328    4.08  4.5e-05 ***</span></span>
<span><span class="co">#&gt; Petal.Length →  0.3528  0.1170    3.01   0.0026 ** </span></span>
<span><span class="co">#&gt; Petal.Width →   0.1222  0.0304    4.02  5.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>We find that Petal.Length and Sepal.Width are significant
(categorical features are not supported yet for average conditional
effects).</p>
<p>Let’s compare the output to statistical outputs:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Analysis of Variance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: Sepal.Length</span></span>
<span><span class="co">#&gt;               Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; Sepal.Width    1   2.060   2.060  15.0011 0.0001625 ***</span></span>
<span><span class="co">#&gt; Petal.Length   1 123.127 123.127 896.8059 &lt; 2.2e-16 ***</span></span>
<span><span class="co">#&gt; Petal.Width    1   2.747   2.747  20.0055 1.556e-05 ***</span></span>
<span><span class="co">#&gt; Species        2   1.296   0.648   4.7212 0.0103288 *  </span></span>
<span><span class="co">#&gt; Residuals    144  19.770   0.137                       </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>Feature importance and the anova report Petal.Length as the most
important feature.</p>
<p>Visualization of the effects:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/PDP.html">PDP</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-unnamed-chunk-9-1.png" style="display: block; margin: auto;"></p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org" class="external-link">effects</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: carData</span></span>
<span><span class="co">#&gt; Use the command</span></span>
<span><span class="co">#&gt;     lattice::trellis.par.set(effectsTheme())</span></span>
<span><span class="co">#&gt;   to customize lattice options for effects plots.</span></span>
<span><span class="co">#&gt; See ?effectsTheme for details.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/effects/man/effect.html" class="external-link">allEffects</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-unnamed-chunk-10-1.png" style="display: block; margin: auto;"></p>
<p>There are some differences between the statistical model and the NN -
which is to be expected because the NN can fit the data more flexible.
But at the same time the differences have large confidence intervals
(e.g. the effect of Petal.Width)</p>
</div>
</div>
<div class="section level2">
<h2 id="architecture">Architecture<a class="anchor" aria-label="anchor" href="#architecture"></a>
</h2>
<p>The architecture in NN usually refers to the width and depth of the
hidden layers (the layers between the input and the output layer) and
their activation functions. You can increase the complexity of the NN by
adding layers and/or making them wider:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># "simple NN" - low complexity</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>              loss<span class="op">=</span> <span class="st">"mse"</span>, validation <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>              hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">5L</span><span class="op">)</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-unnamed-chunk-11-1.png" style="display: block; margin: auto;"></p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># "large NN" - high complexity</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>              loss<span class="op">=</span> <span class="st">"mse"</span>, validation <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>              hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100L</span>, <span class="fl">100</span><span class="op">)</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-unnamed-chunk-11-2.png" style="display: block; margin: auto;"></p>
<p>There is no definitive guide to choosing the right architecture for
the right task. However, there are some general rules/recommendations:
In general, wider, and deeper neural networks can improve generalization
- but this is a double-edged sword because it also increases the risk of
overfitting. So, if you increase the width and depth of the network, you
should also add regularization (e.g., by increasing the lambda
parameter, which corresponds to the regularization strength).
Furthermore, in <a href="https://arxiv.org/abs/2306.10551" class="external-link">Pichler &amp;
Hartig, 2023</a>, we investigated the effects of the hyperparameters on
the prediction performance as a function of the data size. For example,
we found that the <code>selu</code> activation function outperforms
<code>relu</code> for small data sizes (&lt;100 observations).</p>
<p>We recommend starting with moderate sizes (like the defaults), and if
the model doesn’t generalize/converge, try larger networks along with a
regularization that helps to minimize the risk of overfitting (see
<code><a href="../articles/B-Training_neural_networks.html">vignette("B-Training_neural_networks")</a></code> ).</p>
<div class="section level4">
<h4 id="activation-functions">Activation functions<a class="anchor" aria-label="anchor" href="#activation-functions"></a>
</h4>
<p>By default, all layers are fitted with SeLU as activation function.
<span class="math display">\[
relu(x) = max (0,x)
\]</span>You can also adjust the activation function of each layer
individually to build exactly the network you want. In this case you
have to provide a vector the same length as there are hidden layers. The
activation function of the output layer is chosen with the loss argument
and does not have to be provided.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#selu as activation function for all layers:</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">10</span>,<span class="fl">10</span>,<span class="fl">10</span><span class="op">)</span>, activation<span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span></span>
<span><span class="co">#layer specific activation functions:</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>,</span>
<span>              hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">10</span>,<span class="fl">10</span>,<span class="fl">10</span><span class="op">)</span>, activation<span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"relu"</span>,<span class="st">"selu"</span>,<span class="st">"tanh"</span>,<span class="st">"sigmoid"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Note: The default activation function should be adequate for most
tasks. We don’t recommend tuning it.</p>
</div>
</div>
<div class="section level2">
<h2 id="tuning-hyperparameters">Tuning hyperparameters<a class="anchor" aria-label="anchor" href="#tuning-hyperparameters"></a>
</h2>
<div class="section level3">
<h3 id="regularization">Regularization<a class="anchor" aria-label="anchor" href="#regularization"></a>
</h3>
<div class="section level4">
<h4 id="elastic-net-regularization">Elastic net regularization<a class="anchor" aria-label="anchor" href="#elastic-net-regularization"></a>
</h4>
<p>If elastic net is used, ‘cito’ will produce a sparse, generalized
neural network. The L1/L2 loss can be controlled with the arguments
alpha and lambda.</p>
<p><span class="math display">\[
loss = \lambda * [ (1 - \alpha) * |weights| + \alpha |weights|^2 ]
\]</span></p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#elastic net penalty in all layers:</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>, lambda <span class="op">=</span> <span class="fl">0.01</span>, verbose<span class="op">=</span><span class="cn">FALSE</span>, loss <span class="op">=</span> <span class="st">"softmax"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="dropout-regularization">Dropout Regularization<a class="anchor" aria-label="anchor" href="#dropout-regularization"></a>
</h4>
<p>Dropout regularization as proposed in <a href="https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer," class="external-link">Srivastava
et al.</a> can be controlled similar to elastic net regularization. In
this approach, a percentage of different nodes gets left during each
epoch.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#dropout of 35% on all layers:</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, loss <span class="op">=</span> <span class="st">"softmax"</span>, dropout <span class="op">=</span> <span class="fl">0.35</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="learning-rate">Learning rate<a class="anchor" aria-label="anchor" href="#learning-rate"></a>
</h3>
</div>
<div class="section level3">
<h3 id="learning-rate-scheduler">Learning rate scheduler<a class="anchor" aria-label="anchor" href="#learning-rate-scheduler"></a>
</h3>
<p>Learning rate scheduler allow you to start with a high learning rate
and decrease it during the training process. This leads to an overall
faster training. You can choose between different types of schedulers.
Namely, lambda, multiplicative, one_cycle and step.</p>
<p>The function config_lr_scheduler() helps you setup such a scheduler.
See ?config_lr_scheduler() for more information</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step Learning rate scheduler that reduces learning rate every 16 steps by a factor of 0.5</span></span>
<span><span class="va">scheduler</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/config_lr_scheduler.html">config_lr_scheduler</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"step"</span>,</span>
<span>                                 step_size <span class="op">=</span> <span class="fl">16</span>,</span>
<span>                                 gamma <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>,lr <span class="op">=</span> <span class="fl">0.01</span>, lr_scheduler<span class="op">=</span> <span class="va">scheduler</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-lr_scheduler-1.png" style="display: block; margin: auto;"></p>
</div>
<div class="section level3">
<h3 id="optimizer">Optimizer<a class="anchor" aria-label="anchor" href="#optimizer"></a>
</h3>
<p>Optimizer are responsible for fitting the neural network. The
optimizer tries to minimize the loss function. As default the stochastic
gradient descent is used. Custom optimizers can be used with
<code><a href="../reference/config_optimizer.html">config_optimizer()</a></code>.<br>
See <code>?config_optimizer()</code> for more information.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># adam optimizer with learning rate 0.002, betas to 0.95, 0.999 and eps to 1.5e-08</span></span>
<span><span class="va">opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/config_optimizer.html">config_optimizer</a></span><span class="op">(</span></span>
<span>  type <span class="op">=</span> <span class="st">"sgd"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>,  optimizer <span class="op">=</span> <span class="va">opt</span>, lr<span class="op">=</span><span class="fl">0.002</span>, verbose<span class="op">=</span><span class="cn">FALSE</span>, loss <span class="op">=</span> <span class="st">"softmax"</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-optim-1.png" style="display: block; margin: auto;"></p>
</div>
<div class="section level3">
<h3 id="early-stopping">Early Stopping<a class="anchor" aria-label="anchor" href="#early-stopping"></a>
</h3>
<p>Adding early stopping criteria helps you save time by stopping the
training process early, if the validation loss of the current epoch is
bigger than the validation loss n epochs early. The n can be defined by
the early_stopping argument. It is required to set validation &gt;
0.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Stops training if validation loss at current epoch is bigger than that 15 epochs earlier</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Sepal.Length</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>              validation <span class="op">=</span> <span class="fl">0.2</span>, early_stopping <span class="op">=</span> <span class="fl">15</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="A/A-early_stopping-1.png" style="display: block; margin: auto;"></p>
</div>
</div>
<div class="section level2">
<h2 id="automatic-hyperparameter-tuning-experimental">Automatic hyperparameter tuning (experimental)<a class="anchor" aria-label="anchor" href="#automatic-hyperparameter-tuning-experimental"></a>
</h2>
<p>We started to support automatic hyperparameter tuning under Cross
Validation. The tuning strategy is random search, i.e. potential
hyperparameter values are sampled from uniform distributions, the
boundaries can be specified by the user.</p>
<p>We can mark hyperparameters that should be tuned by cito by setting
their values to <code><a href="../reference/tune.html">tune()</a></code>, for example
<code>dnn (..., lr = tune()</code>. <code><a href="../reference/tune.html">tune()</a></code> is a function
that creates a range of random values for the given hyperparameter. You
can also change the maximum and minimum range of these values. The
following table lists the hyperparameters that can currently be
tuned:</p>
<table class="table">
<colgroup>
<col width="18%">
<col width="18%">
<col width="63%">
</colgroup>
<thead><tr class="header">
<th align="left">Hyperparameter</th>
<th align="left">Example</th>
<th align="left">Details</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">hidden</td>
<td align="left"><code>dnn(…,hidden=tune(10, 20, fixed=’depth’))</code></td>
<td align="left">Depth and width can be both tuned or only one of them,
if both of them should be tuned, vectors for lower and upper #’
boundaries must be provided (first = number of nodes)</td>
</tr>
<tr class="even">
<td align="left">bias</td>
<td align="left"><code>dnn(…, bias=tune())</code></td>
<td align="left">Should the bias be turned on or off for all hidden
layers</td>
</tr>
<tr class="odd">
<td align="left">lambda</td>
<td align="left"><code>dnn(…, lambda = tune(0.0001, 0.1))</code></td>
<td align="left">lambda will be tuned within the range (0.0001,
0.1)</td>
</tr>
<tr class="even">
<td align="left">alpha</td>
<td align="left"><code>dnn(…, lambda = tune(0.2, 0.4))</code></td>
<td align="left">alpha will be tuned within the range (0.2, 0.4)</td>
</tr>
<tr class="odd">
<td align="left">activation</td>
<td align="left"><code>dnn(…, activation = tune())</code></td>
<td align="left">activation functions of the hidden layers will be
tuned</td>
</tr>
<tr class="even">
<td align="left">dropout</td>
<td align="left"><code>dnn(…, dropout = tune())</code></td>
<td align="left">Dropout rate will be tuned (globally for all
layers)</td>
</tr>
<tr class="odd">
<td align="left">lr</td>
<td align="left"><code>dnn(…, lr = tune())</code></td>
<td align="left">Learning rate will be tuned</td>
</tr>
<tr class="even">
<td align="left">batchsize</td>
<td align="left"><code>dnn(…, batchsize = tune())</code></td>
<td align="left">batch size will be tuned</td>
</tr>
<tr class="odd">
<td align="left">epochs</td>
<td align="left"><code>dnn(…, batchsize = tune())</code></td>
<td align="left">batchsize will be tuned</td>
</tr>
</tbody>
</table>
<p>The hyperparameters are tuned by random search (i.e., random values
for the hyperparameters within a specified range) and by
cross-validation. The exact tuning regime can be specified with
[config_tuning].</p>
<p>Note that hyperparameter tuning can be expensive. We have implemented
an option to parallelize hyperparameter tuning, including
parallelization over one or more GPUs (the hyperparameter evaluation is
parallelized, not the CV). This can be especially useful for small
models. For example, if you have 4 GPUs, 20 CPU cores, and 20 steps
(random samples from the random search), you could run
<code>dnn(..., device="cuda",lr = tune(), batchsize=tune(), tuning=config_tuning(parallel=20, NGPU=4)</code>,
which will distribute 20 model fits across 4 GPUs, so that each GPU will
process 5 models (in parallel).</p>
<p>Tune learning rate as it is one of the most important
hyperparameters:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nn.fit_tuned</span> <span class="op">=</span> <span class="fu"><a href="../reference/dnn.html">dnn</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">iris</span>,</span>
<span>                   lr <span class="op">=</span> <span class="fu"><a href="../reference/tune.html">tune</a></span><span class="op">(</span><span class="fl">0.0001</span>, <span class="fl">0.1</span><span class="op">)</span>,</span>
<span>                   loss <span class="op">=</span> <span class="st">"softmax"</span>,</span>
<span>                   tuning <span class="op">=</span> <span class="fu"><a href="../reference/config_tuning.html">config_tuning</a></span><span class="op">(</span>steps <span class="op">=</span> <span class="fl">3L</span>, CV <span class="op">=</span> <span class="fl">3L</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Starting hyperparameter tuning...</span></span>
<span><span class="co">#&gt; Fitting final model...</span></span></code></pre></div>
<p>After tuning, the final model is trained with the best set of
hyperparameters and returned.</p>
</div>
<div class="section level2">
<h2 id="continue-training-process">Continue training process<a class="anchor" aria-label="anchor" href="#continue-training-process"></a>
</h2>
<p>You can continue the training process of an existing model with
continue_training().</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># simple example, simply adding another 12 epochs to the training process</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/continue_training.html">continue_training</a></span><span class="op">(</span><span class="va">nn.fit</span>, epochs <span class="op">=</span> <span class="fl">12</span>, verbose<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">nn.fit</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;            [,1]</span></span>
<span><span class="co">#&gt; [1,] -1.0007209</span></span>
<span><span class="co">#&gt; [2,] -1.3226014</span></span>
<span><span class="co">#&gt; [3,] -1.2187723</span></span>
<span><span class="co">#&gt; [4,] -1.2439288</span></span>
<span><span class="co">#&gt; [5,] -0.9474713</span></span>
<span><span class="co">#&gt; [6,] -0.6155921</span></span></code></pre></div>
<p>It also allows you to change any training parameters, for example the
learning rate. You can analyze the training process with
analyze_training().</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># picking the model with the smalles validation loss</span></span>
<span><span class="co"># with changed parameters, in this case a smaller learning rate and a smaller batchsize</span></span>
<span><span class="va">nn.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/continue_training.html">continue_training</a></span><span class="op">(</span><span class="va">nn.fit</span>,</span>
<span>                            epochs <span class="op">=</span> <span class="fl">32</span>,</span>
<span>                            changed_params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.001</span>, batchsize <span class="op">=</span> <span class="fl">16</span><span class="op">)</span>,</span>
<span>                            verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Christian Amesöder, Maximilian Pichler.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
