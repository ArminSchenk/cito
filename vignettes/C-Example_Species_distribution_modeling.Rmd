---
title: "Example: (Multi-) Species distribution models with cito"
author: "Maximilian Pichler"
abstract: "This vignette shows working examples of how to fit (multi-) species distribution models with cito. Training neural networks is tricky compared to other ML algorithms that converge more easily (due to various reasons). The purpose of this vignette is to provide an example workflow and to point out common caveats when training a neural network"
date: "2023-09-29"
output:
 rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
    html_document:
      toc: true
      theme: cerulean
vignette: >
  %\VignetteIndexEntry{Example: (Multi-) Species distribution models with cito}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---



## Species distribution model - African elephant

The goal is to build a SDM for the African elephant. A pre-processed dataset from [Angelov, 2020](https://zenodo.org/record/4048271) can be found in the EcoData package which is only available on github:


```r
if(!require(EcoData)) devtools::install_github(repo = "TheoreticalEcology/EcoData",
                         dependencies = FALSE, build_vignettes = FALSE)

library(EcoData)
df = EcoData::elephant$occurenceData
head(df)
#>       Presence       bio1       bio2       bio3       bio4        bio5
#> 3364         0 -0.4981747 -0.2738045  0.5368968 -0.5409999 -0.36843571
#> 6268         0  0.6085908 -0.5568352  1.0340686 -1.2492050 -0.11835651
#> 10285        0 -0.7973005  1.4648130 -1.0540532  2.0759423  0.07614953
#> 2247         0  0.6385034  1.3435141 -0.1591439 -0.5107148  1.10425291
#> 9821         0  0.6684160 -0.6781341  0.6363311 -0.9906170  0.15950927
#> 1351         0  0.9675418 -0.6781341 -0.3580126 -0.3748202  0.77081398
#>             bio6       bio7       bio8       bio9       bio10       bio11
#> 3364   0.2947850 -0.5260099 -1.2253960  0.2494100 -0.64527314 -0.06267842
#> 6268   0.8221087 -0.8938475  0.4233787  0.7746249  0.09168503  0.94419518
#> 10285 -1.5860029  1.6284678  0.2768209 -1.5153122 -0.03648161 -1.44165748
#> 2247  -0.1622288  0.8577603  0.4600181  0.5855475  0.54026827  0.68153250
#> 9821   0.9099960 -0.8062671  0.3867393  0.8586593  0.31597665  0.94419518
#> 1351   0.8748411 -0.3858812  0.3134604  1.0477367  0.98885151  0.94419518
#>            bio12      bio13       bio14        bio15      bio16      bio17
#> 3364   0.6285371  0.6807958 -0.29703736 -0.008455252  0.7124535 -0.2949994
#> 6268   1.1121516  0.5918442  0.01619202 -0.884507980  0.5607328  0.3506918
#> 10285 -1.2351482 -1.3396742 -0.50585695  0.201797403 -1.3499999 -0.5616980
#> 2247   0.5951165  0.8714061 -0.55806185  0.236839512  1.1012378 -0.5616980
#> 9821   1.1003561  0.5537222  0.59044589 -1.024676416  0.6413344  0.7437213
#> 1351   0.7287986  1.1255533 -0.50585695  0.236839512  1.2956300 -0.4494038
#>             bio18       bio19
#> 3364  -1.06812752  1.96201807
#> 6268   1.22589281 -0.36205814
#> 10285 -0.42763181 -0.62895735
#> 2247  -0.20541902 -0.58378979
#> 9821   0.06254347 -0.05409751
#> 1351  -0.90473576  2.47939193
```

Presence is our response variable and we have the 19 bioclim variables as features/predictors.

Let's split part of the data away so that we can use it at the end to evaluate our model:


```r
indices = sample.int(nrow(df), 300)
test = df[indices,]
df = df[-indices,]
```

### Adjusting optimization parameters - Convergence

We will first try a simple DNN with default values and the binomial likelihood. We use 30% of the data as validation holdout to check for overfitting:


```r
library(cito)
model = dnn(Presence~., data = df,
            batchsize = 100L,
            validation = 0.3, loss = "binomial",
            verbose = FALSE)
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-4-1.png" alt="plot of chunk unnamed-chunk-4" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-4</p>
</div>

We see that the training and test losses were still decreasing which means we didn't train the model long enough. We could now either increase the number of epochs or increase the learning rate so that the model trains faster:


```r
model = dnn(Presence~., data = df,
            batchsize = 100L,
            lr = 0.05,
            validation = 0.3, loss = "binomial",
            verbose = FALSE)
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-5-1.png" alt="plot of chunk unnamed-chunk-5" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-5</p>
</div>

Much better! But still now enough epochs. Also, let's see if we can further decrease the loss by using a wider and deeper neural network:


```r
model = dnn(Presence~., data = df,
            batchsize = 100L,
            hidden = c(100L, 100L, 100L),
            lr = 0.05,
            validation = 0.3, loss = "binomial",
            verbose = FALSE)
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-6-1.png" alt="plot of chunk unnamed-chunk-6" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-6</p>
</div>

At the end of the training, the losses start to get jumpy, which can be a sign of potential overfitting. We can control that by adding a weak regularization (but we only want a L2 regularization, so we set alpha to 1.0):


```r
model = dnn(Presence~., data = df,
            batchsize = 100L,
            epochs = 150L,
            hidden = c(100L, 100L, 100L),
            lr = 0.05,
            lambda = 0.001,
            alpha = 1.0,
            validation = 0.3, loss = "binomial",
            verbose = FALSE)
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-7-1.png" alt="plot of chunk unnamed-chunk-7" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-7</p>
</div>

We will turn on now advanced features that help with the convergence and to reduce overfitting:

-   learning rate scheduler - reduces learning rate during training

-   early stopping - stop training when validation loss starts to increase


```r
model = dnn(Presence~., data = df,
            batchsize = 100L,
            epochs = 150L,
            hidden = c(100L, 100L, 100L),
            lr = 0.05,
            lambda = 0.001,
            alpha = 1.0,
            validation = 0.3, loss = "binomial",
            verbose = FALSE,
            lr_scheduler = config_lr_scheduler("reduce_on_plateau", patience = 7), # reduce learning rate each 7 epochs if the validation loss didn't decrease,
            early_stopping = 14 # stop training when validation loss didn't decrease for 10 epochs
            )
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-8-1.png" alt="plot of chunk unnamed-chunk-8" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-8</p>
</div>

Great! We found now a model architecture and training procedure that fits and trains well. Let's proceed to our final model

### Train final model with bootstrapping to obtain uncertainties

We haven't directly started with bootstrapping because it complicates the adjustment of the training procedure.

Uncertainties can be obtained by using bootstrapping. Be aware that this can be computational expensive:


```r
model_boot = dnn(Presence~., data = df,
                 batchsize = 100L,
                 epochs = 150L,
                 hidden = c(100L, 100L, 100L),
                 lr = 0.05,
                 lambda = 0.001,
                 alpha = 1.0,
                 validation = 0.3, loss = "binomial",
                 verbose = FALSE,
                 lr_scheduler = config_lr_scheduler("reduce_on_plateau", patience = 7), # reduce learning rate each 7 epochs if the validation loss didn't decrease,
                 early_stopping = 14, # stop training when validation loss didn't decrease for 10 epochs
                 bootstrap = 20L,
                 bootstrap_parallel = 5L
            )
```

### Predictions

We can use the model now for predictions:


```r
predictions = predict(model_boot, newdata = test)
dim(predictions)
#> [1]  20 300   1
```

The predictions are 2/3 dimensional because of the bootstrapping. Calculate the AUC interval:


```r
hist(sapply(1:20, function(i) Metrics::auc(test$Presence, predictions[i,,])),
     xlim = c(0, 1), main = "AUC of ensemble model", xlab = "AUC")
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-11-1.png" alt="plot of chunk unnamed-chunk-11" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-11</p>
</div>

We can now predict the habitat suitability of the elephant (Note that spatial dependencies are required):


```r
library(raster)
#> Loading required package: sp
#> The legacy packages maptools, rgdal, and rgeos, underpinning the sp package,
#> which was just loaded, will retire in October 2023.
#> Please refer to R-spatial evolution reports for details, especially
#> https://r-spatial.org/r/2023/05/15/evolution4.html.
#> It may be desirable to make the sf package available;
#> package maintainers should consider adding sf to Suggests:.
#> The sp package is now running under evolution status 2
#>      (status 2 uses the sf package in place of rgdal)
library(sp)
library(rsample)
library(latticeExtra)
#> Loading required package: lattice
#> 
#> Attaching package: 'lattice'
#> The following object is masked from 'package:EcoData':
#> 
#>     melanoma
library(sp)
library(ggplot2)
#> 
#> Attaching package: 'ggplot2'
#> The following object is masked from 'package:latticeExtra':
#> 
#>     layer
library(maptools)
#> Please note that 'maptools' will be retired during October 2023,
#> plan transition at your earliest convenience (see
#> https://r-spatial.org/r/2023/05/15/evolution4.html and earlier blogs
#> for guidance);some functionality will be moved to 'sp'.
#>  Checking rgeos availability: FALSE
customPredictFun = function(model, data) {
  return(apply(predict(model, data), 2:3, mean)[,1])
}

normalized_raster = EcoData::elephant$predictionData

predictions =
  raster::predict(normalized_raster,
                  model_boot,
                  fun = customPredictFun)

habitat_plot =
  spplot(predictions, colorkey = list(space = "left") )
habitat_plot
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-12-1.png" alt="plot of chunk unnamed-chunk-12" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-12</p>
</div>

Moreover, we can visualize the uncertainty of our model, instead of calculating the average occurrence probability, we calculate for each prediction the standard deviation and visualize it:


```r
customPredictFun_sd = function(model, data) {
  return(apply(predict(model, data), 2:3, sd)[,1])
}
predictions =
  raster::predict(normalized_raster,
                  model_boot,
                  fun = customPredictFun_sd)

uncertain_plot =
  spplot(predictions, colorkey = list(space = "left") )
uncertain_plot
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-13</p>
</div>

### Inference

Neural networks are often called black-box models but the tools of explainable AI (xAI) allows us to understand them - and actually infer properties similar to what a linear regression model can provide (the calculation can take some time...):


```r
results = summary(model_boot)
#> Error in if (object$loss == "softmax") resp = object$response_column else resp = object$responses[i - : the condition has length > 1
results
#> Error in eval(expr, envir, enclos): object 'results' not found
```

Bioclim9, 12, 14, and 16 have large significant average conditional effects (\$\\approx\$ linear effects). We can visualize them using accumulated local effect plots:


```r
par(mfrow = c(1, 4))
ALE(model_boot, variable = "bio9")
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-15-1.png" alt="plot of chunk unnamed-chunk-15" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-15</p>
</div>

```r
ALE(model_boot, variable = "bio12")
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-15-2.png" alt="plot of chunk unnamed-chunk-15" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-15</p>
</div>

```r
ALE(model_boot, variable = "bio14")
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-15-3.png" alt="plot of chunk unnamed-chunk-15" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-15</p>
</div>

```r
ALE(model_boot, variable = "bio16")
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-15-4.png" alt="plot of chunk unnamed-chunk-15" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-15</p>
</div>

## Multi-species distribution model

Cito supports many different loss functions which we can use to build multi-species distribution models (MSDM). MSDM are multi-label, i.e. they model and predict simoustanously many responses. We will use eucalypts data from [Pollock et al., 2014](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12180). The dataset has occurrence of 12 species over 458 sites.


```r
load(url("https://github.com/TheoreticalEcology/s-jSDM/raw/master/sjSDM/data/eucalypts.rda"))
# Environment
head(eucalypts$env)
#>   Rockiness Sandiness VallyBotFlat PPTann Loaminess cvTemp      T0
#> 1        60         1            0    785         0    142 6124.01
#> 2        75         1            0    785         0    142 6124.01
#> 3        70         1            0    780         0    142 3252.96
#> 4        40         1            0    778         0    142 1636.63
#> 5        15         1            0    772         0    142 1352.08
#> 6        80         1            0    841         0    142 5018.48

# PA
head(eucalypts$PA)
#>      ALA ARE BAX CAM GON MEL OBL OVA WIL ALP VIM ARO.SAB
#> [1,]   0   0   0   0   0   0   0   0   1   1   0       0
#> [2,]   0   0   0   0   0   0   1   0   1   1   0       0
#> [3,]   0   0   1   0   0   0   0   0   1   1   0       0
#> [4,]   0   0   1   0   0   0   0   0   1   0   0       0
#> [5,]   0   0   1   0   0   0   1   0   0   0   0       0
#> [6,]   0   0   0   0   0   0   0   0   1   1   0       0
```

Bring data into a format that is usable by cito:


```r
df = cbind(eucalypts$PA, scale(eucalypts$env))
head(df)
#>      ALA ARE BAX CAM GON MEL OBL OVA WIL ALP VIM ARO.SAB  Rockiness Sandiness
#> [1,]   0   0   0   0   0   0   0   0   1   1   0       0  1.0315338 0.5716827
#> [2,]   0   0   0   0   0   0   1   0   1   1   0       0  1.4558834 0.5716827
#> [3,]   0   0   1   0   0   0   0   0   1   1   0       0  1.3144335 0.5716827
#> [4,]   0   0   1   0   0   0   0   0   1   0   0       0  0.4657344 0.5716827
#> [5,]   0   0   1   0   0   0   1   0   0   0   0       0 -0.2415148 0.5716827
#> [6,]   0   0   0   0   0   0   0   0   1   1   0       0  1.5973333 0.5716827
#>      VallyBotFlat       PPTann  Loaminess    cvTemp         T0
#> [1,]   -0.5939667 -0.005981517 -0.2134535 -1.056073  0.5378148
#> [2,]   -0.5939667 -0.005981517 -0.2134535 -1.056073  0.5378148
#> [3,]   -0.5939667 -0.045456081 -0.2134535 -1.056073 -0.3404551
#> [4,]   -0.5939667 -0.061245907 -0.2134535 -1.056073 -0.8348993
#> [5,]   -0.5939667 -0.108615385 -0.2134535 -1.056073 -0.9219447
#> [6,]   -0.5939667  0.436133605 -0.2134535 -1.056073  0.1996271
```

We will use the binomial likelihood - each species occurrence data will be modeled by a binomial likelihood. Build simple model:


```r
model = dnn(cbind(ALA, ARE, BAX, CAM, GON, MEL, OBL, OVA, WIL, ALP, VIM, ARO.SAB)~.,
            data = df,
            verbose = FALSE,
            loss = "binomial")
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-18-1.png" alt="plot of chunk unnamed-chunk-18" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-18</p>
</div>

Plot model:


```r
plot(model)
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-19-1.png" alt="plot of chunk unnamed-chunk-19" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-19</p>
</div>

Our NN has now 12 output nodes, one for each species.


```r
head(predict(model))
#>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
#> [1,] 0.1795961 0.1839851 0.4042369 0.1712393 0.1899824 0.1662842 0.3161989
#> [2,] 0.1816017 0.1899037 0.4038403 0.1707434 0.1927823 0.1692383 0.3172422
#> [3,] 0.1907018 0.2074893 0.4015117 0.1982985 0.2005170 0.1869523 0.3175725
#> [4,] 0.1770074 0.1926014 0.3910252 0.1967125 0.1880194 0.1797891 0.3046530
#> [5,] 0.1610143 0.1725444 0.3839702 0.1832990 0.1701354 0.1640236 0.2929430
#> [6,] 0.1915056 0.1987555 0.4091581 0.1823470 0.1978833 0.1792970 0.3201375
#>           [,8]      [,9]     [,10]     [,11]     [,12]
#> [1,] 0.1600941 0.2500909 0.2166301 0.1694678 0.2689682
#> [2,] 0.1642216 0.2541386 0.2220603 0.1722449 0.2708070
#> [3,] 0.1824248 0.2796364 0.2397997 0.1946124 0.2953130
#> [4,] 0.1704870 0.2591339 0.2173808 0.1850987 0.2866380
#> [5,] 0.1508447 0.2321722 0.1899188 0.1685674 0.2717901
#> [6,] 0.1764218 0.2657596 0.2362766 0.1833978 0.2775673
```

### Train model with bootstrapping


```r
model_boot = dnn(cbind(ALA, ARE, BAX, CAM, GON, MEL, OBL, OVA, WIL, ALP, VIM, ARO.SAB)~.,
                 data = df,
                 loss = "binomial",
                                  epochs = 200L,
                 hidden = c(50L, 50L),
                 batchsize = 50L,
                 lr = 0.05,
                 lambda = 0.001,
                 alpha = 1.0,
                 validation = 0.2,
                 verbose = FALSE,
                 lr_scheduler = config_lr_scheduler("reduce_on_plateau", patience = 7), # reduce learning rate each 7 epochs if the validation loss didn't decrease,
                 early_stopping = 14, # stop training when validation loss didn't decrease for 10 epochs
                 bootstrap = 20L,
                 bootstrap_parallel = 5L)
```

We haven't really adjusted the training procedure, so let's check the convergence first:


```r
analyze_training(model_boot)
```


### Inference


```r
results = summary(model_boot)
#> Error in if (object$loss == "softmax") resp = object$response_column else resp = object$responses[i - : the condition has length > 1
results
#> Error in eval(expr, envir, enclos): object 'results' not found
```

cvTemp is significant for many species. Visualization of the effect:


```r
ale_plots = ALE(model_boot, variable = "cvTemp", plot = FALSE)
do.call(gridExtra::grid.arrange, ale_plots)
```

<div class="figure" style="text-align: center">
<img src="C/C-unnamed-chunk-24-1.png" alt="plot of chunk unnamed-chunk-24" width="400px" />
<p class="caption">plot of chunk unnamed-chunk-24</p>
</div>
