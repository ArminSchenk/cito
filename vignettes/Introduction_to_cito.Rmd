---
title: "cito"
author: "Christian Amesoeder"
date: "`r Sys.Date()`"
output:
  
 rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{cito}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

set.seed(123)

# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output

knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    
    x = gsub("\n$", "\n\n", x)
    x[x == ""] = "\n"
    unlist(strsplit(x, "\n"))
    
    x <- unlist(strsplit(x, '\n'))
    if (length(x) > (n+n+1)) {
      # truncate the output
      x <- c(head(x, n), " \n .... \n ",tail(x,n))
    }
    x <- paste(x, sep = " \n ")
  }
  hook_output(x, options)
})

if(!require(torch)) install.packages("torch")
torch::install_torch()

```

## Abstract
Cito allows you to build and train Neural Networks with the standard R syntax. The whole process of generating and training a deep neural network can be done with command. Afterwards all generic R methods can be used on the model, which makes comparisons way easier. It is based on the torch machine learning framework which is natively available for R. Therefore, no Python installation or API is needed for this package. 

## Setup 
### Installing torch 
Before using cito you have to make sure that the current version of torch is installed running. 
```{r setuptorch, eval = FALSE}

if(!require(torch)) install.packages("torch")
library(torch)
install_torch(reinstall = TRUE)

devtools::install_github("citoverse/cito")
library (cito)

``` 
### Data
As an example we will work with the irirs dataset. Which consists of three species and four descriptive variables the flower.  

```{r data}

data <- datasets::iris
head(data)

#scale dataset 
data <- data.frame(scale(data[,-5]),Species = data[,5])
```

## Introduction to models and model structures 
cito allows you to use the formula syntax to build multi layer perceptron networks. Here we build a network with 4 Hidden layers, each with 10 perceptrons. You do not need to adjust factors or include interactions as data columns manually this can be done through the formula syntax.

While this training is done on the cpu, all models can be trained on the gpu with device = "cuda".
```{r simp_models, fig.show='hide',out.lines = 3}
library(cito)
nn.fit <- dnn(Sepal.Length~. , data = data, hidden = c(10,10,10,10), epochs = 12, device = "cpu")


```

You can plot the networks structure to give you an overview of the created object. Be aware that this may take some time for really big networks.  
```{r plotnn, eval = FALSE}
plot(nn.fit)

```
![Structure plot of the generate network](structure_nn.png){ width=70%}

### Adding activation functions
As standard all layers are fitted with relu as activation layer.
$$
relu(x) = max (0,x)
$$
However, you can adjust the activation function of each layer individually to build exactly the network you want. In this case you have to provide a vector the same length as there are hidden layers. The activation function for the output layer (also called "link" function) is chosen automatically and does not have to be provided. 

```{r activation, results ="hide",fig.show='hide' ,eval = FALSE}
#selu as activation function for all layers: 
nn.fit <- dnn(Sepal.Length~., data = data, hidden = c(10,10,10,10), activation= "selu")
#layer specific activation functions: 
nn.fit <- dnn(Sepal.Length~., data = data, 
              hidden = c(10,10,10,10), activation= c("relu","selu","tanh","sigmoid"))


``` 



### Adding a validation set to the training process
In order to see where your model might start to overfit the addition of a validation set can be useful. In dnn() you can put activation = 0.x and the defined percentage will be not used for training and only for validation after each epoch. 

```{r validation, results = "hide", eval = TRUE, out.lines=3, fig.show='hide'}
#30% of data set is used as validation set
nn.fit <- dnn(Sepal.Length~., data = data, epochs = 32, hidden = c(10,10,10,10), validation = 0.3)


```
![Training loss](trainingsloss.png){width=70%}



### Interpreting model output
The standard S3 functions can be used to interpret the model: 


```{r interpret}
#prints the model structure
print(nn.fit) 

#utilize model on new data 
predict(nn.fit,data[1:3,])

```


```{r coef}
#returns weights of neural network
coef(nn.fit)

```

Feature Importance based on [Fisher, Rudin, and Dominici (2018)](https://arxiv.org/abs/1801.01489)
```{r summary}
# Calculate and return feature importance
summary(nn.fit)

```


```{r setup2, include = FALSE}
knitr::knit_hooks$set(output = hook_output)
```



## Training parameter for the Neural Network 
Training a neural network can be an art of itself. Fitting a neural network can take some trial and error. All commonly used approaches to 
### Regularization 
Neural Networks tend to overfit to your dataset. Therefore it is best to use some kind of regularization. In dnn() you can use elastic net regularization and dropout layers.

#### L1/L2 Regularization
The L1/L2 loss is controlled by alpha and lambda, where the loss is simply added to the general loss of the network. 
$$
L1/L2 loss = \lambda * \left[ (1 - \alpha) * |weights| + \alpha ||weights||^2\right
$$ 
The optimizer will try to also minimize the equation above which can lead to a nice generalization and avoids over fitting. 

If a single alpha value is provided each layer will get regularized the same. However, you can regularize each layer individually by providing a vector of alpha values the same length as there are hidden layers + 1, since the input layer also has weights that are relevant. Since FALSE gets converted to 0 in an numeric vector you have to enter NA if no penalty should be added on a specific layer.

```{r alpha, results ="hide",fig.show='hide',eval = FALSE }
#elastic net penalty in all layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), alpha = 0.5, lambda = 0.01)
#L1 generalization in the first layer no penalty on the other layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), 
              alpha = c(0,NA,NA,NA,NA), lambda = 0.01)


``` 

#### Dropout Regularization 

Dropouts are another way to regularize your model. During training in every epoch a perceptrons has a chance to get left out. You can control this percentage with the dropout parameter.


```{r dropout, results ="hide",fig.show='hide' ,eval = FALSE}
#dropout of 35% on all layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), dropout = 0.35)
#dropout of 35% only on last 2 layers: 
nn.fit <- dnn(Species~., data = data, 
              hidden = c(10,10,10,10), dropout = c(0, 0, 0.35, 0.35))


``` 

### Learning rate
The lr parameter controls the so called learning rate. This parameter defines the step size the optimizer takes during training at each epoch. If set too high the optimizer might not find a good optimum because you shoot across certain valleys. If set too low the training process takes too long to be viable. 

### Learning rate scheduler
Learning rate scheduler are allow you to start with a high learning rate and decrease it during the training process. 
You can choose between different types of schedulers. Namely, lambda, multiplicative, one_cycle and step.

The function config_lr_scheduler() helps you setup such a scheduler. See ?config_lr_scheduler() for more information

```{r lr_scheduler,eval = FALSE}
# Step Learning rate scheduler that reduces learning rate every 16 steps by a factor of 0.5
scheduler <- config_lr_scheduler(type = "step",
                                 step_size = 16,
                                 0.5) 

nn.fit <- dnn(Sepal.Length~., data = data,lr = 0.01, lr_scheduler= scheduler)


```


### Optimizer 
Optimizer are responsible for calculating the direction when updating weights in the training process. As standard dnn() uses the Adam optimizer. 


However, you can customize your optimizer with config_optimizer() and use other types of optimizer and adjust the hyperparameter of you optimizer. 
See ?config_optimizer() for more information.

```{r optim,eval = FALSE}

# adam optimizer with learning rate 0.002 with slightly changed betas to 0.95, 0.999 and eps to 1.5e-08
opt <- config_optimizer(
  type = "adam", 
  betas = c(0.95, 0.999), 
  eps = 1.5e-08)

nn.fit <- dnn(Species~., data = data, optimizer = opt, lr=0.002)


```

### Early Stopping 
Adding early stopping criteria helps you save time by stopping the training process early, if the validation loss of the current epoch is bigger than the validation loss n epochs early. The n can be defined by the early_stopping argument. It is required to set validation > 0. 
```{r early_stopping,eval = FALSE}
# Stops training if validation loss at current epoch is bigger than that 15 epochs earlier  
nn.fit <- dnn(Sepal.Length~., data = data, epochs = 1000, validation = 0.2, early_stopping = 15)


```



## Continue training 
In case you want to continue the training process of an existing model you need to use continue_training(). This function allows you to continue the training process where dnn() stopped. 
```{r setup3, include=FALSE}

# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output

knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    
    x = gsub("\n$", "\n\n", x)
    x[x == ""] = "\n"
    unlist(strsplit(x, "\n"))
    
    x <- unlist(strsplit(x, '\n'))
    if (length(x) > (n+n+1)) {
      # truncate the output
      x <- c(head(x, n), " \n .... \n ",tail(x,n))
    }
    x <- paste(x, sep = " \n ")
  }
  hook_output(x, options)
})
```

```{r continue_training,eval = FALSE, fig.show='hide',out.lines = 3}
# simple example, simply adding another 10 epochs to the training process   
nn.fit <- continue_training(nn.fit, epochs = 10)

```
It also allows you to change any training parameters, for example the learning rate. Additional, you can define which epoch the training should continue from. So you can analyze the training process with analyze_training() and afterwards pick an epoch from which on the training should be continued from with other training parameters. 

```{r continue_training2,eval = FALSE, fig.show='hide', out.lines = 3}

# picking a specific epoch, from which on the net should be trained 
# with changed parameters, in this case a smaller learning rate and a smaller batchsize
nn.fit <- continue_training(nn.fit, continue_from = 10, epochs = 10, 
                            changed_params = list(lr = 0.001, batchsize = 16))

```

```{r setup4,include= FALSE}
knitr::knit_hooks$set(output = hook_output)
```




