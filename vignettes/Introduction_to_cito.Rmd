---
title: "cito"
author: "Christian Amesoeder"
date: "`r Sys.Date()`"
output:
  
 rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{cito}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

set.seed(123)

# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output

knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    
    x = gsub("\n$", "\n\n", x)
    x[x == ""] = "\n"
    unlist(strsplit(x, "\n"))
    
    x <- unlist(strsplit(x, '\n'))
    if (length(x) > (n+n+1)) {
      # truncate the output
      x <- c(head(x, n), " \n .... \n ",tail(x,n))
    }
    x <- paste(x, sep = " \n ")
  }
  hook_output(x, options)
})

if(!require(torch)) install.packages("torch")
torch::install_torch()

```

## Abstract
cito aims at helping you build and training Neural Networks with the standard R syntax. It allows the whole model creation process and training to be done with one line of code as well as use all generic methods on the created object. It is based on the torch machine learning framework which is natively available for R. Therefore no Python installation or API is needed for this package. 

## Setup 
Before using cito you have to make sure that the current version of torch is installed running. 
```{r setuptorch, eval = FALSE}

if(!require(torch)) install.packages("torch")
library(torch)
install_torch(reinstall = TRUE)

``` 
## Data
As an example we will work with the irirs dataset. Which consists of three species and four corresponding variables.  

```{r data}

data <- datasets::iris
summary(data)
```

## Simple models and model structures 
With cito it allows to use the formula syntax to now train a simple multi layer perceptron network. Here we build a network with 4 Hidden layers, each with 10 perceptrons. The ever updating plot allows us to observe the training process, which in this case lasts for 32 epochs and is done on the cpu. 
```{r simp_models, fig.show='hide',out.lines = 3}
library(cito)
nn.fit <- dnn(Species~. , data = data, hidden = c(10,10,10,10), epochs = 12, device = "cpu")


```

Cito can plot the networks structure to give you an overview of the created object. For this step you need to have ggrpah installed. 
```{r plotnn, eval = FALSE}
plot(nn.fit)

```
![Structure plot of the generate network](structure_nn.png)



## Adding a validation set to the training process
In order to validate the network during the training process, you can put activation = 0.x. Before training the defined percentage of the data set will be split of and used as a validation set. This can be seen in the plot that gets shown during the training as well as in analyze_training(). 

```{r validation, results = "hide", eval = TRUE, out.lines=3, fig.show='hide'}
nn.fit <- dnn(Species~., data = data, epochs = 32, hidden = c(10,10,10,10), validation = 0.3)


```
![Trainings loss](trainingsloss.png)



## Interpreting model output
The standard S3 functions can be used to interpret the model: 


```{r interpret}
#prints the model structure as text
print(nn.fit) 

#utilize model on new data 
predict(nn.fit,data[1:3,])

```


```{r coef}
#weights of neural network
str(coef(nn.fit))

```

```{r summary}
# Calculate and return feature importance
#summary(nn.fit)

```


```{r setup2, include = FALSE}
knitr::knit_hooks$set(output = hook_output)
```



## Additional options for Neural Networks 
### Using different activation functions
As activation functions a wide array can be chosen. You can either define one activation for all layers or define it for each layer individually. In this case you have to provide a vector the same length as there are hidden layers. The activation function for the output layer is chosen automatically and therefore does not has to be provided. Cito supports the following activation functions: relu, leaky_relu, tanh, elu, rrelu, prelu, softplus, celu, selu, gelu, relu6, sigmoig, softsign, hardtanh, tanhshrink, softshrink, hardshrink, log_sigmoid. 
```{r activation, results ="hide",fig.show='hide' ,eval = FALSE}
#the same activation function for all layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), activation= "relu")
#layer specific activation functions: 
nn.fit <- dnn(Species~., data = data, 
              hidden = c(10,10,10,10), activation= c("relu","selu","tanh","sigmoid"))


``` 

### L1/L2 Regularization 
By setting alpha to a value in [0,1] you can use an elastic net generalization for each layer. The strength of this regularisation is defined with lambda. Where $$[\lambda * \left[ (1 - \alpha) * |weights| + \alpha ||weights||^2\right]$$ is added to the loss. If a single alpha value is provided each layer will get regularized the same. However, you can regularize each layer individually by providing a vector of alpha values the same length as there are hidden layers + 1, since you have to account the input layer. Since FALSE gets converted to 0 in an numeric vector you have to enter NA if no penalty should take place on a specific layer.

```{r alpha, results ="hide",fig.show='hide',eval = FALSE }
#elastic net penalty in all layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), alpha = 0.5, lambda = 0.1)
#L1 generalization in the first layer no penalty on the other layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), 
              alpha = c(0,NA,NA,NA,NA), lambda = 0.1)


``` 

### Dropout Regularisation 

Dropouts are a neat way to regularize your model. During training in every epoch a perceptrons has a chance to get left out. How high this chance is can be defined either over the whole model by providing a single numeric value in [0,1) or layer specific, with a vector of the same length as there are hidden layers. 

```{r dropout, results ="hide",fig.show='hide' ,eval = FALSE}
#dropout of 35% on all layers: 
nn.fit <- dnn(Species~., data = data, hidden = c(10,10,10,10), dropout = 0.35)
#dropout of 35% only on last 2 layers: 
nn.fit <- dnn(Species~., data = data, 
              hidden = c(10,10,10,10), dropout = c(0, 0, 0.35, 0.35))


``` 

### Custom Optimizer 
Optimizer are responsible for tuning every parameter ind the neural network. As standard dnn() uses the Adam optimizer. The learning rate is implemented as a standard argument since it is very comon to adjust that hyperparameter. However, if you want to customize your optimizer config_optimizer() is needed in order adjust any possible hypeparameters. 


```{r optim,eval = FALSE}
#standard use case without changing yperparameters of the adam optimizer
nn.fit <- dnn(Species~., data = data, optimizer = "adam", lr=0.002)


# adam optimizer with learning rate 0.002 with slightly changed betas to 0.9, 0.999 and eps to 1.5e-08
opt<- config_optimizer(type = "adam", betas = c(0.9, 0.999), eps = 1.5e-08)

nn.fit <- dnn(Species~., data = data, optimizer = opt, lr=0.002)


```
### Adjustable learning rate
Learning rate scheduler are also available. You can choose between lambda, multiplicative, one_cycle and step. Every scheduler requires different arguments. See the documentation of the torch package e.g. for step ?torch::lr_step. Those arguments have to be defined in the config list with their lr_scheduler + their original name. See below for an example:

```{r lr_scheduler,eval = FALSE}
# Step Learning rate scheduler that reduces learning rate every 10 steps with a gamma of 0.5
nn.fit <- dnn(Species~., data = data,lr = 0.01, lr_scheduler= "step", 
              config = list(lr_scheduler.step_size = 10, lr_scheduler.gamma = 0.5))


```

### Early Stopping 
Adding early stopping criteria helps saving time by stopping the training process at the current epoch if the validation loss of the current epoch is bigger than the validation loss n epochs early. The n can be defined by the ealry_stopping argument. It is required to set validation > 0. 
```{r early_stopping,eval = FALSE}
# Stops training if validation loss of current epoch is bigger than that 10 epochs earlier  
nn.fit <- dnn(Species~., data = data, epochs = 1000, validation = 0.2, early_stopping = 10)


```



## Continuing Training 
For the case that the specified number of epochs were not enough, cito provides the continue_training() function. It allows to continue the training process where dnn() stopped. 
```{r setup3, include=FALSE}

# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output

knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    
    x = gsub("\n$", "\n\n", x)
    x[x == ""] = "\n"
    unlist(strsplit(x, "\n"))
    
    x <- unlist(strsplit(x, '\n'))
    if (length(x) > (n+n+1)) {
      # truncate the output
      x <- c(head(x, n), " \n .... \n ",tail(x,n))
    }
    x <- paste(x, sep = " \n ")
  }
  hook_output(x, options)
})
```

```{r continue_training,eval = FALSE, fig.show='hide',out.lines = 3}
# simple example, simply adding another 10 epochs to the training process   
nn.fit <- continue_training(nn.fit, epochs = 10)

```
It also allows to change any training parameter, for example the learning rate. Additional, you can define which epoch the training should continue from. So you can analyze the training process with analyze_training() and afterwards pick an epoch from which on the training should be continued from with other training parameters. 

```{r continue_training2,eval = FALSE, fig.show='hide', out.lines = 3}

# picking a specific epoch, from which on the net should be trained 
# with changed parameters, in this case a smaller learning rate and a smaller batchsize
nn.fit <- continue_training(nn.fit, continue_from = 10, epochs = 10, 
                            changed_params = list(lr = 0.001, batchsize = 16))

```

```{r setup4,include= FALSE}
knitr::knit_hooks$set(output = hook_output)
```



## Building a sophisticated Neural Network
A sophisticated Neural Network will require the usage of many different arguments. There are various ways to generalize the model and train it correctly. What is best for your use case has to be explored individually.  
 

